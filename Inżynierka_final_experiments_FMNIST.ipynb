{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb49e0b-05e6-4977-83fa-52625ca6f3d8",
   "metadata": {},
   "source": [
    "# Aktywne uczenie\n",
    "## Przygotowanie Å›rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b9e3b4-dbb2-4af0-a1fa-b61e352ed97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:05.012288Z",
     "iopub.status.busy": "2023-01-12T06:45:05.012006Z",
     "iopub.status.idle": "2023-01-12T06:45:08.605661Z",
     "shell.execute_reply": "2023-01-12T06:45:08.604830Z",
     "shell.execute_reply.started": "2023-01-12T06:45:05.012219Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09499a68-1278-44dc-ac55-3cc5a544255b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:08.607240Z",
     "iopub.status.busy": "2023-01-12T06:45:08.606862Z",
     "iopub.status.idle": "2023-01-12T06:45:08.611926Z",
     "shell.execute_reply": "2023-01-12T06:45:08.611039Z",
     "shell.execute_reply.started": "2023-01-12T06:45:08.607220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b9bdb1-e247-408c-aaaa-9512e837f0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:08.614057Z",
     "iopub.status.busy": "2023-01-12T06:45:08.613352Z",
     "iopub.status.idle": "2023-01-12T06:45:08.620100Z",
     "shell.execute_reply": "2023-01-12T06:45:08.619497Z",
     "shell.execute_reply.started": "2023-01-12T06:45:08.614026Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = True #as True useful with training CNN networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db580480-bdf2-40f1-856d-8e1e51ca684c",
   "metadata": {},
   "source": [
    "## Przygotowanie danych - FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82c950d-d9a0-4bf9-a4b1-ffda805aef54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:11.738500Z",
     "iopub.status.busy": "2023-01-12T06:45:11.737822Z",
     "iopub.status.idle": "2023-01-12T06:45:12.720246Z",
     "shell.execute_reply": "2023-01-12T06:45:12.719617Z",
     "shell.execute_reply.started": "2023-01-12T06:45:11.738460Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64, 64)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd29c841-0864-4bf6-8a3b-ab03f645b275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:55:09.501141Z",
     "iopub.status.busy": "2023-01-08T19:55:09.500177Z",
     "iopub.status.idle": "2023-01-08T19:55:09.510411Z",
     "shell.execute_reply": "2023-01-08T19:55:09.509294Z",
     "shell.execute_reply.started": "2023-01-08T19:55:09.501111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -0.9843, -0.9843, -0.9843,\n",
       "        -0.9843, -0.9922, -0.9922, -0.9843, -0.9843, -0.9922, -1.0000, -1.0000,\n",
       "        -1.0000, -0.9922, -0.9922, -0.9843, -0.9843, -0.9765, -0.9059, -0.8196,\n",
       "        -0.6392, -0.3647, -0.1451, -0.1137, -0.0824, -0.1373, -0.2000, -0.1922,\n",
       "        -0.1451, -0.1137, -0.0902, -0.0667, -0.0118,  0.0510,  0.1216,  0.1922,\n",
       "         0.1843,  0.1216,  0.0745,  0.1451,  0.2078,  0.2078,  0.2078,  0.2157,\n",
       "         0.2157,  0.2314,  0.2392,  0.2471,  0.1843,  0.1216,  0.1216,  0.1451,\n",
       "         0.1686,  0.2000,  0.2392,  0.3176,  0.3882, -0.1686, -0.7725, -0.9922])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainset.data = torch.stack([trainset.data]*3, -1)\n",
    "# testset.data = torch.stack([testset.data]*3, -1)\n",
    "/temp_loader =torch.utils.data.DataLoader(testset)\n",
    "next(iter(temp_loader))[0][0][2][35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224acf86-9006-4f6b-bb19-ca6dccd2bc86",
   "metadata": {},
   "source": [
    "Generate starting indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac16efd0-2009-4204-91fd-632fc9725ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:27:11.487576Z",
     "iopub.status.busy": "2023-01-08T19:27:11.486678Z",
     "iopub.status.idle": "2023-01-08T19:27:11.493980Z",
     "shell.execute_reply": "2023-01-08T19:27:11.492938Z",
     "shell.execute_reply.started": "2023-01-08T19:27:11.487549Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val():\n",
    "    rand_sampler = torch.utils.data.RandomSampler(trainset, replacement=False)\n",
    "    rand_batch_sampler = torch.utils.data.BatchSampler(rand_sampler,\n",
    "                                                       batch_size=int(0.1*len(trainset)),\n",
    "                                                       drop_last=False)\n",
    "    train_idx = []\n",
    "    for i, batch in enumerate(iter(rand_batch_sampler)):\n",
    "        if i == 0:\n",
    "            validation_idx = batch\n",
    "        else:\n",
    "            train_idx.extend(batch)\n",
    "    return train_idx, validation_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b974e6b6-4825-47c0-9bae-ae3c1a3a187e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:27:46.961585Z",
     "iopub.status.busy": "2023-01-08T19:27:46.960727Z",
     "iopub.status.idle": "2023-01-08T19:27:47.605382Z",
     "shell.execute_reply": "2023-01-08T19:27:47.604196Z",
     "shell.execute_reply.started": "2023-01-08T19:27:46.961555Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx_df = pd.DataFrame()\n",
    "val_idx_df = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    train_idx, validation_idx = get_train_val()\n",
    "    train_idx_df[i] = train_idx\n",
    "    val_idx_df[i] = validation_idx\n",
    "train_idx_df.to_csv(\"train_idx.csv\", index=False)\n",
    "val_idx_df.to_csv(\"val_idx.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072fb7f-e936-4333-b4db-0924d87b327d",
   "metadata": {},
   "source": [
    "Load starting indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148a307b-a941-4a45-bad5-501595b2f357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:17.466423Z",
     "iopub.status.busy": "2023-01-12T06:45:17.466179Z",
     "iopub.status.idle": "2023-01-12T06:45:17.510047Z",
     "shell.execute_reply": "2023-01-12T06:45:17.509405Z",
     "shell.execute_reply.started": "2023-01-12T06:45:17.466406Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx_df = pd.read_csv(\"train_idx.csv\")\n",
    "val_idx_df = pd.read_csv(\"val_idx.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8696ed8-0ade-4d29-8baf-8b6acaa37fa8",
   "metadata": {},
   "source": [
    "## Konfiguracja sieci - VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ea004d-97e4-4b9b-82b8-9c99385da81a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:20.299283Z",
     "iopub.status.busy": "2023-01-12T06:45:20.298515Z",
     "iopub.status.idle": "2023-01-12T06:45:30.870155Z",
     "shell.execute_reply": "2023-01-12T06:45:30.869537Z",
     "shell.execute_reply.started": "2023-01-12T06:45:20.299263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de43bab2ae04ce99c5285e442e0d438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vgg16 = models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT) # get vgg16 model with pretrained weights\n",
    "\n",
    "# change the number of classes in the last layer\n",
    "vgg16.classifier[6].out_features = 10\n",
    "# freeze convolution weights\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "input_lastLayer = vgg16.classifier[6].in_features\n",
    "vgg16.classifier[6] = nn.Linear(input_lastLayer,10)\n",
    "vgg16.to(device)\n",
    "net = vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a71f5b-a35f-4819-922f-4ebb933d33c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:30.872998Z",
     "iopub.status.busy": "2023-01-12T06:45:30.872851Z",
     "iopub.status.idle": "2023-01-12T06:45:30.876913Z",
     "shell.execute_reply": "2023-01-12T06:45:30.876199Z",
     "shell.execute_reply.started": "2023-01-12T06:45:30.872983Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2467bb-34f7-47e7-a07e-2bb5d638a997",
   "metadata": {},
   "source": [
    "Generate starting state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42667de0-708c-4b3b-980b-cdc22019caa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T19:29:18.435275Z",
     "iopub.status.busy": "2023-01-08T19:29:18.434919Z",
     "iopub.status.idle": "2023-01-08T19:29:22.319490Z",
     "shell.execute_reply": "2023-01-08T19:29:22.318520Z",
     "shell.execute_reply.started": "2023-01-08T19:29:18.435249Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_dict = copy.deepcopy(net.state_dict())\n",
    "optim_dict = copy.deepcopy(optimizer.state_dict())\n",
    "sched_dict = copy.deepcopy(scheduler.state_dict())\n",
    "\n",
    "torch.save(initial_dict, \"model_dict.pt\")\n",
    "torch.save(optim_dict, \"optimizer_dict.pt\")\n",
    "torch.save(sched_dict, \"scheduler_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d48b4-24fc-4424-a16f-1b92ae4c514d",
   "metadata": {},
   "source": [
    "Load starting state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9139ca-1deb-4b64-bc2b-0e1fa4329a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:33.157897Z",
     "iopub.status.busy": "2023-01-12T06:45:33.157588Z",
     "iopub.status.idle": "2023-01-12T06:45:35.120097Z",
     "shell.execute_reply": "2023-01-12T06:45:35.119513Z",
     "shell.execute_reply.started": "2023-01-12T06:45:33.157877Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_dict =torch.load(\"model_dict.pt\")\n",
    "optim_dict = torch.load(\"optimizer_dict.pt\")\n",
    "sched_dict = torch.load(\"scheduler_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e551c-81d3-4e79-9c67-b3f5960fd729",
   "metadata": {},
   "source": [
    "## Aktywne uczenie - pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf9decf-843a-4caa-bc92-bdb919947697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:37.456333Z",
     "iopub.status.busy": "2023-01-12T06:45:37.456038Z",
     "iopub.status.idle": "2023-01-12T06:45:37.461301Z",
     "shell.execute_reply": "2023-01-12T06:45:37.460714Z",
     "shell.execute_reply.started": "2023-01-12T06:45:37.456312Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for int, data in enumerate(dataloader):\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_module(output, target)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            val_running_correct += (preds == target).sum().item()\n",
    "        val_loss = val_running_loss/len(dataloader.dataset)\n",
    "        val_accuracy = 100. * val_running_correct/len(dataloader.dataset)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f4b749-3fbf-4206-9ed4-c9b3c18199be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:40.199568Z",
     "iopub.status.busy": "2023-01-12T06:45:40.199288Z",
     "iopub.status.idle": "2023-01-12T06:45:40.210117Z",
     "shell.execute_reply": "2023-01-12T06:45:40.209525Z",
     "shell.execute_reply.started": "2023-01-12T06:45:40.199548Z"
    }
   },
   "outputs": [],
   "source": [
    "def active_learn(model, train_data, train_idx, val_idx, heuristic, initial_train_idx, experiment_id):\n",
    "    model.load_state_dict(initial_dict)\n",
    "    optimizer.load_state_dict(optim_dict)\n",
    "    batch_len = int(0.05*len(train_idx)) # calculate length of 5% of training data\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(train_data, val_idx),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    train_loss, train_acc = [], []\n",
    "    for epoch in range(20):\n",
    "        print(\"Epoch: \", epoch)\n",
    "    \n",
    "        if epoch == 0:\n",
    "        # if False:\n",
    "            # batch_idx, train_idx = generate_random_sample(train_idx, batch_len) # equivalent to first (labeled) dataset\n",
    "            batch_idx = initial_train_idx\n",
    "            sub_epochs = 7\n",
    "        else:\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = 0.001\n",
    "            old_idx = batch_idx \n",
    "            batch_idx, train_idx = generate_heuristic_sample(train_idx, batch_len, model, heuristic, train_data) # equivalent to asking for labelling\n",
    "            # consider joining the samples\n",
    "            batch_idx = np.append(old_idx,batch_idx)\n",
    "            sub_epochs = 7\n",
    "        print(f\"Training on {len(batch_idx)} samples\")\n",
    "        \n",
    "        # model.load_state_dict(initial_dict)\n",
    "        # optimizer.load_state_dict(optim_dict)\n",
    "        # print(model.state_dict(['conv1.weight'])[0])\n",
    "        model.train()  # turn on training mode\n",
    "        epoch_subset =  torch.utils.data.Subset(train_data, batch_idx) # get epoch subset \n",
    "        epoch_loader =  torch.utils.data.DataLoader(epoch_subset, batch_size=64, num_workers=8, shuffle=True) # convert into loader\n",
    "        # consider retraining multiple times\n",
    "        for sub_epoch in range(sub_epochs):\n",
    "            total = 0   # total n of samples seen\n",
    "            correct = 0   # total n of coreectly classified samples\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(epoch_loader):\n",
    "                data, target = data[0].to(device), data[1].to(device)\n",
    "                optimizer.zero_grad()  # zero out the gradients\n",
    "                output = model(data)  # get the output of the net\n",
    "                loss = loss_module(output, target)  # calculate the loss \n",
    "                running_loss += loss.item()  # add the loss on the subset batch\n",
    "                _, preds = torch.max(output.data, 1)  # get the predictions\n",
    "                correct += (preds == target).sum().item() # add the n of correctly classified samples\n",
    "                total += target.size(0) # add the total of samples seen\n",
    "                loss.backward()  # backpropagate the weights\n",
    "                optimizer.step()  # optimize\n",
    "                del data, target, output, preds\n",
    "            t_loss = running_loss/total\n",
    "            t_acc = 100. * correct/total\n",
    "            val_loss, val_acc = validate(model, val_loader)\n",
    "            print(f\"Sub epoch {sub_epoch} train acc: {t_acc:.2f} train loss: {t_loss:.4f} val acc: {val_acc:.2f} val loss: {val_loss:.4f}\")\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(t_acc)\n",
    "\n",
    "        # val_loss, val_acc = validate(model, val_loader)\n",
    "        validation_loss.append(val_loss)\n",
    "        validation_acc.append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {t_loss:.4f}, Train Acc: {t_acc:.2f} \",\n",
    "          f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}\")\n",
    "        if heuristic is not None:\n",
    "            torch.save(model.state_dict(), f'{experiment_id}/epoch{epoch}_{heuristic.__name__}.pt')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f'{experiment_id}/epoch{epoch}_random.pt')\n",
    "    return train_loss, train_acc, validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b10248-4860-4bfa-ba46-12936af997c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:42.433438Z",
     "iopub.status.busy": "2023-01-12T06:45:42.433195Z",
     "iopub.status.idle": "2023-01-12T06:45:42.438415Z",
     "shell.execute_reply": "2023-01-12T06:45:42.437855Z",
     "shell.execute_reply.started": "2023-01-12T06:45:42.433415Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_acc_per_class(model, testloader, classes):\n",
    "    \n",
    "    model.eval()\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data    \n",
    "            images = images.to(device)\n",
    "            outputs = model(images).cpu()   \n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    all_acc = []\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        all_acc.append(accuracy)\n",
    "        print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, \n",
    "                                                       accuracy))\n",
    "    print(f\"Average accuracy is {sum(all_acc)/len(all_acc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b3582-953a-4519-83b9-9751255e12d2",
   "metadata": {},
   "source": [
    "## Heurystyki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fffd838-4f79-423a-be77-79e4161e38e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:47.381448Z",
     "iopub.status.busy": "2023-01-12T06:45:47.381212Z",
     "iopub.status.idle": "2023-01-12T06:45:47.386182Z",
     "shell.execute_reply": "2023-01-12T06:45:47.385617Z",
     "shell.execute_reply.started": "2023-01-12T06:45:47.381429Z"
    }
   },
   "outputs": [],
   "source": [
    "def largest_margin_heuristic(indices, n_samples, model, data):\n",
    "    if len(indices) <= n_samples:\n",
    "        return indices, []\n",
    "    with torch.no_grad():\n",
    "        heuristic_loader = torch.utils.data.DataLoader(\n",
    "          torch.utils.data.Subset(data, indices),\n",
    "          batch_size=64,\n",
    "          num_workers=8\n",
    "        )\n",
    "        diff = np.array([])\n",
    "        for data in heuristic_loader:\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(data)\n",
    "            probs = torch.softmax(output, axis=1)\n",
    "            batch_diff = torch.max(probs.data, 1)[0] - torch.min(probs.data, 1)[0]\n",
    "            diff = np.append(diff, batch_diff.cpu().numpy())\n",
    "        #choose n_samples with smallest ?\n",
    "        \n",
    "        smallest = np.argpartition(diff, n_samples)[:n_samples]\n",
    "        chosen = indices[smallest]\n",
    "        leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "        return chosen, leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba9f5c1-c9da-49de-a322-c495b82c41fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:48.308929Z",
     "iopub.status.busy": "2023-01-12T06:45:48.308693Z",
     "iopub.status.idle": "2023-01-12T06:45:48.314319Z",
     "shell.execute_reply": "2023-01-12T06:45:48.313531Z",
     "shell.execute_reply.started": "2023-01-12T06:45:48.308912Z"
    }
   },
   "outputs": [],
   "source": [
    "def smallest_margin_heuristic(indices, n_samples, model, data):\n",
    "    if len(indices) <= n_samples:\n",
    "        return indices, []\n",
    "    with torch.no_grad():\n",
    "        heuristic_loader = torch.utils.data.DataLoader(\n",
    "          torch.utils.data.Subset(data, indices),\n",
    "          batch_size=64,\n",
    "          num_workers=8\n",
    "        )\n",
    "        diff = np.array([])\n",
    "        for data in heuristic_loader:\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(data)\n",
    "            probs = torch.softmax(output, axis=1)\n",
    "            top2 = torch.topk(probs.data, 2).values\n",
    "            batch_diff = top2[:,0] - top2[:,1]\n",
    "            diff = np.append(diff, batch_diff.cpu().numpy())\n",
    "        #choose n_samples with smallest ?\n",
    "        smallest = np.argpartition(diff, n_samples)[:n_samples]\n",
    "        chosen = indices[smallest]\n",
    "        leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "        return chosen, leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297389c5-24c1-4557-93e2-3dd4920c17e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:49.117155Z",
     "iopub.status.busy": "2023-01-12T06:45:49.116920Z",
     "iopub.status.idle": "2023-01-12T06:45:49.122090Z",
     "shell.execute_reply": "2023-01-12T06:45:49.121436Z",
     "shell.execute_reply.started": "2023-01-12T06:45:49.117133Z"
    }
   },
   "outputs": [],
   "source": [
    "def least_confidence_heuristic(indices, n_samples, model, data):\n",
    "    if len(indices) <= n_samples:\n",
    "        return indices, []\n",
    "    with torch.no_grad():\n",
    "        heuristic_loader = torch.utils.data.DataLoader(\n",
    "          torch.utils.data.Subset(data, indices),\n",
    "          batch_size=64,\n",
    "          num_workers=8\n",
    "        )\n",
    "        max_probs = np.array([])\n",
    "        for data in heuristic_loader:\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(data)\n",
    "            probs = torch.softmax(output, axis=1)\n",
    "            max_prob = torch.max(output.data, 1)[0]\n",
    "            max_probs = np.append(max_probs, max_prob.cpu().numpy())\n",
    "        #choose n_samples with smallest ?\n",
    "        smallest = np.argpartition(max_probs, n_samples)[:n_samples]\n",
    "        chosen = indices[smallest]\n",
    "        leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "        return chosen, leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45948a5c-237e-4102-86a8-08af8d23da42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:49.892994Z",
     "iopub.status.busy": "2023-01-12T06:45:49.892618Z",
     "iopub.status.idle": "2023-01-12T06:45:49.900851Z",
     "shell.execute_reply": "2023-01-12T06:45:49.900148Z",
     "shell.execute_reply.started": "2023-01-12T06:45:49.892970Z"
    }
   },
   "outputs": [],
   "source": [
    "def mc_dropout_heuristic(indices, n_samples, model, data):\n",
    "    if len(indices) <= n_samples:\n",
    "        return indices, []\n",
    "    with torch.no_grad():\n",
    "        heuristic_loader = torch.utils.data.DataLoader(\n",
    "          torch.utils.data.Subset(data, indices),\n",
    "          batch_size=64,\n",
    "          num_workers=8\n",
    "        )\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Dropout) or isinstance(m, nn.Dropout2d):\n",
    "                m.train(True)\n",
    "                count += 1\n",
    "        assert count > 0, 'We can only do models with dropout!'\n",
    "        i = 0\n",
    "        all_results = np.array([])\n",
    "        for data in heuristic_loader:\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            input = data.repeat(7, 1, 1, 1)\n",
    "            output = model(input).data\n",
    "            average_output = output.view(7, data.size(0), -1).mean(dim=0)\n",
    "            probs = torch.softmax(average_output,axis=1)\n",
    "            entropy = (-probs * probs.log()).sum(dim=1, keepdim=True)\n",
    "            all_results = np.append(all_results, entropy.cpu().numpy())\n",
    "            i+=1\n",
    "        #choose n_samples with largest ?\n",
    "        # print(all_results, all_results.shape)\n",
    "        smallest = np.argpartition(all_results, n_samples)[-n_samples:]\n",
    "        # print(smallest, smallest.shape)\n",
    "        # print(smallest[:n_samples], smallest[:n_samples].shape)\n",
    "        chosen = indices[smallest]\n",
    "        leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "        return chosen, leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dad11b71-27ef-45bb-ab17-e7f68cd13efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:50.899058Z",
     "iopub.status.busy": "2023-01-12T06:45:50.898780Z",
     "iopub.status.idle": "2023-01-12T06:45:50.905212Z",
     "shell.execute_reply": "2023-01-12T06:45:50.904642Z",
     "shell.execute_reply.started": "2023-01-12T06:45:50.899038Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_svm_heuristic(indices, n_samples, model, data):\n",
    "    if len(indices) <= n_samples:\n",
    "        return indices, []\n",
    "    # global svm, train_svm\n",
    "    svm_results = np.array([])\n",
    "    # if train_svm:\n",
    "    if True:\n",
    "        heuristic_loader = torch.utils.data.DataLoader(\n",
    "              torch.utils.data.Subset(data, indices),\n",
    "              batch_size=64,\n",
    "              num_workers=8\n",
    "            )\n",
    "        for data in heuristic_loader:\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(data)\n",
    "            svm_result = svm(output)\n",
    "            svm_results = np.append(svm_results, svm_result.detach().cpu().numpy())\n",
    "        #choose n_samples with smallest ?\n",
    "        smallest = np.argpartition(svm_results, n_samples)[:n_samples]\n",
    "        chosen = indices[smallest]\n",
    "        leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "        # train_svm = False\n",
    "        return chosen, leftover\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            heuristic_loader = torch.utils.data.DataLoader(\n",
    "              torch.utils.data.Subset(data, indices),\n",
    "              batch_size=64,\n",
    "              num_workers=8\n",
    "            )\n",
    "            for data in heuristic_loader:\n",
    "                data, target = data[0].to(device), data[1].to(device)\n",
    "                output = model(data)\n",
    "                svm_result = svm(output)\n",
    "                svm_results = np.append(svm_results, svm_result.cpu().numpy())\n",
    "            #choose n_samples with largest ?\n",
    "            smallest = np.argpartition(svm_results, n_samples)[-n_samples:]\n",
    "            chosen = indices[smallest]\n",
    "            leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "            return chosen, leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2268a31a-1bce-4a7f-8416-ad0f6ecba90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:52.795474Z",
     "iopub.status.busy": "2023-01-12T06:45:52.795238Z",
     "iopub.status.idle": "2023-01-12T06:45:52.799241Z",
     "shell.execute_reply": "2023-01-12T06:45:52.798433Z",
     "shell.execute_reply.started": "2023-01-12T06:45:52.795456Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_random_sample(indices, n_samples):\n",
    "    chosen = np.random.choice(indices, n_samples, replace=False)\n",
    "    leftover = np.setdiff1d(indices, chosen, assume_unique=True)\n",
    "  # leftover = np.delete(indices, chosen)\n",
    "    return chosen, leftover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe017bbe-b673-42d5-a061-4d0f295ca95a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:45:54.284611Z",
     "iopub.status.busy": "2023-01-12T06:45:54.284290Z",
     "iopub.status.idle": "2023-01-12T06:45:54.288725Z",
     "shell.execute_reply": "2023-01-12T06:45:54.288012Z",
     "shell.execute_reply.started": "2023-01-12T06:45:54.284602Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_heuristic_sample(indices, n_samples, model, heuristic=None, data=None):\n",
    "    if heuristic is None:\n",
    "        return generate_random_sample(indices, n_samples)\n",
    "    else:\n",
    "        return heuristic(indices, n_samples, model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb286ad0-1bc6-46c3-ac0c-3ecce6d198a4",
   "metadata": {},
   "source": [
    "## Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77642640-194b-400c-b372-e57d72394ea1",
   "metadata": {},
   "source": [
    "## Baseline of 20 epochs on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a063aa05-58c7-41be-bbb6-1a4f799712e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T20:35:30.742823Z",
     "iopub.status.busy": "2023-01-08T20:35:30.742479Z",
     "iopub.status.idle": "2023-01-08T20:35:30.753690Z",
     "shell.execute_reply": "2023-01-08T20:35:30.752888Z",
     "shell.execute_reply.started": "2023-01-08T20:35:30.742795Z"
    }
   },
   "outputs": [],
   "source": [
    "def base_learn(model, train_data, train_idx, val_idx):\n",
    "    model.load_state_dict(initial_dict)\n",
    "    optimizer.load_state_dict(optim_dict)\n",
    "    scheduler.load_state_dict(sched_dict)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(train_data, val_idx),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    train_loss, train_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    batch_idx = train_idx\n",
    "    print(f\"Training on {len(batch_idx)} samples\")\n",
    "    sub_epochs = 7\n",
    "    model.train()  # turn on training mode\n",
    "    epoch_subset =  torch.utils.data.Subset(train_data, batch_idx) # get epoch subset \n",
    "    epoch_loader =  torch.utils.data.DataLoader(epoch_subset, batch_size=64, num_workers=8, shuffle=True) # convert into loader\n",
    "    # consider retraining multiple times\n",
    "    for sub_epoch in range(sub_epochs):\n",
    "        total = 0   # total n of samples seen\n",
    "        correct = 0   # total n of coreectly classified samples\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(epoch_loader):\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()  # zero out the gradients\n",
    "            output = model(data)  # get the output of the net\n",
    "            loss = loss_module(output, target)  # calculate the loss \n",
    "            running_loss += loss.item()  # add the loss on the subset batch\n",
    "            _, preds = torch.max(output.data, 1)  # get the predictions\n",
    "            correct += (preds == target).sum().item() # add the n of correctly classified samples\n",
    "            total += target.size(0) # add the total of samples seen\n",
    "            loss.backward()  # backpropagate the weights\n",
    "            optimizer.step()  # optimize\n",
    "            del data, target, output, preds\n",
    "        t_loss = running_loss/total\n",
    "        t_acc = 100. * correct/total\n",
    "        val_loss, val_acc = validate(model, val_loader)\n",
    "        print(f\"Sub epoch {sub_epoch} train acc: {t_acc:.2f} train loss: {t_loss:.4f} val acc: {val_acc:.2f} val loss: {val_loss:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    train_loss.append(t_loss)\n",
    "    train_acc.append(t_acc)\n",
    "\n",
    "    validation_loss.append(val_loss)\n",
    "    validation_acc.append(val_acc)\n",
    "    \n",
    "    ts_loss, ts_acc = validate(model, test_loader)\n",
    "    test_loss.append(ts_loss)\n",
    "    test_acc.append(ts_acc)\n",
    "\n",
    "    print(f\"Train Loss: {t_loss:.4f}, Train Acc: {t_acc:.2f} \",\n",
    "      f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.2f}\")\n",
    "    # if heuristic is not None:\n",
    "    #     torch.save(model.state_dict(), f'{experiment_id}/epoch{epoch}_{heuristic.__name__}.pt')\n",
    "    # else:\n",
    "    #     torch.save(model.state_dict(), f'{experiment_id}/epoch{epoch}_random.pt')\n",
    "    return train_loss, train_acc, validation_loss, validation_acc, test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2fd418c-334c-4fc3-98fa-eb8ce8222a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T20:35:33.132661Z",
     "iopub.status.busy": "2023-01-08T20:35:33.132309Z",
     "iopub.status.idle": "2023-01-08T21:17:58.919120Z",
     "shell.execute_reply": "2023-01-08T21:17:58.918195Z",
     "shell.execute_reply.started": "2023-01-08T20:35:33.132635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 54000 samples\n",
      "Sub epoch 0 train acc: 77.92 train loss: 0.0114 val acc: 84.92 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 87.96 train loss: 0.0053 val acc: 88.70 val loss: 0.0049\n",
      "Sub epoch 2 train acc: 89.78 train loss: 0.0043 val acc: 89.10 val loss: 0.0051\n",
      "Sub epoch 3 train acc: 90.75 train loss: 0.0040 val acc: 88.72 val loss: 0.0055\n",
      "Sub epoch 4 train acc: 91.45 train loss: 0.0036 val acc: 88.77 val loss: 0.0054\n",
      "Sub epoch 5 train acc: 92.10 train loss: 0.0033 val acc: 89.48 val loss: 0.0049\n",
      "Sub epoch 6 train acc: 93.01 train loss: 0.0030 val acc: 89.63 val loss: 0.0054\n",
      "Train Loss: 0.0030, Train Acc: 93.01  Validation Loss: 0.0054, Validation Acc: 89.63\n",
      "Training on 54000 samples\n",
      "Sub epoch 0 train acc: 77.73 train loss: 0.0114 val acc: 83.43 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 87.95 train loss: 0.0053 val acc: 87.98 val loss: 0.0053\n",
      "Sub epoch 2 train acc: 89.67 train loss: 0.0044 val acc: 88.15 val loss: 0.0053\n",
      "Sub epoch 3 train acc: 90.64 train loss: 0.0040 val acc: 87.77 val loss: 0.0056\n",
      "Sub epoch 4 train acc: 91.44 train loss: 0.0036 val acc: 88.20 val loss: 0.0056\n",
      "Sub epoch 5 train acc: 92.32 train loss: 0.0033 val acc: 88.87 val loss: 0.0051\n",
      "Sub epoch 6 train acc: 93.01 train loss: 0.0029 val acc: 89.33 val loss: 0.0056\n",
      "Train Loss: 0.0029, Train Acc: 93.01  Validation Loss: 0.0056, Validation Acc: 89.33\n",
      "Training on 54000 samples\n",
      "Sub epoch 0 train acc: 77.75 train loss: 0.0115 val acc: 84.55 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 87.99 train loss: 0.0052 val acc: 88.18 val loss: 0.0052\n",
      "Sub epoch 2 train acc: 89.84 train loss: 0.0044 val acc: 86.75 val loss: 0.0056\n",
      "Sub epoch 3 train acc: 90.76 train loss: 0.0039 val acc: 88.45 val loss: 0.0054\n",
      "Sub epoch 4 train acc: 91.71 train loss: 0.0035 val acc: 87.85 val loss: 0.0057\n",
      "Sub epoch 5 train acc: 92.63 train loss: 0.0031 val acc: 87.52 val loss: 0.0071\n",
      "Sub epoch 6 train acc: 93.10 train loss: 0.0029 val acc: 88.12 val loss: 0.0065\n",
      "Train Loss: 0.0029, Train Acc: 93.10  Validation Loss: 0.0065, Validation Acc: 88.12\n",
      "Training on 54000 samples\n",
      "Sub epoch 0 train acc: 78.10 train loss: 0.0114 val acc: 86.18 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 88.14 train loss: 0.0052 val acc: 88.32 val loss: 0.0052\n",
      "Sub epoch 2 train acc: 89.71 train loss: 0.0043 val acc: 88.35 val loss: 0.0055\n",
      "Sub epoch 3 train acc: 90.65 train loss: 0.0039 val acc: 88.93 val loss: 0.0049\n",
      "Sub epoch 4 train acc: 91.71 train loss: 0.0035 val acc: 89.18 val loss: 0.0054\n",
      "Sub epoch 5 train acc: 92.45 train loss: 0.0032 val acc: 88.63 val loss: 0.0056\n",
      "Sub epoch 6 train acc: 93.08 train loss: 0.0029 val acc: 89.40 val loss: 0.0056\n",
      "Train Loss: 0.0029, Train Acc: 93.08  Validation Loss: 0.0056, Validation Acc: 89.40\n",
      "Training on 54000 samples\n",
      "Sub epoch 0 train acc: 77.81 train loss: 0.0114 val acc: 85.85 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 88.17 train loss: 0.0052 val acc: 88.23 val loss: 0.0052\n",
      "Sub epoch 2 train acc: 89.55 train loss: 0.0044 val acc: 87.45 val loss: 0.0056\n",
      "Sub epoch 3 train acc: 90.71 train loss: 0.0040 val acc: 88.67 val loss: 0.0051\n",
      "Sub epoch 4 train acc: 91.66 train loss: 0.0036 val acc: 88.58 val loss: 0.0053\n",
      "Sub epoch 5 train acc: 92.32 train loss: 0.0032 val acc: 88.47 val loss: 0.0056\n",
      "Sub epoch 6 train acc: 93.21 train loss: 0.0029 val acc: 88.20 val loss: 0.0061\n",
      "Train Loss: 0.0029, Train Acc: 93.21  Validation Loss: 0.0061, Validation Acc: 88.20\n"
     ]
    }
   ],
   "source": [
    "train_loss_df = pd.DataFrame()\n",
    "train_acc_df = pd.DataFrame()\n",
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_acc_df = pd.DataFrame()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    train_loss, train_acc, validation_loss, validation_acc, test_loss, test_acc = \\\n",
    "        base_learn(net, trainset, train_idx_df[i].to_list(), val_idx_df[i].to_list())\n",
    "    train_loss_df[i] = train_loss\n",
    "    train_acc_df[i] = train_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    test_loss_df[i] = test_loss\n",
    "    test_acc_df[i] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eff8eea3-8884-495c-8bff-d5218690cb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T21:19:00.131035Z",
     "iopub.status.busy": "2023-01-08T21:19:00.130658Z",
     "iopub.status.idle": "2023-01-08T21:19:00.886375Z",
     "shell.execute_reply": "2023-01-08T21:19:00.885117Z",
     "shell.execute_reply.started": "2023-01-08T21:19:00.131008Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_df.to_csv(\"train_loss_fashion_whole.csv\")\n",
    "train_acc_df.to_csv(\"train_acc_fashion_whole.csv\")\n",
    "val_loss_df.to_csv(\"val_loss_fashion_whole.csv\")\n",
    "val_acc_df.to_csv(\"val_acc_fashion_whole.csv\")\n",
    "test_loss_df.to_csv(\"test_loss_fashion_whole.csv\")\n",
    "test_acc_df.to_csv(\"test_acc_fashion_whole.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98986e70-9598-4147-ad3a-b16287774971",
   "metadata": {},
   "source": [
    "Generate first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27948770-75ee-4dee-a47a-15887fcebc79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T17:41:06.368734Z",
     "iopub.status.busy": "2023-01-11T17:41:06.368212Z",
     "iopub.status.idle": "2023-01-11T17:41:10.259498Z",
     "shell.execute_reply": "2023-01-11T17:41:10.258973Z",
     "shell.execute_reply.started": "2023-01-11T17:41:06.368692Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    train_idx = train_idx_df[i].to_list()\n",
    "    batch_idx, train_idx = generate_random_sample(train_idx, 4*2250)\n",
    "    batch_df[i] = batch_idx\n",
    "    train_df[i] = train_idx\n",
    "batch_df.to_csv(\"first_batch_idx.csv\", index=False)\n",
    "train_df.to_csv(\"other_train_idx.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a30cc7-22b4-4863-82db-bab102351949",
   "metadata": {},
   "source": [
    "Load first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda46c33-4363-4358-b936-7252a1dfac1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:46:05.355591Z",
     "iopub.status.busy": "2023-01-12T06:46:05.355290Z",
     "iopub.status.idle": "2023-01-12T06:46:05.434902Z",
     "shell.execute_reply": "2023-01-12T06:46:05.434202Z",
     "shell.execute_reply.started": "2023-01-12T06:46:05.355565Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_df = pd.read_csv(\"first_batch_idx.csv\")\n",
    "train_df = pd.read_csv(\"other_train_idx.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fabfb-fe40-4538-aa96-da8cc7036e1a",
   "metadata": {},
   "source": [
    "### WybÃ³r losowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f271308-0929-4710-b263-3fb5f1d947ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T17:41:31.514301Z",
     "iopub.status.busy": "2023-01-11T17:41:31.513396Z",
     "iopub.status.idle": "2023-01-11T20:33:26.784351Z",
     "shell.execute_reply": "2023-01-11T20:33:26.783669Z",
     "shell.execute_reply.started": "2023-01-11T17:41:31.514292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.57 train loss: 0.0142 val acc: 81.00 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 84.49 train loss: 0.0070 val acc: 85.27 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 88.83 train loss: 0.0049 val acc: 85.70 val loss: 0.0066\n",
      "Sub epoch 3 train acc: 91.09 train loss: 0.0038 val acc: 84.32 val loss: 0.0079\n",
      "Sub epoch 4 train acc: 91.09 train loss: 0.0039 val acc: 86.37 val loss: 0.0072\n",
      "Sub epoch 5 train acc: 92.92 train loss: 0.0031 val acc: 85.90 val loss: 0.0078\n",
      "Sub epoch 6 train acc: 93.83 train loss: 0.0028 val acc: 86.07 val loss: 0.0085\n",
      "Train Loss: 0.0028, Train Acc: 93.83  Validation Loss: 0.0085, Validation Acc: 86.07\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 81.04 train loss: 0.0141 val acc: 82.40 val loss: 0.0098\n",
      "Sub epoch 1 train acc: 90.75 train loss: 0.0042 val acc: 87.52 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 94.98 train loss: 0.0022 val acc: 87.77 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 96.41 train loss: 0.0015 val acc: 88.23 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 97.01 train loss: 0.0014 val acc: 88.05 val loss: 0.0088\n",
      "Sub epoch 5 train acc: 97.05 train loss: 0.0013 val acc: 86.67 val loss: 0.0103\n",
      "Sub epoch 6 train acc: 98.59 train loss: 0.0007 val acc: 88.12 val loss: 0.0091\n",
      "Train Loss: 0.0007, Train Acc: 98.59  Validation Loss: 0.0091, Validation Acc: 88.12\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 79.53 train loss: 0.0163 val acc: 84.38 val loss: 0.0083\n",
      "Sub epoch 1 train acc: 92.38 train loss: 0.0036 val acc: 88.32 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 95.88 train loss: 0.0018 val acc: 86.87 val loss: 0.0089\n",
      "Sub epoch 3 train acc: 97.16 train loss: 0.0013 val acc: 87.70 val loss: 0.0091\n",
      "Sub epoch 4 train acc: 97.87 train loss: 0.0009 val acc: 88.23 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 97.89 train loss: 0.0009 val acc: 87.77 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 98.64 train loss: 0.0006 val acc: 87.53 val loss: 0.0121\n",
      "Train Loss: 0.0006, Train Acc: 98.64  Validation Loss: 0.0121, Validation Acc: 87.53\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 80.21 train loss: 0.0155 val acc: 84.50 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 92.95 train loss: 0.0033 val acc: 88.25 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 96.48 train loss: 0.0017 val acc: 88.33 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 98.51 train loss: 0.0008 val acc: 88.90 val loss: 0.0077\n",
      "Sub epoch 4 train acc: 98.95 train loss: 0.0006 val acc: 88.93 val loss: 0.0081\n",
      "Sub epoch 5 train acc: 99.11 train loss: 0.0005 val acc: 88.97 val loss: 0.0085\n",
      "Sub epoch 6 train acc: 99.29 train loss: 0.0004 val acc: 88.95 val loss: 0.0089\n",
      "Train Loss: 0.0004, Train Acc: 99.29  Validation Loss: 0.0089, Validation Acc: 88.95\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 79.56 train loss: 0.0144 val acc: 86.77 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 93.07 train loss: 0.0032 val acc: 88.27 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 96.07 train loss: 0.0018 val acc: 88.52 val loss: 0.0078\n",
      "Sub epoch 3 train acc: 97.43 train loss: 0.0012 val acc: 88.78 val loss: 0.0085\n",
      "Sub epoch 4 train acc: 97.73 train loss: 0.0010 val acc: 88.43 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 98.19 train loss: 0.0008 val acc: 87.97 val loss: 0.0103\n",
      "Sub epoch 6 train acc: 98.34 train loss: 0.0008 val acc: 87.55 val loss: 0.0126\n",
      "Train Loss: 0.0008, Train Acc: 98.34  Validation Loss: 0.0126, Validation Acc: 87.55\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 79.89 train loss: 0.0146 val acc: 86.52 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 94.51 train loss: 0.0026 val acc: 88.78 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 97.15 train loss: 0.0013 val acc: 88.93 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 98.05 train loss: 0.0009 val acc: 88.92 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 98.61 train loss: 0.0006 val acc: 89.12 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 98.73 train loss: 0.0006 val acc: 89.10 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 99.52 train loss: 0.0003 val acc: 89.42 val loss: 0.0108\n",
      "Train Loss: 0.0003, Train Acc: 99.52  Validation Loss: 0.0108, Validation Acc: 89.42\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 80.12 train loss: 0.0135 val acc: 86.57 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 95.16 train loss: 0.0024 val acc: 88.58 val loss: 0.0080\n",
      "Sub epoch 2 train acc: 97.58 train loss: 0.0012 val acc: 88.97 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 98.48 train loss: 0.0008 val acc: 88.32 val loss: 0.0102\n",
      "Sub epoch 4 train acc: 98.88 train loss: 0.0005 val acc: 88.75 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 99.17 train loss: 0.0004 val acc: 88.83 val loss: 0.0118\n",
      "Sub epoch 6 train acc: 98.40 train loss: 0.0007 val acc: 88.55 val loss: 0.0120\n",
      "Train Loss: 0.0007, Train Acc: 98.40  Validation Loss: 0.0120, Validation Acc: 88.55\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 80.18 train loss: 0.0136 val acc: 85.85 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 95.24 train loss: 0.0023 val acc: 88.70 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 97.78 train loss: 0.0011 val acc: 88.78 val loss: 0.0080\n",
      "Sub epoch 3 train acc: 99.14 train loss: 0.0005 val acc: 89.33 val loss: 0.0086\n",
      "Sub epoch 4 train acc: 99.38 train loss: 0.0004 val acc: 89.08 val loss: 0.0090\n",
      "Sub epoch 5 train acc: 99.52 train loss: 0.0003 val acc: 89.13 val loss: 0.0094\n",
      "Sub epoch 6 train acc: 99.58 train loss: 0.0003 val acc: 89.28 val loss: 0.0098\n",
      "Train Loss: 0.0003, Train Acc: 99.58  Validation Loss: 0.0098, Validation Acc: 89.28\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 79.43 train loss: 0.0130 val acc: 86.52 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 95.23 train loss: 0.0023 val acc: 88.83 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 97.52 train loss: 0.0012 val acc: 88.88 val loss: 0.0080\n",
      "Sub epoch 3 train acc: 98.14 train loss: 0.0009 val acc: 89.33 val loss: 0.0089\n",
      "Sub epoch 4 train acc: 98.53 train loss: 0.0007 val acc: 88.88 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 98.91 train loss: 0.0005 val acc: 88.07 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 98.83 train loss: 0.0006 val acc: 88.92 val loss: 0.0111\n",
      "Train Loss: 0.0006, Train Acc: 98.83  Validation Loss: 0.0111, Validation Acc: 88.92\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 80.32 train loss: 0.0130 val acc: 86.42 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 95.93 train loss: 0.0020 val acc: 88.93 val loss: 0.0079\n",
      "Sub epoch 2 train acc: 97.78 train loss: 0.0011 val acc: 88.75 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 98.60 train loss: 0.0007 val acc: 88.93 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 98.92 train loss: 0.0005 val acc: 89.23 val loss: 0.0114\n",
      "Sub epoch 5 train acc: 98.90 train loss: 0.0005 val acc: 88.70 val loss: 0.0133\n",
      "Sub epoch 6 train acc: 99.01 train loss: 0.0005 val acc: 88.80 val loss: 0.0125\n",
      "Train Loss: 0.0005, Train Acc: 99.01  Validation Loss: 0.0125, Validation Acc: 88.80\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 81.24 train loss: 0.0118 val acc: 87.28 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 96.85 train loss: 0.0017 val acc: 88.97 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 98.55 train loss: 0.0008 val acc: 89.02 val loss: 0.0098\n",
      "Sub epoch 3 train acc: 98.94 train loss: 0.0005 val acc: 89.10 val loss: 0.0099\n",
      "Sub epoch 4 train acc: 99.56 train loss: 0.0003 val acc: 89.43 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 99.77 train loss: 0.0002 val acc: 89.47 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 99.82 train loss: 0.0001 val acc: 89.47 val loss: 0.0119\n",
      "Train Loss: 0.0001, Train Acc: 99.82  Validation Loss: 0.0119, Validation Acc: 89.47\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 80.34 train loss: 0.0131 val acc: 87.67 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 96.10 train loss: 0.0020 val acc: 89.43 val loss: 0.0077\n",
      "Sub epoch 2 train acc: 98.17 train loss: 0.0010 val acc: 89.08 val loss: 0.0087\n",
      "Sub epoch 3 train acc: 98.63 train loss: 0.0007 val acc: 89.58 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 98.91 train loss: 0.0005 val acc: 88.72 val loss: 0.0111\n",
      "Sub epoch 5 train acc: 98.86 train loss: 0.0005 val acc: 88.92 val loss: 0.0122\n",
      "Sub epoch 6 train acc: 98.86 train loss: 0.0006 val acc: 88.73 val loss: 0.0121\n",
      "Train Loss: 0.0006, Train Acc: 98.86  Validation Loss: 0.0121, Validation Acc: 88.73\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 81.79 train loss: 0.0119 val acc: 87.05 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 94.90 train loss: 0.0025 val acc: 89.67 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 96.91 train loss: 0.0016 val acc: 89.68 val loss: 0.0078\n",
      "Sub epoch 3 train acc: 97.86 train loss: 0.0012 val acc: 89.40 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 98.34 train loss: 0.0009 val acc: 89.38 val loss: 0.0084\n",
      "Sub epoch 5 train acc: 98.72 train loss: 0.0007 val acc: 89.60 val loss: 0.0088\n",
      "Sub epoch 6 train acc: 99.01 train loss: 0.0006 val acc: 89.45 val loss: 0.0091\n",
      "Train Loss: 0.0006, Train Acc: 99.01  Validation Loss: 0.0091, Validation Acc: 89.45\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 80.32 train loss: 0.0133 val acc: 87.85 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 95.80 train loss: 0.0021 val acc: 89.00 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 97.82 train loss: 0.0012 val acc: 89.28 val loss: 0.0078\n",
      "Sub epoch 3 train acc: 98.35 train loss: 0.0008 val acc: 89.50 val loss: 0.0089\n",
      "Sub epoch 4 train acc: 98.61 train loss: 0.0007 val acc: 89.23 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 98.79 train loss: 0.0006 val acc: 89.45 val loss: 0.0106\n",
      "Sub epoch 6 train acc: 98.71 train loss: 0.0006 val acc: 89.33 val loss: 0.0106\n",
      "Train Loss: 0.0006, Train Acc: 98.71  Validation Loss: 0.0106, Validation Acc: 89.33\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 81.64 train loss: 0.0118 val acc: 88.40 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 97.20 train loss: 0.0016 val acc: 89.65 val loss: 0.0079\n",
      "Sub epoch 2 train acc: 98.58 train loss: 0.0008 val acc: 89.53 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 98.93 train loss: 0.0006 val acc: 89.55 val loss: 0.0101\n",
      "Sub epoch 4 train acc: 98.94 train loss: 0.0005 val acc: 88.85 val loss: 0.0113\n",
      "Sub epoch 5 train acc: 99.47 train loss: 0.0003 val acc: 89.98 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 99.78 train loss: 0.0001 val acc: 90.02 val loss: 0.0119\n",
      "Train Loss: 0.0001, Train Acc: 99.78  Validation Loss: 0.0119, Validation Acc: 90.02\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 80.73 train loss: 0.0125 val acc: 86.48 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 96.79 train loss: 0.0017 val acc: 89.25 val loss: 0.0075\n",
      "Sub epoch 2 train acc: 98.46 train loss: 0.0008 val acc: 89.67 val loss: 0.0099\n",
      "Sub epoch 3 train acc: 98.82 train loss: 0.0006 val acc: 89.85 val loss: 0.0105\n",
      "Sub epoch 4 train acc: 99.04 train loss: 0.0005 val acc: 89.37 val loss: 0.0110\n",
      "Sub epoch 5 train acc: 99.02 train loss: 0.0005 val acc: 89.32 val loss: 0.0119\n",
      "Sub epoch 6 train acc: 98.91 train loss: 0.0005 val acc: 89.47 val loss: 0.0117\n",
      "Train Loss: 0.0005, Train Acc: 98.91  Validation Loss: 0.0117, Validation Acc: 89.47\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 80.76 train loss: 0.0122 val acc: 86.38 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 97.02 train loss: 0.0015 val acc: 89.90 val loss: 0.0084\n",
      "Sub epoch 2 train acc: 98.97 train loss: 0.0006 val acc: 90.18 val loss: 0.0089\n",
      "Sub epoch 3 train acc: 99.21 train loss: 0.0005 val acc: 90.25 val loss: 0.0092\n",
      "Sub epoch 4 train acc: 99.33 train loss: 0.0004 val acc: 90.23 val loss: 0.0095\n",
      "Sub epoch 5 train acc: 99.44 train loss: 0.0003 val acc: 90.23 val loss: 0.0099\n",
      "Sub epoch 6 train acc: 99.53 train loss: 0.0003 val acc: 90.32 val loss: 0.0105\n",
      "Train Loss: 0.0003, Train Acc: 99.53  Validation Loss: 0.0105, Validation Acc: 90.32\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 79.63 train loss: 0.0127 val acc: 88.03 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 96.39 train loss: 0.0018 val acc: 89.85 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 98.12 train loss: 0.0010 val acc: 90.17 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.71 train loss: 0.0006 val acc: 89.60 val loss: 0.0102\n",
      "Sub epoch 4 train acc: 98.82 train loss: 0.0006 val acc: 89.53 val loss: 0.0118\n",
      "Sub epoch 5 train acc: 98.70 train loss: 0.0006 val acc: 89.42 val loss: 0.0132\n",
      "Sub epoch 6 train acc: 99.46 train loss: 0.0003 val acc: 90.12 val loss: 0.0126\n",
      "Train Loss: 0.0003, Train Acc: 99.46  Validation Loss: 0.0126, Validation Acc: 90.12\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 80.15 train loss: 0.0124 val acc: 86.78 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 96.92 train loss: 0.0017 val acc: 90.08 val loss: 0.0077\n",
      "Sub epoch 2 train acc: 98.61 train loss: 0.0008 val acc: 89.87 val loss: 0.0095\n",
      "Sub epoch 3 train acc: 98.85 train loss: 0.0006 val acc: 90.22 val loss: 0.0113\n",
      "Sub epoch 4 train acc: 98.97 train loss: 0.0006 val acc: 89.83 val loss: 0.0120\n",
      "Sub epoch 5 train acc: 98.77 train loss: 0.0007 val acc: 89.43 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 98.89 train loss: 0.0006 val acc: 89.55 val loss: 0.0117\n",
      "Train Loss: 0.0006, Train Acc: 98.89  Validation Loss: 0.0117, Validation Acc: 89.55\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 80.82 train loss: 0.0123 val acc: 87.33 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 97.61 train loss: 0.0013 val acc: 89.97 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.94 train loss: 0.0006 val acc: 89.90 val loss: 0.0093\n",
      "Sub epoch 3 train acc: 99.50 train loss: 0.0003 val acc: 90.15 val loss: 0.0099\n",
      "Sub epoch 4 train acc: 99.61 train loss: 0.0002 val acc: 90.25 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 99.66 train loss: 0.0002 val acc: 90.00 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 99.71 train loss: 0.0002 val acc: 90.13 val loss: 0.0113\n",
      "Train Loss: 0.0002, Train Acc: 99.71  Validation Loss: 0.0113, Validation Acc: 90.13\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.48 train loss: 0.0139 val acc: 79.30 val loss: 0.0098\n",
      "Sub epoch 1 train acc: 84.52 train loss: 0.0068 val acc: 83.18 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 88.77 train loss: 0.0048 val acc: 83.97 val loss: 0.0073\n",
      "Sub epoch 3 train acc: 90.04 train loss: 0.0041 val acc: 85.98 val loss: 0.0071\n",
      "Sub epoch 4 train acc: 92.19 train loss: 0.0036 val acc: 84.23 val loss: 0.0087\n",
      "Sub epoch 5 train acc: 93.79 train loss: 0.0027 val acc: 86.42 val loss: 0.0083\n",
      "Sub epoch 6 train acc: 94.63 train loss: 0.0024 val acc: 86.73 val loss: 0.0087\n",
      "Train Loss: 0.0024, Train Acc: 94.63  Validation Loss: 0.0087, Validation Acc: 86.73\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 79.93 train loss: 0.0152 val acc: 83.20 val loss: 0.0092\n",
      "Sub epoch 1 train acc: 91.20 train loss: 0.0042 val acc: 86.88 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 94.92 train loss: 0.0022 val acc: 86.98 val loss: 0.0074\n",
      "Sub epoch 3 train acc: 96.09 train loss: 0.0017 val acc: 86.60 val loss: 0.0086\n",
      "Sub epoch 4 train acc: 97.49 train loss: 0.0012 val acc: 86.48 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 97.78 train loss: 0.0010 val acc: 86.90 val loss: 0.0099\n",
      "Sub epoch 6 train acc: 97.59 train loss: 0.0010 val acc: 87.08 val loss: 0.0110\n",
      "Train Loss: 0.0010, Train Acc: 97.59  Validation Loss: 0.0110, Validation Acc: 87.08\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 79.21 train loss: 0.0168 val acc: 84.85 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 93.01 train loss: 0.0035 val acc: 87.18 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 96.24 train loss: 0.0017 val acc: 86.82 val loss: 0.0075\n",
      "Sub epoch 3 train acc: 97.48 train loss: 0.0012 val acc: 87.33 val loss: 0.0089\n",
      "Sub epoch 4 train acc: 99.03 train loss: 0.0005 val acc: 87.93 val loss: 0.0088\n",
      "Sub epoch 5 train acc: 99.44 train loss: 0.0003 val acc: 87.93 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 99.61 train loss: 0.0003 val acc: 87.93 val loss: 0.0095\n",
      "Train Loss: 0.0003, Train Acc: 99.61  Validation Loss: 0.0095, Validation Acc: 87.93\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 79.03 train loss: 0.0167 val acc: 85.03 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 92.19 train loss: 0.0037 val acc: 87.45 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 95.71 train loss: 0.0020 val acc: 87.55 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 97.28 train loss: 0.0013 val acc: 88.05 val loss: 0.0077\n",
      "Sub epoch 4 train acc: 97.77 train loss: 0.0010 val acc: 87.43 val loss: 0.0089\n",
      "Sub epoch 5 train acc: 98.25 train loss: 0.0008 val acc: 87.20 val loss: 0.0097\n",
      "Sub epoch 6 train acc: 98.55 train loss: 0.0007 val acc: 87.57 val loss: 0.0095\n",
      "Train Loss: 0.0007, Train Acc: 98.55  Validation Loss: 0.0095, Validation Acc: 87.57\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 79.72 train loss: 0.0152 val acc: 85.53 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 93.88 train loss: 0.0029 val acc: 88.05 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 96.94 train loss: 0.0014 val acc: 88.40 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 97.94 train loss: 0.0010 val acc: 88.32 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 98.48 train loss: 0.0007 val acc: 88.08 val loss: 0.0101\n",
      "Sub epoch 5 train acc: 98.72 train loss: 0.0006 val acc: 88.08 val loss: 0.0107\n",
      "Sub epoch 6 train acc: 99.55 train loss: 0.0003 val acc: 88.53 val loss: 0.0104\n",
      "Train Loss: 0.0003, Train Acc: 99.55  Validation Loss: 0.0104, Validation Acc: 88.53\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 79.95 train loss: 0.0152 val acc: 86.18 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 94.64 train loss: 0.0025 val acc: 88.37 val loss: 0.0071\n",
      "Sub epoch 2 train acc: 97.33 train loss: 0.0012 val acc: 88.02 val loss: 0.0083\n",
      "Sub epoch 3 train acc: 98.44 train loss: 0.0007 val acc: 87.72 val loss: 0.0098\n",
      "Sub epoch 4 train acc: 98.66 train loss: 0.0006 val acc: 87.95 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 98.87 train loss: 0.0005 val acc: 87.83 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 98.80 train loss: 0.0006 val acc: 87.95 val loss: 0.0121\n",
      "Train Loss: 0.0006, Train Acc: 98.80  Validation Loss: 0.0121, Validation Acc: 87.95\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 80.24 train loss: 0.0139 val acc: 84.70 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 95.06 train loss: 0.0023 val acc: 88.42 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 97.88 train loss: 0.0011 val acc: 88.32 val loss: 0.0081\n",
      "Sub epoch 3 train acc: 99.11 train loss: 0.0005 val acc: 88.93 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 99.41 train loss: 0.0004 val acc: 88.83 val loss: 0.0086\n",
      "Sub epoch 5 train acc: 99.53 train loss: 0.0003 val acc: 88.85 val loss: 0.0090\n",
      "Sub epoch 6 train acc: 99.63 train loss: 0.0003 val acc: 88.82 val loss: 0.0093\n",
      "Train Loss: 0.0003, Train Acc: 99.63  Validation Loss: 0.0093, Validation Acc: 88.82\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 79.86 train loss: 0.0132 val acc: 86.72 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 95.00 train loss: 0.0024 val acc: 88.40 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 97.29 train loss: 0.0013 val acc: 88.47 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 98.26 train loss: 0.0008 val acc: 88.60 val loss: 0.0092\n",
      "Sub epoch 4 train acc: 98.67 train loss: 0.0007 val acc: 88.65 val loss: 0.0105\n",
      "Sub epoch 5 train acc: 98.69 train loss: 0.0006 val acc: 88.40 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 98.78 train loss: 0.0006 val acc: 88.33 val loss: 0.0105\n",
      "Train Loss: 0.0006, Train Acc: 98.78  Validation Loss: 0.0105, Validation Acc: 88.33\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 80.57 train loss: 0.0131 val acc: 86.48 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 96.01 train loss: 0.0021 val acc: 87.88 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 98.02 train loss: 0.0010 val acc: 88.58 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 98.61 train loss: 0.0007 val acc: 88.23 val loss: 0.0095\n",
      "Sub epoch 4 train acc: 98.86 train loss: 0.0005 val acc: 88.58 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 99.10 train loss: 0.0004 val acc: 88.58 val loss: 0.0124\n",
      "Sub epoch 6 train acc: 98.79 train loss: 0.0006 val acc: 88.33 val loss: 0.0126\n",
      "Train Loss: 0.0006, Train Acc: 98.79  Validation Loss: 0.0126, Validation Acc: 88.33\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 81.16 train loss: 0.0124 val acc: 86.23 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 96.37 train loss: 0.0019 val acc: 88.42 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.35 train loss: 0.0009 val acc: 88.28 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.93 train loss: 0.0006 val acc: 88.78 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 99.56 train loss: 0.0003 val acc: 88.98 val loss: 0.0107\n",
      "Sub epoch 5 train acc: 99.77 train loss: 0.0002 val acc: 88.83 val loss: 0.0112\n",
      "Sub epoch 6 train acc: 99.81 train loss: 0.0001 val acc: 88.92 val loss: 0.0116\n",
      "Train Loss: 0.0001, Train Acc: 99.81  Validation Loss: 0.0116, Validation Acc: 88.92\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 79.87 train loss: 0.0129 val acc: 86.05 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 96.07 train loss: 0.0020 val acc: 88.57 val loss: 0.0084\n",
      "Sub epoch 2 train acc: 98.17 train loss: 0.0010 val acc: 88.58 val loss: 0.0097\n",
      "Sub epoch 3 train acc: 98.67 train loss: 0.0007 val acc: 88.77 val loss: 0.0095\n",
      "Sub epoch 4 train acc: 99.10 train loss: 0.0005 val acc: 88.75 val loss: 0.0114\n",
      "Sub epoch 5 train acc: 98.86 train loss: 0.0005 val acc: 89.05 val loss: 0.0125\n",
      "Sub epoch 6 train acc: 98.92 train loss: 0.0005 val acc: 88.88 val loss: 0.0128\n",
      "Train Loss: 0.0005, Train Acc: 98.92  Validation Loss: 0.0128, Validation Acc: 88.88\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 81.56 train loss: 0.0127 val acc: 87.47 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 94.89 train loss: 0.0026 val acc: 88.95 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 96.63 train loss: 0.0017 val acc: 89.37 val loss: 0.0075\n",
      "Sub epoch 3 train acc: 97.55 train loss: 0.0013 val acc: 89.50 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 98.18 train loss: 0.0010 val acc: 89.48 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 98.67 train loss: 0.0008 val acc: 89.35 val loss: 0.0088\n",
      "Sub epoch 6 train acc: 98.99 train loss: 0.0006 val acc: 89.33 val loss: 0.0092\n",
      "Train Loss: 0.0006, Train Acc: 98.99  Validation Loss: 0.0092, Validation Acc: 89.33\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 80.80 train loss: 0.0118 val acc: 86.97 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 95.87 train loss: 0.0022 val acc: 89.35 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 97.83 train loss: 0.0012 val acc: 88.63 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 98.47 train loss: 0.0008 val acc: 89.02 val loss: 0.0094\n",
      "Sub epoch 4 train acc: 98.78 train loss: 0.0006 val acc: 88.87 val loss: 0.0099\n",
      "Sub epoch 5 train acc: 99.50 train loss: 0.0003 val acc: 89.30 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 99.71 train loss: 0.0002 val acc: 89.22 val loss: 0.0115\n",
      "Train Loss: 0.0002, Train Acc: 99.71  Validation Loss: 0.0115, Validation Acc: 89.22\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 80.95 train loss: 0.0118 val acc: 87.12 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 96.39 train loss: 0.0019 val acc: 88.67 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 98.34 train loss: 0.0009 val acc: 88.63 val loss: 0.0094\n",
      "Sub epoch 3 train acc: 98.75 train loss: 0.0006 val acc: 88.73 val loss: 0.0106\n",
      "Sub epoch 4 train acc: 98.92 train loss: 0.0005 val acc: 88.82 val loss: 0.0124\n",
      "Sub epoch 5 train acc: 98.86 train loss: 0.0006 val acc: 88.25 val loss: 0.0124\n",
      "Sub epoch 6 train acc: 98.76 train loss: 0.0006 val acc: 88.83 val loss: 0.0137\n",
      "Train Loss: 0.0006, Train Acc: 98.76  Validation Loss: 0.0137, Validation Acc: 88.83\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 80.83 train loss: 0.0124 val acc: 87.42 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 97.13 train loss: 0.0015 val acc: 88.92 val loss: 0.0086\n",
      "Sub epoch 2 train acc: 99.04 train loss: 0.0005 val acc: 89.07 val loss: 0.0093\n",
      "Sub epoch 3 train acc: 99.28 train loss: 0.0004 val acc: 88.97 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 99.39 train loss: 0.0004 val acc: 89.13 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 99.52 train loss: 0.0003 val acc: 89.03 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 99.59 train loss: 0.0003 val acc: 88.90 val loss: 0.0112\n",
      "Train Loss: 0.0003, Train Acc: 99.59  Validation Loss: 0.0112, Validation Acc: 88.90\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 78.87 train loss: 0.0130 val acc: 86.30 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 96.16 train loss: 0.0020 val acc: 88.82 val loss: 0.0086\n",
      "Sub epoch 2 train acc: 98.09 train loss: 0.0010 val acc: 89.20 val loss: 0.0093\n",
      "Sub epoch 3 train acc: 98.69 train loss: 0.0007 val acc: 88.75 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 98.82 train loss: 0.0006 val acc: 89.03 val loss: 0.0118\n",
      "Sub epoch 5 train acc: 98.76 train loss: 0.0006 val acc: 88.83 val loss: 0.0133\n",
      "Sub epoch 6 train acc: 99.51 train loss: 0.0003 val acc: 89.45 val loss: 0.0125\n",
      "Train Loss: 0.0003, Train Acc: 99.51  Validation Loss: 0.0125, Validation Acc: 89.45\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 80.66 train loss: 0.0121 val acc: 87.12 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 97.17 train loss: 0.0016 val acc: 89.08 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.61 train loss: 0.0008 val acc: 89.00 val loss: 0.0094\n",
      "Sub epoch 3 train acc: 99.00 train loss: 0.0005 val acc: 88.88 val loss: 0.0111\n",
      "Sub epoch 4 train acc: 99.07 train loss: 0.0005 val acc: 88.77 val loss: 0.0124\n",
      "Sub epoch 5 train acc: 98.83 train loss: 0.0006 val acc: 89.47 val loss: 0.0123\n",
      "Sub epoch 6 train acc: 98.96 train loss: 0.0005 val acc: 89.50 val loss: 0.0134\n",
      "Train Loss: 0.0005, Train Acc: 98.96  Validation Loss: 0.0134, Validation Acc: 89.50\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 80.46 train loss: 0.0120 val acc: 87.47 val loss: 0.0092\n",
      "Sub epoch 1 train acc: 97.36 train loss: 0.0015 val acc: 88.95 val loss: 0.0100\n",
      "Sub epoch 2 train acc: 98.86 train loss: 0.0006 val acc: 89.30 val loss: 0.0114\n",
      "Sub epoch 3 train acc: 99.53 train loss: 0.0003 val acc: 89.82 val loss: 0.0119\n",
      "Sub epoch 4 train acc: 99.66 train loss: 0.0002 val acc: 89.72 val loss: 0.0125\n",
      "Sub epoch 5 train acc: 99.72 train loss: 0.0002 val acc: 89.60 val loss: 0.0131\n",
      "Sub epoch 6 train acc: 99.76 train loss: 0.0002 val acc: 89.68 val loss: 0.0136\n",
      "Train Loss: 0.0002, Train Acc: 99.76  Validation Loss: 0.0136, Validation Acc: 89.68\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 79.02 train loss: 0.0128 val acc: 88.40 val loss: 0.0101\n",
      "Sub epoch 1 train acc: 97.03 train loss: 0.0016 val acc: 89.65 val loss: 0.0101\n",
      "Sub epoch 2 train acc: 98.56 train loss: 0.0008 val acc: 89.40 val loss: 0.0110\n",
      "Sub epoch 3 train acc: 98.89 train loss: 0.0006 val acc: 89.35 val loss: 0.0127\n",
      "Sub epoch 4 train acc: 99.01 train loss: 0.0005 val acc: 88.82 val loss: 0.0135\n",
      "Sub epoch 5 train acc: 98.88 train loss: 0.0006 val acc: 89.43 val loss: 0.0161\n",
      "Sub epoch 6 train acc: 98.92 train loss: 0.0006 val acc: 88.65 val loss: 0.0153\n",
      "Train Loss: 0.0006, Train Acc: 98.92  Validation Loss: 0.0153, Validation Acc: 88.65\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 80.14 train loss: 0.0122 val acc: 87.62 val loss: 0.0087\n",
      "Sub epoch 1 train acc: 97.56 train loss: 0.0014 val acc: 89.13 val loss: 0.0112\n",
      "Sub epoch 2 train acc: 98.83 train loss: 0.0006 val acc: 89.33 val loss: 0.0110\n",
      "Sub epoch 3 train acc: 99.13 train loss: 0.0005 val acc: 89.05 val loss: 0.0137\n",
      "Sub epoch 4 train acc: 99.19 train loss: 0.0004 val acc: 88.97 val loss: 0.0142\n",
      "Sub epoch 5 train acc: 99.08 train loss: 0.0005 val acc: 89.25 val loss: 0.0156\n",
      "Sub epoch 6 train acc: 98.83 train loss: 0.0006 val acc: 88.92 val loss: 0.0165\n",
      "Train Loss: 0.0006, Train Acc: 98.83  Validation Loss: 0.0165, Validation Acc: 88.92\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 73.03 train loss: 0.0139 val acc: 80.20 val loss: 0.0094\n",
      "Sub epoch 1 train acc: 85.00 train loss: 0.0067 val acc: 83.57 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 88.96 train loss: 0.0047 val acc: 83.62 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 90.29 train loss: 0.0042 val acc: 84.70 val loss: 0.0076\n",
      "Sub epoch 4 train acc: 95.18 train loss: 0.0022 val acc: 86.70 val loss: 0.0069\n",
      "Sub epoch 5 train acc: 96.74 train loss: 0.0014 val acc: 86.90 val loss: 0.0077\n",
      "Sub epoch 6 train acc: 97.64 train loss: 0.0011 val acc: 86.72 val loss: 0.0081\n",
      "Train Loss: 0.0011, Train Acc: 97.64  Validation Loss: 0.0081, Validation Acc: 86.72\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 76.71 train loss: 0.0183 val acc: 77.72 val loss: 0.0109\n",
      "Sub epoch 1 train acc: 88.62 train loss: 0.0054 val acc: 86.40 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 92.11 train loss: 0.0034 val acc: 86.22 val loss: 0.0085\n",
      "Sub epoch 3 train acc: 94.15 train loss: 0.0025 val acc: 86.52 val loss: 0.0075\n",
      "Sub epoch 4 train acc: 95.75 train loss: 0.0018 val acc: 86.18 val loss: 0.0091\n",
      "Sub epoch 5 train acc: 96.82 train loss: 0.0015 val acc: 86.30 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 97.16 train loss: 0.0012 val acc: 85.65 val loss: 0.0114\n",
      "Train Loss: 0.0012, Train Acc: 97.16  Validation Loss: 0.0114, Validation Acc: 85.65\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 79.36 train loss: 0.0158 val acc: 83.45 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 91.45 train loss: 0.0039 val acc: 86.78 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 93.83 train loss: 0.0027 val acc: 87.63 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 95.16 train loss: 0.0022 val acc: 87.65 val loss: 0.0071\n",
      "Sub epoch 4 train acc: 96.19 train loss: 0.0018 val acc: 87.37 val loss: 0.0073\n",
      "Sub epoch 5 train acc: 96.87 train loss: 0.0015 val acc: 87.45 val loss: 0.0075\n",
      "Sub epoch 6 train acc: 97.64 train loss: 0.0012 val acc: 87.40 val loss: 0.0078\n",
      "Train Loss: 0.0012, Train Acc: 97.64  Validation Loss: 0.0078, Validation Acc: 87.40\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 77.84 train loss: 0.0163 val acc: 84.30 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 91.03 train loss: 0.0040 val acc: 86.17 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 94.06 train loss: 0.0026 val acc: 87.02 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 95.77 train loss: 0.0018 val acc: 87.00 val loss: 0.0089\n",
      "Sub epoch 4 train acc: 96.94 train loss: 0.0013 val acc: 87.58 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 98.58 train loss: 0.0007 val acc: 88.05 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 99.14 train loss: 0.0005 val acc: 87.97 val loss: 0.0095\n",
      "Train Loss: 0.0005, Train Acc: 99.14  Validation Loss: 0.0095, Validation Acc: 87.97\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 80.12 train loss: 0.0139 val acc: 84.82 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 92.93 train loss: 0.0033 val acc: 87.98 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 95.36 train loss: 0.0020 val acc: 87.25 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 96.96 train loss: 0.0014 val acc: 87.98 val loss: 0.0089\n",
      "Sub epoch 4 train acc: 97.68 train loss: 0.0010 val acc: 87.50 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 98.23 train loss: 0.0008 val acc: 87.85 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 98.47 train loss: 0.0008 val acc: 87.28 val loss: 0.0130\n",
      "Train Loss: 0.0008, Train Acc: 98.47  Validation Loss: 0.0130, Validation Acc: 87.28\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 80.70 train loss: 0.0137 val acc: 85.78 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 94.10 train loss: 0.0028 val acc: 87.67 val loss: 0.0075\n",
      "Sub epoch 2 train acc: 97.53 train loss: 0.0013 val acc: 88.62 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 97.96 train loss: 0.0010 val acc: 88.47 val loss: 0.0078\n",
      "Sub epoch 4 train acc: 98.27 train loss: 0.0009 val acc: 88.60 val loss: 0.0081\n",
      "Sub epoch 5 train acc: 98.55 train loss: 0.0008 val acc: 88.58 val loss: 0.0084\n",
      "Sub epoch 6 train acc: 98.79 train loss: 0.0007 val acc: 88.63 val loss: 0.0087\n",
      "Train Loss: 0.0007, Train Acc: 98.79  Validation Loss: 0.0087, Validation Acc: 88.63\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 79.72 train loss: 0.0131 val acc: 86.28 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 93.75 train loss: 0.0029 val acc: 87.53 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 96.23 train loss: 0.0018 val acc: 88.15 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 97.34 train loss: 0.0012 val acc: 87.88 val loss: 0.0087\n",
      "Sub epoch 4 train acc: 97.82 train loss: 0.0010 val acc: 87.98 val loss: 0.0102\n",
      "Sub epoch 5 train acc: 98.13 train loss: 0.0009 val acc: 87.62 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 99.25 train loss: 0.0004 val acc: 88.62 val loss: 0.0108\n",
      "Train Loss: 0.0004, Train Acc: 99.25  Validation Loss: 0.0108, Validation Acc: 88.62\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 80.68 train loss: 0.0123 val acc: 86.60 val loss: 0.0086\n",
      "Sub epoch 1 train acc: 95.21 train loss: 0.0023 val acc: 87.92 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 97.37 train loss: 0.0012 val acc: 88.65 val loss: 0.0086\n",
      "Sub epoch 3 train acc: 98.27 train loss: 0.0009 val acc: 88.22 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 98.30 train loss: 0.0008 val acc: 87.95 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 98.51 train loss: 0.0007 val acc: 88.33 val loss: 0.0116\n",
      "Sub epoch 6 train acc: 98.70 train loss: 0.0006 val acc: 87.97 val loss: 0.0129\n",
      "Train Loss: 0.0006, Train Acc: 98.70  Validation Loss: 0.0129, Validation Acc: 87.97\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 80.69 train loss: 0.0121 val acc: 86.08 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 95.76 train loss: 0.0021 val acc: 87.52 val loss: 0.0080\n",
      "Sub epoch 2 train acc: 98.09 train loss: 0.0010 val acc: 88.50 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 99.11 train loss: 0.0005 val acc: 88.80 val loss: 0.0095\n",
      "Sub epoch 4 train acc: 99.38 train loss: 0.0004 val acc: 88.57 val loss: 0.0099\n",
      "Sub epoch 5 train acc: 99.54 train loss: 0.0003 val acc: 88.57 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 99.60 train loss: 0.0003 val acc: 88.58 val loss: 0.0106\n",
      "Train Loss: 0.0003, Train Acc: 99.60  Validation Loss: 0.0106, Validation Acc: 88.58\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 79.49 train loss: 0.0125 val acc: 84.63 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 95.19 train loss: 0.0023 val acc: 88.32 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 97.59 train loss: 0.0012 val acc: 88.42 val loss: 0.0084\n",
      "Sub epoch 3 train acc: 98.20 train loss: 0.0009 val acc: 88.15 val loss: 0.0094\n",
      "Sub epoch 4 train acc: 98.45 train loss: 0.0007 val acc: 87.73 val loss: 0.0120\n",
      "Sub epoch 5 train acc: 97.78 train loss: 0.0010 val acc: 88.08 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 98.51 train loss: 0.0007 val acc: 87.88 val loss: 0.0117\n",
      "Train Loss: 0.0007, Train Acc: 98.51  Validation Loss: 0.0117, Validation Acc: 87.88\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 80.96 train loss: 0.0123 val acc: 86.57 val loss: 0.0095\n",
      "Sub epoch 1 train acc: 95.97 train loss: 0.0021 val acc: 88.45 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.04 train loss: 0.0010 val acc: 88.53 val loss: 0.0099\n",
      "Sub epoch 3 train acc: 98.70 train loss: 0.0007 val acc: 88.30 val loss: 0.0119\n",
      "Sub epoch 4 train acc: 98.91 train loss: 0.0005 val acc: 88.07 val loss: 0.0130\n",
      "Sub epoch 5 train acc: 98.70 train loss: 0.0007 val acc: 87.73 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 98.84 train loss: 0.0006 val acc: 88.22 val loss: 0.0135\n",
      "Train Loss: 0.0006, Train Acc: 98.84  Validation Loss: 0.0135, Validation Acc: 88.22\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 81.19 train loss: 0.0116 val acc: 86.03 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 96.71 train loss: 0.0018 val acc: 88.22 val loss: 0.0093\n",
      "Sub epoch 2 train acc: 98.38 train loss: 0.0008 val acc: 88.47 val loss: 0.0099\n",
      "Sub epoch 3 train acc: 98.76 train loss: 0.0006 val acc: 88.20 val loss: 0.0119\n",
      "Sub epoch 4 train acc: 99.53 train loss: 0.0003 val acc: 88.85 val loss: 0.0121\n",
      "Sub epoch 5 train acc: 99.73 train loss: 0.0002 val acc: 88.80 val loss: 0.0124\n",
      "Sub epoch 6 train acc: 99.79 train loss: 0.0001 val acc: 88.78 val loss: 0.0128\n",
      "Train Loss: 0.0001, Train Acc: 99.79  Validation Loss: 0.0128, Validation Acc: 88.78\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 80.42 train loss: 0.0123 val acc: 86.82 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 96.24 train loss: 0.0019 val acc: 88.47 val loss: 0.0099\n",
      "Sub epoch 2 train acc: 98.13 train loss: 0.0009 val acc: 88.48 val loss: 0.0101\n",
      "Sub epoch 3 train acc: 98.65 train loss: 0.0006 val acc: 88.45 val loss: 0.0116\n",
      "Sub epoch 4 train acc: 98.73 train loss: 0.0006 val acc: 88.72 val loss: 0.0125\n",
      "Sub epoch 5 train acc: 98.81 train loss: 0.0006 val acc: 88.40 val loss: 0.0136\n",
      "Sub epoch 6 train acc: 98.72 train loss: 0.0007 val acc: 87.93 val loss: 0.0126\n",
      "Train Loss: 0.0007, Train Acc: 98.72  Validation Loss: 0.0126, Validation Acc: 87.93\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 81.58 train loss: 0.0115 val acc: 87.05 val loss: 0.0087\n",
      "Sub epoch 1 train acc: 95.41 train loss: 0.0023 val acc: 88.65 val loss: 0.0093\n",
      "Sub epoch 2 train acc: 97.06 train loss: 0.0015 val acc: 88.52 val loss: 0.0097\n",
      "Sub epoch 3 train acc: 98.03 train loss: 0.0011 val acc: 88.63 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 98.59 train loss: 0.0008 val acc: 88.67 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 98.90 train loss: 0.0006 val acc: 88.87 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 99.12 train loss: 0.0005 val acc: 88.88 val loss: 0.0118\n",
      "Train Loss: 0.0005, Train Acc: 99.12  Validation Loss: 0.0118, Validation Acc: 88.88\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 81.04 train loss: 0.0115 val acc: 86.12 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 95.96 train loss: 0.0020 val acc: 88.08 val loss: 0.0091\n",
      "Sub epoch 2 train acc: 97.84 train loss: 0.0011 val acc: 89.00 val loss: 0.0101\n",
      "Sub epoch 3 train acc: 98.48 train loss: 0.0008 val acc: 88.80 val loss: 0.0111\n",
      "Sub epoch 4 train acc: 98.62 train loss: 0.0007 val acc: 88.90 val loss: 0.0137\n",
      "Sub epoch 5 train acc: 99.40 train loss: 0.0003 val acc: 89.05 val loss: 0.0134\n",
      "Sub epoch 6 train acc: 99.68 train loss: 0.0002 val acc: 88.92 val loss: 0.0140\n",
      "Train Loss: 0.0002, Train Acc: 99.68  Validation Loss: 0.0140, Validation Acc: 88.92\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 79.49 train loss: 0.0127 val acc: 85.60 val loss: 0.0094\n",
      "Sub epoch 1 train acc: 96.52 train loss: 0.0018 val acc: 88.82 val loss: 0.0095\n",
      "Sub epoch 2 train acc: 98.39 train loss: 0.0008 val acc: 88.05 val loss: 0.0105\n",
      "Sub epoch 3 train acc: 98.75 train loss: 0.0006 val acc: 88.78 val loss: 0.0135\n",
      "Sub epoch 4 train acc: 98.96 train loss: 0.0005 val acc: 88.62 val loss: 0.0137\n",
      "Sub epoch 5 train acc: 98.91 train loss: 0.0005 val acc: 88.52 val loss: 0.0158\n",
      "Sub epoch 6 train acc: 98.68 train loss: 0.0008 val acc: 88.55 val loss: 0.0144\n",
      "Train Loss: 0.0008, Train Acc: 98.68  Validation Loss: 0.0144, Validation Acc: 88.55\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 80.93 train loss: 0.0117 val acc: 86.42 val loss: 0.0083\n",
      "Sub epoch 1 train acc: 97.41 train loss: 0.0014 val acc: 88.75 val loss: 0.0095\n",
      "Sub epoch 2 train acc: 99.09 train loss: 0.0005 val acc: 89.18 val loss: 0.0102\n",
      "Sub epoch 3 train acc: 99.37 train loss: 0.0004 val acc: 89.25 val loss: 0.0108\n",
      "Sub epoch 4 train acc: 99.47 train loss: 0.0003 val acc: 89.23 val loss: 0.0114\n",
      "Sub epoch 5 train acc: 99.54 train loss: 0.0003 val acc: 89.27 val loss: 0.0119\n",
      "Sub epoch 6 train acc: 99.62 train loss: 0.0002 val acc: 89.33 val loss: 0.0126\n",
      "Train Loss: 0.0002, Train Acc: 99.62  Validation Loss: 0.0126, Validation Acc: 89.33\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 79.88 train loss: 0.0125 val acc: 86.70 val loss: 0.0089\n",
      "Sub epoch 1 train acc: 96.32 train loss: 0.0018 val acc: 88.65 val loss: 0.0093\n",
      "Sub epoch 2 train acc: 98.42 train loss: 0.0008 val acc: 89.37 val loss: 0.0112\n",
      "Sub epoch 3 train acc: 98.72 train loss: 0.0006 val acc: 88.50 val loss: 0.0121\n",
      "Sub epoch 4 train acc: 98.88 train loss: 0.0006 val acc: 89.17 val loss: 0.0135\n",
      "Sub epoch 5 train acc: 98.97 train loss: 0.0007 val acc: 89.03 val loss: 0.0134\n",
      "Sub epoch 6 train acc: 99.52 train loss: 0.0003 val acc: 89.45 val loss: 0.0145\n",
      "Train Loss: 0.0003, Train Acc: 99.52  Validation Loss: 0.0145, Validation Acc: 89.45\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 81.11 train loss: 0.0117 val acc: 87.07 val loss: 0.0097\n",
      "Sub epoch 1 train acc: 97.33 train loss: 0.0015 val acc: 89.15 val loss: 0.0097\n",
      "Sub epoch 2 train acc: 98.71 train loss: 0.0007 val acc: 88.92 val loss: 0.0122\n",
      "Sub epoch 3 train acc: 98.97 train loss: 0.0005 val acc: 89.23 val loss: 0.0134\n",
      "Sub epoch 4 train acc: 98.99 train loss: 0.0005 val acc: 88.98 val loss: 0.0137\n",
      "Sub epoch 5 train acc: 98.94 train loss: 0.0006 val acc: 88.50 val loss: 0.0168\n",
      "Sub epoch 6 train acc: 98.83 train loss: 0.0006 val acc: 89.22 val loss: 0.0155\n",
      "Train Loss: 0.0006, Train Acc: 98.83  Validation Loss: 0.0155, Validation Acc: 89.22\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 80.30 train loss: 0.0121 val acc: 86.28 val loss: 0.0085\n",
      "Sub epoch 1 train acc: 97.34 train loss: 0.0014 val acc: 88.93 val loss: 0.0110\n",
      "Sub epoch 2 train acc: 98.81 train loss: 0.0007 val acc: 88.85 val loss: 0.0124\n",
      "Sub epoch 3 train acc: 99.43 train loss: 0.0003 val acc: 89.20 val loss: 0.0131\n",
      "Sub epoch 4 train acc: 99.60 train loss: 0.0002 val acc: 89.10 val loss: 0.0133\n",
      "Sub epoch 5 train acc: 99.66 train loss: 0.0002 val acc: 89.02 val loss: 0.0141\n",
      "Sub epoch 6 train acc: 99.71 train loss: 0.0002 val acc: 89.07 val loss: 0.0149\n",
      "Train Loss: 0.0002, Train Acc: 99.71  Validation Loss: 0.0149, Validation Acc: 89.07\n"
     ]
    }
   ],
   "source": [
    "train_loss_df = pd.DataFrame()\n",
    "train_acc_df = pd.DataFrame()\n",
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    train_loss, train_acc, validation_loss, validation_acc = \\\n",
    "        active_learn(net, trainset, train_df[i].to_list(), val_idx_df[i].to_list(), \n",
    "                     heuristic=None, initial_train_idx=batch_df[i].to_list(),\n",
    "                     experiment_id=i)\n",
    "    train_loss_df[i] = train_loss\n",
    "    train_acc_df[i] = train_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8fc4fbd-2fc0-40c3-9954-03f5defc67b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T20:33:26.785984Z",
     "iopub.status.busy": "2023-01-11T20:33:26.785794Z",
     "iopub.status.idle": "2023-01-11T20:33:27.448645Z",
     "shell.execute_reply": "2023-01-11T20:33:27.448099Z",
     "shell.execute_reply.started": "2023-01-11T20:33:26.785967Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_df.to_csv(\"t_loss_random.csv\",index=False)\n",
    "train_acc_df.to_csv(\"t_acc_random.csv\",index=False)\n",
    "val_loss_df.to_csv(\"v_loss_random.csv\",index=False)\n",
    "val_acc_df.to_csv(\"v_acc_random.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e7fbe54-e82b-48e9-8417-7d6a364e840b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T20:33:27.451787Z",
     "iopub.status.busy": "2023-01-11T20:33:27.451616Z",
     "iopub.status.idle": "2023-01-11T20:39:03.595150Z",
     "shell.execute_reply": "2023-01-11T20:39:03.594255Z",
     "shell.execute_reply.started": "2023-01-11T20:33:27.451772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.44\n",
      "86.97\n",
      "86.48\n",
      "87.48\n",
      "86.18\n",
      "88.13\n",
      "87.6\n",
      "88.39\n",
      "87.86\n",
      "87.64\n",
      "88.59\n",
      "87.8\n",
      "89.09\n",
      "88.26\n",
      "88.97\n",
      "88.7\n",
      "89.49\n",
      "89.16\n",
      "88.95\n",
      "89.49\n",
      "[0.009525631396472454, 0.010316131988167762, 0.013447535765171052, 0.009738220472633839, 0.013471759486198425, 0.012005363115295768, 0.012922690609097481, 0.011377813646942377, 0.012481050392985344, 0.013728470582515001, 0.012797351814806462, 0.014021091724187135, 0.010831553285755217, 0.012995947771705686, 0.01398697234094143, 0.013313394506275654, 0.011538404696807265, 0.01488361993022263, 0.015001903466880321, 0.013418179763108492] [85.44, 86.97, 86.48, 87.48, 86.18, 88.13, 87.6, 88.39, 87.86, 87.64, 88.59, 87.8, 89.09, 88.26, 88.97, 88.7, 89.49, 89.16, 88.95, 89.49]\n",
      "85.9\n",
      "86.41\n",
      "87.66\n",
      "86.63\n",
      "87.42\n",
      "87.67\n",
      "88.38\n",
      "87.93\n",
      "87.98\n",
      "89.11\n",
      "88.5\n",
      "88.78\n",
      "89.0\n",
      "88.19\n",
      "88.82\n",
      "88.86\n",
      "88.89\n",
      "89.28\n",
      "88.85\n",
      "88.58\n",
      "[0.009107118813693523, 0.011975440914928913, 0.010064474602788687, 0.010730746471881866, 0.011462384861707687, 0.012580435878783464, 0.010085494581609964, 0.011069421184062957, 0.012049169044941663, 0.011530605384707451, 0.012992315131984651, 0.009129608130082487, 0.011535733450204134, 0.01380887254625559, 0.011544059575721622, 0.012538534700497985, 0.014047032283246517, 0.012799143129587173, 0.01406397006958723, 0.01520099928304553] [85.9, 86.41, 87.66, 86.63, 87.42, 87.67, 88.38, 87.93, 87.98, 89.11, 88.5, 88.78, 89.0, 88.19, 88.82, 88.86, 88.89, 89.28, 88.85, 88.58]\n",
      "87.36\n",
      "85.58\n",
      "87.54\n",
      "87.78\n",
      "87.18\n",
      "88.02\n",
      "88.14\n",
      "87.83\n",
      "88.83\n",
      "87.8\n",
      "88.14\n",
      "88.7\n",
      "87.73\n",
      "88.99\n",
      "89.12\n",
      "88.35\n",
      "89.25\n",
      "89.18\n",
      "88.98\n",
      "89.35\n",
      "[0.007714814388006926, 0.01118344346061349, 0.007543753385543823, 0.008935322318226099, 0.011839217099547387, 0.00893459637723863, 0.010657294343411922, 0.013042339085787535, 0.010519961851090193, 0.011594934426248073, 0.013232811246067285, 0.012191978428512811, 0.012533757942914963, 0.01171373826265335, 0.012427120280265809, 0.01434984267950058, 0.012539058526605367, 0.013786823892965913, 0.014363160099647939, 0.014075907235778869] [87.36, 85.58, 87.54, 87.78, 87.18, 88.02, 88.14, 87.83, 88.83, 87.8, 88.14, 88.7, 87.73, 88.99, 89.12, 88.35, 89.25, 89.18, 88.98, 89.35]\n"
     ]
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_acc_df = pd.DataFrame()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(trainset, val_idx_df[i]),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    for k in range(20):\n",
    "        net.load_state_dict(torch.load(f\"{i}/epoch{k}_random.pt\"))\n",
    "        \n",
    "        v_loss, v_acc = validate(net, val_loader)\n",
    "        t_loss, t_acc = validate(net, test_loader)\n",
    "        validation_loss.append(v_loss)\n",
    "        validation_acc.append(v_acc)\n",
    "        test_loss.append(t_loss)\n",
    "        test_acc.append(t_acc)\n",
    "        print(t_acc)\n",
    "\n",
    "    test_loss_df[i] = test_loss\n",
    "    test_acc_df[i] = test_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c924b9e-9bcc-4a8f-91a8-8e130fa4ec34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T20:39:03.596392Z",
     "iopub.status.busy": "2023-01-11T20:39:03.596181Z",
     "iopub.status.idle": "2023-01-11T20:39:08.009216Z",
     "shell.execute_reply": "2023-01-11T20:39:08.008586Z",
     "shell.execute_reply.started": "2023-01-11T20:39:03.596374Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.to_csv(\"val_loss_random.csv\",index=False)\n",
    "val_acc_df.to_csv(\"val_acc_random.csv\",index=False)\n",
    "test_loss_df.to_csv(\"test_loss_random.csv\",index=False)\n",
    "test_acc_df.to_csv(\"test_acc_random.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451191a2-3aee-4a08-ae1e-9d3261018dfd",
   "metadata": {},
   "source": [
    "### Najmniejsza pewnoÅ›Ä‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19f5cddd-833c-49e3-8d0a-31db0fb7b64e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T20:39:08.010489Z",
     "iopub.status.busy": "2023-01-11T20:39:08.010280Z",
     "iopub.status.idle": "2023-01-11T23:37:11.653605Z",
     "shell.execute_reply": "2023-01-11T23:37:11.652967Z",
     "shell.execute_reply.started": "2023-01-11T20:39:08.010469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.59 train loss: 0.0141 val acc: 81.75 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 84.54 train loss: 0.0068 val acc: 83.28 val loss: 0.0083\n",
      "Sub epoch 2 train acc: 88.42 train loss: 0.0051 val acc: 85.12 val loss: 0.0068\n",
      "Sub epoch 3 train acc: 90.83 train loss: 0.0040 val acc: 86.22 val loss: 0.0070\n",
      "Sub epoch 4 train acc: 91.83 train loss: 0.0035 val acc: 86.42 val loss: 0.0072\n",
      "Sub epoch 5 train acc: 92.90 train loss: 0.0032 val acc: 86.22 val loss: 0.0084\n",
      "Sub epoch 6 train acc: 93.57 train loss: 0.0028 val acc: 84.82 val loss: 0.0102\n",
      "Train Loss: 0.0028, Train Acc: 93.57  Validation Loss: 0.0102, Validation Acc: 84.82\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 73.39 train loss: 0.0169 val acc: 84.05 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 86.44 train loss: 0.0061 val acc: 87.75 val loss: 0.0061\n",
      "Sub epoch 2 train acc: 91.19 train loss: 0.0037 val acc: 87.83 val loss: 0.0080\n",
      "Sub epoch 3 train acc: 93.05 train loss: 0.0029 val acc: 88.33 val loss: 0.0076\n",
      "Sub epoch 4 train acc: 95.22 train loss: 0.0021 val acc: 88.00 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 95.33 train loss: 0.0021 val acc: 88.30 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 96.29 train loss: 0.0017 val acc: 87.05 val loss: 0.0106\n",
      "Train Loss: 0.0017, Train Acc: 96.29  Validation Loss: 0.0106, Validation Acc: 87.05\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 69.87 train loss: 0.0187 val acc: 84.82 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 86.67 train loss: 0.0058 val acc: 88.45 val loss: 0.0063\n",
      "Sub epoch 2 train acc: 91.35 train loss: 0.0036 val acc: 87.55 val loss: 0.0080\n",
      "Sub epoch 3 train acc: 93.64 train loss: 0.0027 val acc: 88.98 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 95.28 train loss: 0.0020 val acc: 87.98 val loss: 0.0101\n",
      "Sub epoch 5 train acc: 95.91 train loss: 0.0018 val acc: 88.35 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 98.43 train loss: 0.0008 val acc: 89.33 val loss: 0.0096\n",
      "Train Loss: 0.0008, Train Acc: 98.43  Validation Loss: 0.0096, Validation Acc: 89.33\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 67.08 train loss: 0.0188 val acc: 86.42 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 86.00 train loss: 0.0060 val acc: 89.27 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 91.56 train loss: 0.0037 val acc: 89.93 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 94.17 train loss: 0.0026 val acc: 89.02 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 95.54 train loss: 0.0020 val acc: 89.27 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 95.77 train loss: 0.0019 val acc: 89.02 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 96.53 train loss: 0.0015 val acc: 89.35 val loss: 0.0112\n",
      "Train Loss: 0.0015, Train Acc: 96.53  Validation Loss: 0.0112, Validation Acc: 89.35\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 66.86 train loss: 0.0189 val acc: 86.97 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 89.21 train loss: 0.0050 val acc: 89.12 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 93.86 train loss: 0.0028 val acc: 89.92 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 97.25 train loss: 0.0014 val acc: 90.67 val loss: 0.0086\n",
      "Sub epoch 4 train acc: 97.92 train loss: 0.0010 val acc: 90.63 val loss: 0.0090\n",
      "Sub epoch 5 train acc: 98.31 train loss: 0.0009 val acc: 90.65 val loss: 0.0095\n",
      "Sub epoch 6 train acc: 98.60 train loss: 0.0007 val acc: 90.60 val loss: 0.0099\n",
      "Train Loss: 0.0007, Train Acc: 98.60  Validation Loss: 0.0099, Validation Acc: 90.60\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 65.74 train loss: 0.0181 val acc: 87.53 val loss: 0.0058\n",
      "Sub epoch 1 train acc: 88.67 train loss: 0.0051 val acc: 89.70 val loss: 0.0059\n",
      "Sub epoch 2 train acc: 92.84 train loss: 0.0032 val acc: 89.97 val loss: 0.0071\n",
      "Sub epoch 3 train acc: 94.68 train loss: 0.0024 val acc: 90.12 val loss: 0.0079\n",
      "Sub epoch 4 train acc: 95.47 train loss: 0.0020 val acc: 90.08 val loss: 0.0082\n",
      "Sub epoch 5 train acc: 96.35 train loss: 0.0016 val acc: 89.73 val loss: 0.0100\n",
      "Sub epoch 6 train acc: 96.62 train loss: 0.0015 val acc: 90.10 val loss: 0.0109\n",
      "Train Loss: 0.0015, Train Acc: 96.62  Validation Loss: 0.0109, Validation Acc: 90.10\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 67.35 train loss: 0.0177 val acc: 87.47 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 92.04 train loss: 0.0037 val acc: 89.98 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 96.04 train loss: 0.0019 val acc: 89.75 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 97.35 train loss: 0.0012 val acc: 89.95 val loss: 0.0092\n",
      "Sub epoch 4 train acc: 97.39 train loss: 0.0012 val acc: 89.80 val loss: 0.0111\n",
      "Sub epoch 5 train acc: 99.01 train loss: 0.0005 val acc: 89.87 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 99.52 train loss: 0.0003 val acc: 89.90 val loss: 0.0115\n",
      "Train Loss: 0.0003, Train Acc: 99.52  Validation Loss: 0.0115, Validation Acc: 89.90\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 67.25 train loss: 0.0178 val acc: 88.52 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 92.68 train loss: 0.0034 val acc: 90.07 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 96.38 train loss: 0.0017 val acc: 90.15 val loss: 0.0083\n",
      "Sub epoch 3 train acc: 97.24 train loss: 0.0013 val acc: 90.32 val loss: 0.0092\n",
      "Sub epoch 4 train acc: 97.80 train loss: 0.0010 val acc: 90.23 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 97.45 train loss: 0.0012 val acc: 89.55 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 97.11 train loss: 0.0013 val acc: 89.17 val loss: 0.0112\n",
      "Train Loss: 0.0013, Train Acc: 97.11  Validation Loss: 0.0112, Validation Acc: 89.17\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 70.93 train loss: 0.0163 val acc: 89.30 val loss: 0.0054\n",
      "Sub epoch 1 train acc: 95.11 train loss: 0.0024 val acc: 90.48 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 98.10 train loss: 0.0010 val acc: 90.40 val loss: 0.0083\n",
      "Sub epoch 3 train acc: 98.31 train loss: 0.0008 val acc: 89.72 val loss: 0.0096\n",
      "Sub epoch 4 train acc: 98.63 train loss: 0.0007 val acc: 89.87 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 98.53 train loss: 0.0007 val acc: 89.70 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 98.10 train loss: 0.0009 val acc: 89.68 val loss: 0.0104\n",
      "Train Loss: 0.0009, Train Acc: 98.10  Validation Loss: 0.0104, Validation Acc: 89.68\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 71.75 train loss: 0.0159 val acc: 88.25 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 96.33 train loss: 0.0020 val acc: 90.60 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 98.58 train loss: 0.0008 val acc: 90.38 val loss: 0.0087\n",
      "Sub epoch 3 train acc: 98.91 train loss: 0.0006 val acc: 90.05 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 98.56 train loss: 0.0007 val acc: 89.80 val loss: 0.0106\n",
      "Sub epoch 5 train acc: 99.52 train loss: 0.0003 val acc: 90.32 val loss: 0.0103\n",
      "Sub epoch 6 train acc: 99.81 train loss: 0.0002 val acc: 90.47 val loss: 0.0110\n",
      "Train Loss: 0.0002, Train Acc: 99.81  Validation Loss: 0.0110, Validation Acc: 90.47\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 72.37 train loss: 0.0154 val acc: 88.87 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 95.86 train loss: 0.0021 val acc: 90.25 val loss: 0.0077\n",
      "Sub epoch 2 train acc: 98.30 train loss: 0.0009 val acc: 90.08 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.83 train loss: 0.0006 val acc: 89.97 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 98.52 train loss: 0.0007 val acc: 89.83 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 98.62 train loss: 0.0007 val acc: 89.43 val loss: 0.0124\n",
      "Sub epoch 6 train acc: 98.29 train loss: 0.0009 val acc: 89.83 val loss: 0.0117\n",
      "Train Loss: 0.0009, Train Acc: 98.29  Validation Loss: 0.0117, Validation Acc: 89.83\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 75.06 train loss: 0.0144 val acc: 90.03 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 97.25 train loss: 0.0015 val acc: 90.28 val loss: 0.0084\n",
      "Sub epoch 2 train acc: 99.39 train loss: 0.0005 val acc: 90.55 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 99.60 train loss: 0.0003 val acc: 90.62 val loss: 0.0092\n",
      "Sub epoch 4 train acc: 99.67 train loss: 0.0003 val acc: 90.55 val loss: 0.0095\n",
      "Sub epoch 5 train acc: 99.73 train loss: 0.0003 val acc: 90.62 val loss: 0.0098\n",
      "Sub epoch 6 train acc: 99.77 train loss: 0.0002 val acc: 90.42 val loss: 0.0100\n",
      "Train Loss: 0.0002, Train Acc: 99.77  Validation Loss: 0.0100, Validation Acc: 90.42\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 73.55 train loss: 0.0151 val acc: 87.65 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 96.01 train loss: 0.0020 val acc: 89.87 val loss: 0.0079\n",
      "Sub epoch 2 train acc: 98.42 train loss: 0.0009 val acc: 89.62 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 99.00 train loss: 0.0006 val acc: 90.17 val loss: 0.0096\n",
      "Sub epoch 4 train acc: 98.81 train loss: 0.0006 val acc: 89.80 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 98.76 train loss: 0.0006 val acc: 90.10 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 99.47 train loss: 0.0003 val acc: 90.70 val loss: 0.0110\n",
      "Train Loss: 0.0003, Train Acc: 99.47  Validation Loss: 0.0110, Validation Acc: 90.70\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 76.24 train loss: 0.0140 val acc: 89.55 val loss: 0.0059\n",
      "Sub epoch 1 train acc: 97.74 train loss: 0.0012 val acc: 89.82 val loss: 0.0094\n",
      "Sub epoch 2 train acc: 99.25 train loss: 0.0004 val acc: 90.18 val loss: 0.0109\n",
      "Sub epoch 3 train acc: 99.47 train loss: 0.0003 val acc: 90.17 val loss: 0.0118\n",
      "Sub epoch 4 train acc: 99.17 train loss: 0.0004 val acc: 89.73 val loss: 0.0112\n",
      "Sub epoch 5 train acc: 99.10 train loss: 0.0005 val acc: 89.82 val loss: 0.0128\n",
      "Sub epoch 6 train acc: 99.01 train loss: 0.0005 val acc: 89.42 val loss: 0.0119\n",
      "Train Loss: 0.0005, Train Acc: 99.01  Validation Loss: 0.0119, Validation Acc: 89.42\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 77.89 train loss: 0.0133 val acc: 89.40 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 98.13 train loss: 0.0010 val acc: 90.02 val loss: 0.0106\n",
      "Sub epoch 2 train acc: 99.50 train loss: 0.0003 val acc: 90.05 val loss: 0.0125\n",
      "Sub epoch 3 train acc: 99.85 train loss: 0.0001 val acc: 90.20 val loss: 0.0129\n",
      "Sub epoch 4 train acc: 99.94 train loss: 0.0001 val acc: 90.25 val loss: 0.0133\n",
      "Sub epoch 5 train acc: 99.95 train loss: 0.0000 val acc: 90.45 val loss: 0.0139\n",
      "Sub epoch 6 train acc: 99.96 train loss: 0.0000 val acc: 90.30 val loss: 0.0143\n",
      "Train Loss: 0.0000, Train Acc: 99.96  Validation Loss: 0.0143, Validation Acc: 90.30\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 75.01 train loss: 0.0147 val acc: 87.88 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 97.23 train loss: 0.0013 val acc: 90.13 val loss: 0.0097\n",
      "Sub epoch 2 train acc: 99.29 train loss: 0.0004 val acc: 90.03 val loss: 0.0112\n",
      "Sub epoch 3 train acc: 99.45 train loss: 0.0003 val acc: 90.10 val loss: 0.0115\n",
      "Sub epoch 4 train acc: 99.31 train loss: 0.0003 val acc: 90.15 val loss: 0.0136\n",
      "Sub epoch 5 train acc: 99.31 train loss: 0.0003 val acc: 89.85 val loss: 0.0145\n",
      "Sub epoch 6 train acc: 98.93 train loss: 0.0006 val acc: 89.58 val loss: 0.0157\n",
      "Train Loss: 0.0006, Train Acc: 98.93  Validation Loss: 0.0157, Validation Acc: 89.58\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 80.06 train loss: 0.0126 val acc: 89.38 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 98.53 train loss: 0.0009 val acc: 90.32 val loss: 0.0112\n",
      "Sub epoch 2 train acc: 99.55 train loss: 0.0002 val acc: 90.52 val loss: 0.0139\n",
      "Sub epoch 3 train acc: 99.65 train loss: 0.0002 val acc: 90.33 val loss: 0.0143\n",
      "Sub epoch 4 train acc: 99.51 train loss: 0.0003 val acc: 89.70 val loss: 0.0139\n",
      "Sub epoch 5 train acc: 99.32 train loss: 0.0003 val acc: 90.08 val loss: 0.0148\n",
      "Sub epoch 6 train acc: 99.31 train loss: 0.0004 val acc: 89.70 val loss: 0.0153\n",
      "Train Loss: 0.0004, Train Acc: 99.31  Validation Loss: 0.0153, Validation Acc: 89.70\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 81.36 train loss: 0.0118 val acc: 89.50 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 98.74 train loss: 0.0007 val acc: 90.18 val loss: 0.0108\n",
      "Sub epoch 2 train acc: 99.70 train loss: 0.0002 val acc: 90.22 val loss: 0.0135\n",
      "Sub epoch 3 train acc: 99.69 train loss: 0.0002 val acc: 90.18 val loss: 0.0144\n",
      "Sub epoch 4 train acc: 99.93 train loss: 0.0000 val acc: 90.38 val loss: 0.0148\n",
      "Sub epoch 5 train acc: 99.98 train loss: 0.0000 val acc: 90.40 val loss: 0.0153\n",
      "Sub epoch 6 train acc: 99.99 train loss: 0.0000 val acc: 90.38 val loss: 0.0158\n",
      "Train Loss: 0.0000, Train Acc: 99.99  Validation Loss: 0.0158, Validation Acc: 90.38\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 77.38 train loss: 0.0136 val acc: 88.83 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 97.85 train loss: 0.0011 val acc: 90.52 val loss: 0.0104\n",
      "Sub epoch 2 train acc: 99.44 train loss: 0.0003 val acc: 90.02 val loss: 0.0135\n",
      "Sub epoch 3 train acc: 99.63 train loss: 0.0002 val acc: 89.93 val loss: 0.0146\n",
      "Sub epoch 4 train acc: 99.55 train loss: 0.0002 val acc: 89.97 val loss: 0.0147\n",
      "Sub epoch 5 train acc: 99.29 train loss: 0.0004 val acc: 89.80 val loss: 0.0151\n",
      "Sub epoch 6 train acc: 99.27 train loss: 0.0005 val acc: 90.10 val loss: 0.0145\n",
      "Train Loss: 0.0005, Train Acc: 99.27  Validation Loss: 0.0145, Validation Acc: 90.10\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 80.89 train loss: 0.0120 val acc: 88.15 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 96.68 train loss: 0.0016 val acc: 90.23 val loss: 0.0090\n",
      "Sub epoch 2 train acc: 98.38 train loss: 0.0008 val acc: 90.42 val loss: 0.0101\n",
      "Sub epoch 3 train acc: 99.07 train loss: 0.0004 val acc: 90.38 val loss: 0.0113\n",
      "Sub epoch 4 train acc: 99.50 train loss: 0.0003 val acc: 90.40 val loss: 0.0123\n",
      "Sub epoch 5 train acc: 99.74 train loss: 0.0002 val acc: 90.30 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 99.85 train loss: 0.0001 val acc: 90.38 val loss: 0.0151\n",
      "Train Loss: 0.0001, Train Acc: 99.85  Validation Loss: 0.0151, Validation Acc: 90.38\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.44 train loss: 0.0142 val acc: 78.83 val loss: 0.0105\n",
      "Sub epoch 1 train acc: 84.41 train loss: 0.0070 val acc: 84.25 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 88.30 train loss: 0.0049 val acc: 85.85 val loss: 0.0067\n",
      "Sub epoch 3 train acc: 90.99 train loss: 0.0038 val acc: 85.58 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 92.69 train loss: 0.0031 val acc: 85.33 val loss: 0.0074\n",
      "Sub epoch 5 train acc: 96.41 train loss: 0.0016 val acc: 87.47 val loss: 0.0070\n",
      "Sub epoch 6 train acc: 98.02 train loss: 0.0009 val acc: 87.35 val loss: 0.0075\n",
      "Train Loss: 0.0009, Train Acc: 98.02  Validation Loss: 0.0075, Validation Acc: 87.35\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 69.34 train loss: 0.0183 val acc: 80.10 val loss: 0.0093\n",
      "Sub epoch 1 train acc: 82.91 train loss: 0.0074 val acc: 86.53 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 87.76 train loss: 0.0051 val acc: 87.67 val loss: 0.0075\n",
      "Sub epoch 3 train acc: 90.62 train loss: 0.0039 val acc: 86.55 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 92.93 train loss: 0.0032 val acc: 87.37 val loss: 0.0079\n",
      "Sub epoch 5 train acc: 93.95 train loss: 0.0026 val acc: 87.45 val loss: 0.0088\n",
      "Sub epoch 6 train acc: 95.13 train loss: 0.0021 val acc: 86.33 val loss: 0.0102\n",
      "Train Loss: 0.0021, Train Acc: 95.13  Validation Loss: 0.0102, Validation Acc: 86.33\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 69.54 train loss: 0.0185 val acc: 85.15 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 85.46 train loss: 0.0061 val acc: 87.87 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 93.25 train loss: 0.0030 val acc: 88.82 val loss: 0.0071\n",
      "Sub epoch 3 train acc: 94.75 train loss: 0.0024 val acc: 88.83 val loss: 0.0074\n",
      "Sub epoch 4 train acc: 95.66 train loss: 0.0020 val acc: 88.92 val loss: 0.0080\n",
      "Sub epoch 5 train acc: 96.37 train loss: 0.0017 val acc: 88.90 val loss: 0.0084\n",
      "Sub epoch 6 train acc: 96.77 train loss: 0.0015 val acc: 88.93 val loss: 0.0087\n",
      "Train Loss: 0.0015, Train Acc: 96.77  Validation Loss: 0.0087, Validation Acc: 88.93\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 64.67 train loss: 0.0193 val acc: 85.95 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 83.64 train loss: 0.0068 val acc: 88.88 val loss: 0.0060\n",
      "Sub epoch 2 train acc: 88.83 train loss: 0.0045 val acc: 88.38 val loss: 0.0067\n",
      "Sub epoch 3 train acc: 91.52 train loss: 0.0035 val acc: 88.48 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 93.96 train loss: 0.0026 val acc: 88.22 val loss: 0.0090\n",
      "Sub epoch 5 train acc: 94.80 train loss: 0.0022 val acc: 88.77 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 97.87 train loss: 0.0010 val acc: 89.10 val loss: 0.0103\n",
      "Train Loss: 0.0010, Train Acc: 97.87  Validation Loss: 0.0103, Validation Acc: 89.10\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 65.64 train loss: 0.0182 val acc: 85.17 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 87.90 train loss: 0.0052 val acc: 88.75 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 92.67 train loss: 0.0033 val acc: 89.20 val loss: 0.0078\n",
      "Sub epoch 3 train acc: 94.39 train loss: 0.0024 val acc: 89.05 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 95.53 train loss: 0.0020 val acc: 88.58 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 95.64 train loss: 0.0019 val acc: 88.83 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 95.88 train loss: 0.0018 val acc: 88.52 val loss: 0.0107\n",
      "Train Loss: 0.0018, Train Acc: 95.88  Validation Loss: 0.0107, Validation Acc: 88.52\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 67.33 train loss: 0.0183 val acc: 86.88 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 91.20 train loss: 0.0040 val acc: 89.48 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 95.43 train loss: 0.0022 val acc: 89.17 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 97.93 train loss: 0.0011 val acc: 89.53 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 98.52 train loss: 0.0008 val acc: 89.58 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 98.76 train loss: 0.0007 val acc: 89.75 val loss: 0.0101\n",
      "Sub epoch 6 train acc: 98.95 train loss: 0.0006 val acc: 89.78 val loss: 0.0105\n",
      "Train Loss: 0.0006, Train Acc: 98.95  Validation Loss: 0.0105, Validation Acc: 89.78\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 65.71 train loss: 0.0182 val acc: 88.18 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 90.55 train loss: 0.0043 val acc: 89.22 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 94.83 train loss: 0.0025 val acc: 89.12 val loss: 0.0075\n",
      "Sub epoch 3 train acc: 96.11 train loss: 0.0018 val acc: 89.68 val loss: 0.0085\n",
      "Sub epoch 4 train acc: 96.78 train loss: 0.0015 val acc: 88.95 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 96.84 train loss: 0.0014 val acc: 89.38 val loss: 0.0097\n",
      "Sub epoch 6 train acc: 96.84 train loss: 0.0014 val acc: 88.70 val loss: 0.0102\n",
      "Train Loss: 0.0014, Train Acc: 96.84  Validation Loss: 0.0102, Validation Acc: 88.70\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 70.58 train loss: 0.0161 val acc: 88.03 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 94.02 train loss: 0.0030 val acc: 89.28 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 97.14 train loss: 0.0015 val acc: 89.73 val loss: 0.0086\n",
      "Sub epoch 3 train acc: 97.92 train loss: 0.0010 val acc: 89.22 val loss: 0.0099\n",
      "Sub epoch 4 train acc: 98.21 train loss: 0.0009 val acc: 89.15 val loss: 0.0107\n",
      "Sub epoch 5 train acc: 97.99 train loss: 0.0009 val acc: 88.47 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 97.87 train loss: 0.0010 val acc: 89.27 val loss: 0.0114\n",
      "Train Loss: 0.0010, Train Acc: 97.87  Validation Loss: 0.0114, Validation Acc: 89.27\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 70.10 train loss: 0.0168 val acc: 88.60 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 95.30 train loss: 0.0024 val acc: 89.60 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 97.93 train loss: 0.0011 val acc: 89.25 val loss: 0.0090\n",
      "Sub epoch 3 train acc: 98.23 train loss: 0.0009 val acc: 89.27 val loss: 0.0096\n",
      "Sub epoch 4 train acc: 99.40 train loss: 0.0004 val acc: 89.42 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 99.73 train loss: 0.0002 val acc: 89.47 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 99.79 train loss: 0.0002 val acc: 89.52 val loss: 0.0106\n",
      "Train Loss: 0.0002, Train Acc: 99.79  Validation Loss: 0.0106, Validation Acc: 89.52\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 69.58 train loss: 0.0167 val acc: 88.50 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 94.68 train loss: 0.0026 val acc: 89.68 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 97.62 train loss: 0.0013 val acc: 89.32 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 98.15 train loss: 0.0010 val acc: 89.50 val loss: 0.0091\n",
      "Sub epoch 4 train acc: 98.14 train loss: 0.0009 val acc: 89.58 val loss: 0.0094\n",
      "Sub epoch 5 train acc: 98.16 train loss: 0.0009 val acc: 89.10 val loss: 0.0105\n",
      "Sub epoch 6 train acc: 98.26 train loss: 0.0008 val acc: 89.17 val loss: 0.0108\n",
      "Train Loss: 0.0008, Train Acc: 98.26  Validation Loss: 0.0108, Validation Acc: 89.17\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 74.52 train loss: 0.0142 val acc: 88.67 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 94.59 train loss: 0.0028 val acc: 90.02 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 96.94 train loss: 0.0016 val acc: 89.90 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 98.06 train loss: 0.0011 val acc: 89.77 val loss: 0.0075\n",
      "Sub epoch 4 train acc: 98.68 train loss: 0.0009 val acc: 89.63 val loss: 0.0079\n",
      "Sub epoch 5 train acc: 99.08 train loss: 0.0007 val acc: 89.53 val loss: 0.0084\n",
      "Sub epoch 6 train acc: 99.34 train loss: 0.0005 val acc: 89.47 val loss: 0.0087\n",
      "Train Loss: 0.0005, Train Acc: 99.34  Validation Loss: 0.0087, Validation Acc: 89.47\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 73.44 train loss: 0.0149 val acc: 88.07 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 95.72 train loss: 0.0022 val acc: 89.07 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 98.21 train loss: 0.0010 val acc: 89.50 val loss: 0.0085\n",
      "Sub epoch 3 train acc: 98.42 train loss: 0.0008 val acc: 89.42 val loss: 0.0091\n",
      "Sub epoch 4 train acc: 98.68 train loss: 0.0007 val acc: 89.22 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 99.51 train loss: 0.0003 val acc: 89.77 val loss: 0.0098\n",
      "Sub epoch 6 train acc: 99.79 train loss: 0.0002 val acc: 89.67 val loss: 0.0103\n",
      "Train Loss: 0.0002, Train Acc: 99.79  Validation Loss: 0.0103, Validation Acc: 89.67\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 74.97 train loss: 0.0148 val acc: 88.30 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 96.84 train loss: 0.0016 val acc: 89.63 val loss: 0.0079\n",
      "Sub epoch 2 train acc: 98.86 train loss: 0.0006 val acc: 89.32 val loss: 0.0096\n",
      "Sub epoch 3 train acc: 98.79 train loss: 0.0006 val acc: 89.50 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 98.91 train loss: 0.0005 val acc: 89.03 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 98.95 train loss: 0.0005 val acc: 89.40 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 98.58 train loss: 0.0007 val acc: 88.97 val loss: 0.0119\n",
      "Train Loss: 0.0007, Train Acc: 98.58  Validation Loss: 0.0119, Validation Acc: 88.97\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 78.15 train loss: 0.0128 val acc: 89.32 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 98.04 train loss: 0.0010 val acc: 89.53 val loss: 0.0093\n",
      "Sub epoch 2 train acc: 99.65 train loss: 0.0002 val acc: 89.87 val loss: 0.0097\n",
      "Sub epoch 3 train acc: 99.84 train loss: 0.0001 val acc: 89.77 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 99.88 train loss: 0.0001 val acc: 89.83 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 99.92 train loss: 0.0001 val acc: 89.78 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 99.94 train loss: 0.0001 val acc: 89.70 val loss: 0.0119\n",
      "Train Loss: 0.0001, Train Acc: 99.94  Validation Loss: 0.0119, Validation Acc: 89.70\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 76.04 train loss: 0.0140 val acc: 88.83 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 96.77 train loss: 0.0016 val acc: 89.58 val loss: 0.0082\n",
      "Sub epoch 2 train acc: 98.80 train loss: 0.0006 val acc: 89.07 val loss: 0.0097\n",
      "Sub epoch 3 train acc: 99.12 train loss: 0.0004 val acc: 89.37 val loss: 0.0108\n",
      "Sub epoch 4 train acc: 99.15 train loss: 0.0004 val acc: 89.50 val loss: 0.0122\n",
      "Sub epoch 5 train acc: 98.92 train loss: 0.0006 val acc: 88.90 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 99.65 train loss: 0.0002 val acc: 89.63 val loss: 0.0120\n",
      "Train Loss: 0.0002, Train Acc: 99.65  Validation Loss: 0.0120, Validation Acc: 89.63\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 79.33 train loss: 0.0123 val acc: 89.03 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 98.16 train loss: 0.0010 val acc: 89.10 val loss: 0.0106\n",
      "Sub epoch 2 train acc: 99.37 train loss: 0.0003 val acc: 89.12 val loss: 0.0125\n",
      "Sub epoch 3 train acc: 99.43 train loss: 0.0003 val acc: 89.15 val loss: 0.0125\n",
      "Sub epoch 4 train acc: 99.35 train loss: 0.0003 val acc: 88.72 val loss: 0.0127\n",
      "Sub epoch 5 train acc: 99.13 train loss: 0.0004 val acc: 88.75 val loss: 0.0142\n",
      "Sub epoch 6 train acc: 99.01 train loss: 0.0005 val acc: 89.45 val loss: 0.0119\n",
      "Train Loss: 0.0005, Train Acc: 99.01  Validation Loss: 0.0119, Validation Acc: 89.45\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 80.31 train loss: 0.0121 val acc: 88.67 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 98.46 train loss: 0.0008 val acc: 89.58 val loss: 0.0109\n",
      "Sub epoch 2 train acc: 99.57 train loss: 0.0002 val acc: 89.52 val loss: 0.0124\n",
      "Sub epoch 3 train acc: 99.90 train loss: 0.0001 val acc: 89.42 val loss: 0.0126\n",
      "Sub epoch 4 train acc: 99.97 train loss: 0.0000 val acc: 89.32 val loss: 0.0132\n",
      "Sub epoch 5 train acc: 99.98 train loss: 0.0000 val acc: 89.40 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 99.98 train loss: 0.0000 val acc: 89.47 val loss: 0.0141\n",
      "Train Loss: 0.0000, Train Acc: 99.98  Validation Loss: 0.0141, Validation Acc: 89.47\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 78.39 train loss: 0.0133 val acc: 88.43 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 97.71 train loss: 0.0012 val acc: 89.68 val loss: 0.0097\n",
      "Sub epoch 2 train acc: 99.39 train loss: 0.0003 val acc: 89.60 val loss: 0.0121\n",
      "Sub epoch 3 train acc: 99.45 train loss: 0.0003 val acc: 89.17 val loss: 0.0131\n",
      "Sub epoch 4 train acc: 99.49 train loss: 0.0003 val acc: 89.22 val loss: 0.0136\n",
      "Sub epoch 5 train acc: 99.15 train loss: 0.0004 val acc: 89.37 val loss: 0.0118\n",
      "Sub epoch 6 train acc: 99.20 train loss: 0.0005 val acc: 89.05 val loss: 0.0138\n",
      "Train Loss: 0.0005, Train Acc: 99.20  Validation Loss: 0.0138, Validation Acc: 89.05\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 81.82 train loss: 0.0117 val acc: 88.15 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 98.76 train loss: 0.0007 val acc: 89.83 val loss: 0.0122\n",
      "Sub epoch 2 train acc: 99.73 train loss: 0.0002 val acc: 89.80 val loss: 0.0147\n",
      "Sub epoch 3 train acc: 99.59 train loss: 0.0002 val acc: 89.43 val loss: 0.0142\n",
      "Sub epoch 4 train acc: 99.51 train loss: 0.0003 val acc: 88.80 val loss: 0.0157\n",
      "Sub epoch 5 train acc: 99.33 train loss: 0.0003 val acc: 89.62 val loss: 0.0163\n",
      "Sub epoch 6 train acc: 99.14 train loss: 0.0005 val acc: 88.87 val loss: 0.0171\n",
      "Train Loss: 0.0005, Train Acc: 99.14  Validation Loss: 0.0171, Validation Acc: 88.87\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 82.28 train loss: 0.0112 val acc: 88.23 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 98.80 train loss: 0.0006 val acc: 89.68 val loss: 0.0133\n",
      "Sub epoch 2 train acc: 99.77 train loss: 0.0001 val acc: 89.57 val loss: 0.0148\n",
      "Sub epoch 3 train acc: 99.83 train loss: 0.0001 val acc: 89.90 val loss: 0.0163\n",
      "Sub epoch 4 train acc: 99.93 train loss: 0.0000 val acc: 89.97 val loss: 0.0166\n",
      "Sub epoch 5 train acc: 99.98 train loss: 0.0000 val acc: 89.82 val loss: 0.0174\n",
      "Sub epoch 6 train acc: 99.99 train loss: 0.0000 val acc: 89.87 val loss: 0.0179\n",
      "Train Loss: 0.0000, Train Acc: 99.99  Validation Loss: 0.0179, Validation Acc: 89.87\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.99 train loss: 0.0137 val acc: 81.17 val loss: 0.0087\n",
      "Sub epoch 1 train acc: 84.63 train loss: 0.0069 val acc: 81.90 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 88.60 train loss: 0.0049 val acc: 83.93 val loss: 0.0080\n",
      "Sub epoch 3 train acc: 90.51 train loss: 0.0041 val acc: 85.30 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 92.60 train loss: 0.0032 val acc: 84.27 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 93.33 train loss: 0.0029 val acc: 83.07 val loss: 0.0107\n",
      "Sub epoch 6 train acc: 94.04 train loss: 0.0029 val acc: 83.93 val loss: 0.0105\n",
      "Train Loss: 0.0029, Train Acc: 94.04  Validation Loss: 0.0105, Validation Acc: 83.93\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 75.02 train loss: 0.0151 val acc: 79.02 val loss: 0.0099\n",
      "Sub epoch 1 train acc: 87.99 train loss: 0.0051 val acc: 87.13 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 91.13 train loss: 0.0036 val acc: 87.52 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 92.95 train loss: 0.0029 val acc: 87.53 val loss: 0.0071\n",
      "Sub epoch 4 train acc: 94.36 train loss: 0.0024 val acc: 87.58 val loss: 0.0073\n",
      "Sub epoch 5 train acc: 95.29 train loss: 0.0020 val acc: 88.02 val loss: 0.0077\n",
      "Sub epoch 6 train acc: 96.49 train loss: 0.0016 val acc: 87.87 val loss: 0.0080\n",
      "Train Loss: 0.0016, Train Acc: 96.49  Validation Loss: 0.0080, Validation Acc: 87.87\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 65.93 train loss: 0.0203 val acc: 83.68 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 82.51 train loss: 0.0073 val acc: 87.68 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 87.23 train loss: 0.0051 val acc: 88.15 val loss: 0.0067\n",
      "Sub epoch 3 train acc: 90.39 train loss: 0.0040 val acc: 88.28 val loss: 0.0075\n",
      "Sub epoch 4 train acc: 93.00 train loss: 0.0030 val acc: 88.23 val loss: 0.0082\n",
      "Sub epoch 5 train acc: 96.72 train loss: 0.0015 val acc: 88.52 val loss: 0.0085\n",
      "Sub epoch 6 train acc: 97.86 train loss: 0.0010 val acc: 88.45 val loss: 0.0090\n",
      "Train Loss: 0.0010, Train Acc: 97.86  Validation Loss: 0.0090, Validation Acc: 88.45\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 65.10 train loss: 0.0196 val acc: 83.80 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 84.27 train loss: 0.0066 val acc: 88.20 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 89.42 train loss: 0.0044 val acc: 88.08 val loss: 0.0074\n",
      "Sub epoch 3 train acc: 91.24 train loss: 0.0036 val acc: 88.52 val loss: 0.0075\n",
      "Sub epoch 4 train acc: 93.90 train loss: 0.0025 val acc: 88.65 val loss: 0.0087\n",
      "Sub epoch 5 train acc: 94.96 train loss: 0.0022 val acc: 88.02 val loss: 0.0100\n",
      "Sub epoch 6 train acc: 95.31 train loss: 0.0021 val acc: 87.80 val loss: 0.0108\n",
      "Train Loss: 0.0021, Train Acc: 95.31  Validation Loss: 0.0108, Validation Acc: 87.80\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 67.66 train loss: 0.0178 val acc: 85.95 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 89.28 train loss: 0.0047 val acc: 88.27 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 95.19 train loss: 0.0022 val acc: 89.00 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 96.37 train loss: 0.0018 val acc: 89.13 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 96.93 train loss: 0.0015 val acc: 89.05 val loss: 0.0087\n",
      "Sub epoch 5 train acc: 97.29 train loss: 0.0013 val acc: 89.05 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 97.84 train loss: 0.0012 val acc: 89.33 val loss: 0.0095\n",
      "Train Loss: 0.0012, Train Acc: 97.84  Validation Loss: 0.0095, Validation Acc: 89.33\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 64.71 train loss: 0.0191 val acc: 85.32 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 87.78 train loss: 0.0053 val acc: 88.62 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 92.10 train loss: 0.0034 val acc: 88.57 val loss: 0.0074\n",
      "Sub epoch 3 train acc: 94.29 train loss: 0.0025 val acc: 88.82 val loss: 0.0091\n",
      "Sub epoch 4 train acc: 94.80 train loss: 0.0023 val acc: 88.55 val loss: 0.0089\n",
      "Sub epoch 5 train acc: 95.72 train loss: 0.0019 val acc: 89.40 val loss: 0.0092\n",
      "Sub epoch 6 train acc: 98.30 train loss: 0.0010 val acc: 89.70 val loss: 0.0099\n",
      "Train Loss: 0.0010, Train Acc: 98.30  Validation Loss: 0.0099, Validation Acc: 89.70\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 68.25 train loss: 0.0169 val acc: 88.07 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 91.73 train loss: 0.0038 val acc: 89.33 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 94.96 train loss: 0.0022 val acc: 89.47 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 96.49 train loss: 0.0016 val acc: 89.38 val loss: 0.0096\n",
      "Sub epoch 4 train acc: 96.87 train loss: 0.0014 val acc: 88.98 val loss: 0.0109\n",
      "Sub epoch 5 train acc: 96.79 train loss: 0.0014 val acc: 88.63 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 96.84 train loss: 0.0015 val acc: 89.55 val loss: 0.0107\n",
      "Train Loss: 0.0015, Train Acc: 96.84  Validation Loss: 0.0107, Validation Acc: 89.55\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 69.07 train loss: 0.0174 val acc: 87.52 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 93.62 train loss: 0.0031 val acc: 89.22 val loss: 0.0071\n",
      "Sub epoch 2 train acc: 97.00 train loss: 0.0016 val acc: 89.42 val loss: 0.0087\n",
      "Sub epoch 3 train acc: 98.72 train loss: 0.0007 val acc: 89.53 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 99.18 train loss: 0.0005 val acc: 89.70 val loss: 0.0094\n",
      "Sub epoch 5 train acc: 99.29 train loss: 0.0004 val acc: 89.77 val loss: 0.0098\n",
      "Sub epoch 6 train acc: 99.43 train loss: 0.0004 val acc: 89.78 val loss: 0.0102\n",
      "Train Loss: 0.0004, Train Acc: 99.43  Validation Loss: 0.0102, Validation Acc: 89.78\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 66.59 train loss: 0.0180 val acc: 87.63 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 92.99 train loss: 0.0033 val acc: 89.78 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 96.30 train loss: 0.0018 val acc: 89.57 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 97.23 train loss: 0.0013 val acc: 89.57 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 97.63 train loss: 0.0011 val acc: 89.18 val loss: 0.0096\n",
      "Sub epoch 5 train acc: 97.67 train loss: 0.0010 val acc: 89.38 val loss: 0.0105\n",
      "Sub epoch 6 train acc: 97.59 train loss: 0.0012 val acc: 88.60 val loss: 0.0116\n",
      "Train Loss: 0.0012, Train Acc: 97.59  Validation Loss: 0.0116, Validation Acc: 88.60\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 71.89 train loss: 0.0157 val acc: 87.43 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 95.62 train loss: 0.0022 val acc: 89.82 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 97.93 train loss: 0.0010 val acc: 89.78 val loss: 0.0089\n",
      "Sub epoch 3 train acc: 98.58 train loss: 0.0007 val acc: 89.65 val loss: 0.0108\n",
      "Sub epoch 4 train acc: 98.68 train loss: 0.0006 val acc: 89.52 val loss: 0.0118\n",
      "Sub epoch 5 train acc: 98.56 train loss: 0.0007 val acc: 89.23 val loss: 0.0122\n",
      "Sub epoch 6 train acc: 98.22 train loss: 0.0009 val acc: 89.30 val loss: 0.0121\n",
      "Train Loss: 0.0009, Train Acc: 98.22  Validation Loss: 0.0121, Validation Acc: 89.30\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 72.51 train loss: 0.0155 val acc: 87.83 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 96.39 train loss: 0.0019 val acc: 89.72 val loss: 0.0083\n",
      "Sub epoch 2 train acc: 98.78 train loss: 0.0007 val acc: 89.67 val loss: 0.0104\n",
      "Sub epoch 3 train acc: 99.02 train loss: 0.0005 val acc: 89.58 val loss: 0.0113\n",
      "Sub epoch 4 train acc: 99.68 train loss: 0.0002 val acc: 89.82 val loss: 0.0117\n",
      "Sub epoch 5 train acc: 99.87 train loss: 0.0001 val acc: 89.78 val loss: 0.0122\n",
      "Sub epoch 6 train acc: 99.90 train loss: 0.0001 val acc: 89.83 val loss: 0.0126\n",
      "Train Loss: 0.0001, Train Acc: 99.90  Validation Loss: 0.0126, Validation Acc: 89.83\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 71.47 train loss: 0.0157 val acc: 88.18 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 95.56 train loss: 0.0022 val acc: 89.55 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 98.36 train loss: 0.0009 val acc: 89.60 val loss: 0.0104\n",
      "Sub epoch 3 train acc: 98.99 train loss: 0.0006 val acc: 89.57 val loss: 0.0109\n",
      "Sub epoch 4 train acc: 98.82 train loss: 0.0006 val acc: 89.68 val loss: 0.0122\n",
      "Sub epoch 5 train acc: 98.70 train loss: 0.0006 val acc: 89.32 val loss: 0.0136\n",
      "Sub epoch 6 train acc: 98.54 train loss: 0.0007 val acc: 89.32 val loss: 0.0136\n",
      "Train Loss: 0.0007, Train Acc: 98.54  Validation Loss: 0.0136, Validation Acc: 89.32\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 75.17 train loss: 0.0149 val acc: 88.30 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 94.97 train loss: 0.0026 val acc: 89.68 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 97.39 train loss: 0.0014 val acc: 89.58 val loss: 0.0084\n",
      "Sub epoch 3 train acc: 98.51 train loss: 0.0009 val acc: 89.75 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 99.08 train loss: 0.0006 val acc: 89.77 val loss: 0.0094\n",
      "Sub epoch 5 train acc: 99.42 train loss: 0.0005 val acc: 89.63 val loss: 0.0099\n",
      "Sub epoch 6 train acc: 99.64 train loss: 0.0003 val acc: 89.75 val loss: 0.0106\n",
      "Train Loss: 0.0003, Train Acc: 99.64  Validation Loss: 0.0106, Validation Acc: 89.75\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 75.93 train loss: 0.0141 val acc: 88.10 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 96.76 train loss: 0.0017 val acc: 89.42 val loss: 0.0089\n",
      "Sub epoch 2 train acc: 98.92 train loss: 0.0007 val acc: 89.52 val loss: 0.0102\n",
      "Sub epoch 3 train acc: 99.05 train loss: 0.0005 val acc: 89.63 val loss: 0.0115\n",
      "Sub epoch 4 train acc: 99.15 train loss: 0.0005 val acc: 89.58 val loss: 0.0126\n",
      "Sub epoch 5 train acc: 99.74 train loss: 0.0002 val acc: 89.92 val loss: 0.0127\n",
      "Sub epoch 6 train acc: 99.91 train loss: 0.0001 val acc: 89.95 val loss: 0.0132\n",
      "Train Loss: 0.0001, Train Acc: 99.91  Validation Loss: 0.0132, Validation Acc: 89.95\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 76.66 train loss: 0.0141 val acc: 88.20 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 97.35 train loss: 0.0013 val acc: 89.68 val loss: 0.0095\n",
      "Sub epoch 2 train acc: 99.16 train loss: 0.0004 val acc: 89.40 val loss: 0.0113\n",
      "Sub epoch 3 train acc: 99.38 train loss: 0.0003 val acc: 89.88 val loss: 0.0135\n",
      "Sub epoch 4 train acc: 99.11 train loss: 0.0004 val acc: 89.12 val loss: 0.0144\n",
      "Sub epoch 5 train acc: 99.16 train loss: 0.0004 val acc: 89.13 val loss: 0.0149\n",
      "Sub epoch 6 train acc: 98.60 train loss: 0.0008 val acc: 89.30 val loss: 0.0135\n",
      "Train Loss: 0.0008, Train Acc: 98.60  Validation Loss: 0.0135, Validation Acc: 89.30\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 79.67 train loss: 0.0126 val acc: 87.38 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 98.31 train loss: 0.0009 val acc: 89.33 val loss: 0.0120\n",
      "Sub epoch 2 train acc: 99.74 train loss: 0.0002 val acc: 89.65 val loss: 0.0124\n",
      "Sub epoch 3 train acc: 99.86 train loss: 0.0001 val acc: 89.75 val loss: 0.0129\n",
      "Sub epoch 4 train acc: 99.90 train loss: 0.0001 val acc: 89.62 val loss: 0.0134\n",
      "Sub epoch 5 train acc: 99.92 train loss: 0.0001 val acc: 89.63 val loss: 0.0140\n",
      "Sub epoch 6 train acc: 99.93 train loss: 0.0001 val acc: 89.63 val loss: 0.0146\n",
      "Train Loss: 0.0001, Train Acc: 99.93  Validation Loss: 0.0146, Validation Acc: 89.63\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 77.05 train loss: 0.0143 val acc: 88.17 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 97.31 train loss: 0.0014 val acc: 88.70 val loss: 0.0106\n",
      "Sub epoch 2 train acc: 99.13 train loss: 0.0005 val acc: 89.17 val loss: 0.0117\n",
      "Sub epoch 3 train acc: 99.49 train loss: 0.0003 val acc: 89.10 val loss: 0.0146\n",
      "Sub epoch 4 train acc: 99.32 train loss: 0.0004 val acc: 89.67 val loss: 0.0145\n",
      "Sub epoch 5 train acc: 99.27 train loss: 0.0004 val acc: 89.05 val loss: 0.0144\n",
      "Sub epoch 6 train acc: 99.72 train loss: 0.0002 val acc: 89.45 val loss: 0.0148\n",
      "Train Loss: 0.0002, Train Acc: 99.72  Validation Loss: 0.0148, Validation Acc: 89.45\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 79.71 train loss: 0.0125 val acc: 88.20 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 98.06 train loss: 0.0010 val acc: 89.43 val loss: 0.0121\n",
      "Sub epoch 2 train acc: 99.62 train loss: 0.0002 val acc: 89.53 val loss: 0.0145\n",
      "Sub epoch 3 train acc: 99.68 train loss: 0.0002 val acc: 89.03 val loss: 0.0162\n",
      "Sub epoch 4 train acc: 99.34 train loss: 0.0004 val acc: 89.40 val loss: 0.0152\n",
      "Sub epoch 5 train acc: 99.22 train loss: 0.0004 val acc: 89.10 val loss: 0.0155\n",
      "Sub epoch 6 train acc: 99.31 train loss: 0.0004 val acc: 89.12 val loss: 0.0177\n",
      "Train Loss: 0.0004, Train Acc: 99.31  Validation Loss: 0.0177, Validation Acc: 89.12\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 80.66 train loss: 0.0123 val acc: 87.48 val loss: 0.0086\n",
      "Sub epoch 1 train acc: 98.57 train loss: 0.0007 val acc: 89.60 val loss: 0.0133\n",
      "Sub epoch 2 train acc: 99.73 train loss: 0.0001 val acc: 89.33 val loss: 0.0152\n",
      "Sub epoch 3 train acc: 99.95 train loss: 0.0000 val acc: 89.47 val loss: 0.0156\n",
      "Sub epoch 4 train acc: 99.98 train loss: 0.0000 val acc: 89.43 val loss: 0.0160\n",
      "Sub epoch 5 train acc: 99.98 train loss: 0.0000 val acc: 89.42 val loss: 0.0165\n",
      "Sub epoch 6 train acc: 99.99 train loss: 0.0000 val acc: 89.45 val loss: 0.0170\n",
      "Train Loss: 0.0000, Train Acc: 99.99  Validation Loss: 0.0170, Validation Acc: 89.45\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 78.63 train loss: 0.0134 val acc: 86.08 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 97.89 train loss: 0.0010 val acc: 89.27 val loss: 0.0122\n",
      "Sub epoch 2 train acc: 99.51 train loss: 0.0002 val acc: 88.83 val loss: 0.0153\n",
      "Sub epoch 3 train acc: 99.57 train loss: 0.0002 val acc: 89.10 val loss: 0.0170\n",
      "Sub epoch 4 train acc: 99.58 train loss: 0.0002 val acc: 89.38 val loss: 0.0169\n",
      "Sub epoch 5 train acc: 99.21 train loss: 0.0004 val acc: 88.98 val loss: 0.0184\n",
      "Sub epoch 6 train acc: 99.16 train loss: 0.0005 val acc: 88.88 val loss: 0.0210\n",
      "Train Loss: 0.0005, Train Acc: 99.16  Validation Loss: 0.0210, Validation Acc: 88.88\n"
     ]
    }
   ],
   "source": [
    "train_loss_df = pd.DataFrame()\n",
    "train_acc_df = pd.DataFrame()\n",
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    train_loss, train_acc, validation_loss, validation_acc = \\\n",
    "        active_learn(net, trainset, np.array(train_df[i].to_list()), np.array(val_idx_df[i].to_list()), \n",
    "                      heuristic=least_confidence_heuristic, initial_train_idx=np.array(batch_df[i].to_list()),\n",
    "                     experiment_id=i)\n",
    "    train_loss_df[i] = train_loss\n",
    "    train_acc_df[i] = train_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64f986a3-fc10-480c-826d-65097795594b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T23:37:11.655338Z",
     "iopub.status.busy": "2023-01-11T23:37:11.654580Z",
     "iopub.status.idle": "2023-01-11T23:37:12.539261Z",
     "shell.execute_reply": "2023-01-11T23:37:12.538603Z",
     "shell.execute_reply.started": "2023-01-11T23:37:11.655315Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_df.to_csv(\"t_loss_lc.csv\",index=False)\n",
    "train_acc_df.to_csv(\"t_acc_lc.csv\",index=False)\n",
    "val_loss_df.to_csv(\"v_loss_lc.csv\",index=False)\n",
    "val_acc_df.to_csv(\"v_acc_lc.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cebfcc73-78f6-4345-ad03-442e9573898f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T23:37:12.540736Z",
     "iopub.status.busy": "2023-01-11T23:37:12.540105Z",
     "iopub.status.idle": "2023-01-11T23:43:28.809171Z",
     "shell.execute_reply": "2023-01-11T23:43:28.808481Z",
     "shell.execute_reply.started": "2023-01-11T23:37:12.540677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.82\n",
      "86.28\n",
      "87.92\n",
      "87.96\n",
      "89.1\n",
      "88.83\n",
      "89.4\n",
      "88.8\n",
      "88.48\n",
      "89.66\n",
      "88.81\n",
      "89.43\n",
      "89.48\n",
      "89.02\n",
      "89.66\n",
      "88.77\n",
      "88.68\n",
      "89.74\n",
      "88.94\n",
      "89.54\n",
      "[0.010846050864458084, 0.012131248204410076, 0.010629665391519666, 0.012602807514369489, 0.010652915196120738, 0.012366693031042814, 0.0117666459152475, 0.012304335830360651, 0.01149319866001606, 0.011627317145839334, 0.01211261975876987, 0.010741387387178837, 0.01207820691820234, 0.012832903622835875, 0.01526280838958919, 0.01652849762365222, 0.016938083976507187, 0.01729756361544132, 0.016354300340265037, 0.01668296679109335] [83.82, 86.28, 87.92, 87.96, 89.1, 88.83, 89.4, 88.8, 88.48, 89.66, 88.81, 89.43, 89.48, 89.02, 89.66, 88.77, 88.68, 89.74, 88.94, 89.54]\n",
      "86.81\n",
      "85.81\n",
      "88.42\n",
      "88.97\n",
      "88.35\n",
      "89.09\n",
      "88.75\n",
      "88.54\n",
      "89.47\n",
      "89.29\n",
      "89.73\n",
      "89.4\n",
      "88.57\n",
      "89.52\n",
      "89.4\n",
      "88.92\n",
      "89.48\n",
      "89.04\n",
      "89.23\n",
      "89.6\n",
      "[0.008059917865693569, 0.01118792281150818, 0.009270594184845686, 0.010665647437423468, 0.01136741586253047, 0.011137138981372119, 0.010491987311095, 0.01214660474807024, 0.011603502813726664, 0.011374981324374676, 0.009118623530119658, 0.011391552502661944, 0.013191667905449867, 0.012994775849580765, 0.012313245879113675, 0.01380393495708704, 0.01518552156686783, 0.01569109952263534, 0.017435381832998247, 0.018849031304847448] [86.81, 85.81, 88.42, 88.97, 88.35, 89.09, 88.75, 88.54, 89.47, 89.29, 89.73, 89.4, 88.57, 89.52, 89.4, 88.92, 89.48, 89.04, 89.23, 89.6]\n",
      "84.34\n",
      "87.78\n",
      "88.92\n",
      "88.16\n",
      "89.17\n",
      "89.23\n",
      "88.9\n",
      "89.48\n",
      "88.64\n",
      "88.9\n",
      "89.51\n",
      "88.93\n",
      "89.57\n",
      "89.5\n",
      "88.95\n",
      "89.59\n",
      "89.53\n",
      "89.21\n",
      "89.87\n",
      "89.2\n",
      "[0.010104749508202076, 0.007764813119918108, 0.009389431972801685, 0.01121964718028903, 0.009999204163253308, 0.010451857222989202, 0.011230444985628129, 0.010529740725457669, 0.011435162401199342, 0.012010734586417675, 0.012209670931473375, 0.013503068310045637, 0.010283273847773671, 0.013340305047668517, 0.014147798460815103, 0.013970169936865569, 0.015175802303105593, 0.01699589610286057, 0.016283099517947996, 0.020953092347085474] [84.34, 87.78, 88.92, 88.16, 89.17, 89.23, 88.9, 89.48, 88.64, 88.9, 89.51, 88.93, 89.57, 89.5, 88.95, 89.59, 89.53, 89.21, 89.87, 89.2]\n"
     ]
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_acc_df = pd.DataFrame()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(trainset, val_idx_df[i]),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    for k in range(20):\n",
    "        net.load_state_dict(torch.load(f\"{i}/epoch{k}_least_confidence_heuristic.pt\"))\n",
    "        \n",
    "        v_loss, v_acc = validate(net, val_loader)\n",
    "        t_loss, t_acc = validate(net, test_loader)\n",
    "        validation_loss.append(v_loss)\n",
    "        validation_acc.append(v_acc)\n",
    "        test_loss.append(t_loss)\n",
    "        test_acc.append(t_acc)\n",
    "        print(t_acc)\n",
    "\n",
    "    test_loss_df[i] = test_loss\n",
    "    test_acc_df[i] = test_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5a429b-9736-4c28-b87b-ceb1117b666b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T23:43:28.810568Z",
     "iopub.status.busy": "2023-01-11T23:43:28.810123Z",
     "iopub.status.idle": "2023-01-11T23:43:29.534623Z",
     "shell.execute_reply": "2023-01-11T23:43:29.533263Z",
     "shell.execute_reply.started": "2023-01-11T23:43:28.810549Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.to_csv(\"val_loss_lc.csv\",index=False)\n",
    "val_acc_df.to_csv(\"val_acc_lc.csv\",index=False)\n",
    "test_loss_df.to_csv(\"test_loss_lc.csv\",index=False)\n",
    "test_acc_df.to_csv(\"test_acc_lc.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151a678-5157-4bd3-acc8-babb57ff9c12",
   "metadata": {},
   "source": [
    "### Najmniejszy margines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3cfc5cc-1324-43c8-9158-06a47d7a35f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T06:46:57.978502Z",
     "iopub.status.busy": "2023-01-12T06:46:57.978185Z",
     "iopub.status.idle": "2023-01-12T09:45:49.939444Z",
     "shell.execute_reply": "2023-01-12T09:45:49.938715Z",
     "shell.execute_reply.started": "2023-01-12T06:46:57.978477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.51 train loss: 0.0139 val acc: 81.77 val loss: 0.0091\n",
      "Sub epoch 1 train acc: 84.57 train loss: 0.0069 val acc: 85.93 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 88.08 train loss: 0.0051 val acc: 86.73 val loss: 0.0061\n",
      "Sub epoch 3 train acc: 90.81 train loss: 0.0040 val acc: 84.72 val loss: 0.0071\n",
      "Sub epoch 4 train acc: 92.00 train loss: 0.0035 val acc: 85.12 val loss: 0.0090\n",
      "Sub epoch 5 train acc: 93.08 train loss: 0.0031 val acc: 84.83 val loss: 0.0095\n",
      "Sub epoch 6 train acc: 93.38 train loss: 0.0030 val acc: 87.03 val loss: 0.0077\n",
      "Train Loss: 0.0030, Train Acc: 93.38  Validation Loss: 0.0077, Validation Acc: 87.03\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 72.36 train loss: 0.0168 val acc: 85.85 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 85.47 train loss: 0.0062 val acc: 87.27 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 89.62 train loss: 0.0041 val acc: 88.17 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 91.80 train loss: 0.0033 val acc: 87.82 val loss: 0.0074\n",
      "Sub epoch 4 train acc: 93.73 train loss: 0.0026 val acc: 87.82 val loss: 0.0079\n",
      "Sub epoch 5 train acc: 95.16 train loss: 0.0020 val acc: 87.98 val loss: 0.0087\n",
      "Sub epoch 6 train acc: 95.77 train loss: 0.0020 val acc: 87.48 val loss: 0.0098\n",
      "Train Loss: 0.0020, Train Acc: 95.77  Validation Loss: 0.0098, Validation Acc: 87.48\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 68.59 train loss: 0.0189 val acc: 86.27 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 86.69 train loss: 0.0057 val acc: 88.63 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 91.56 train loss: 0.0036 val acc: 88.25 val loss: 0.0075\n",
      "Sub epoch 3 train acc: 93.95 train loss: 0.0026 val acc: 88.50 val loss: 0.0085\n",
      "Sub epoch 4 train acc: 95.58 train loss: 0.0019 val acc: 88.82 val loss: 0.0091\n",
      "Sub epoch 5 train acc: 95.83 train loss: 0.0017 val acc: 88.78 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 96.27 train loss: 0.0016 val acc: 88.80 val loss: 0.0108\n",
      "Train Loss: 0.0016, Train Acc: 96.27  Validation Loss: 0.0108, Validation Acc: 88.80\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 65.96 train loss: 0.0197 val acc: 84.92 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 86.64 train loss: 0.0057 val acc: 89.12 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 92.20 train loss: 0.0034 val acc: 89.27 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 93.92 train loss: 0.0025 val acc: 89.63 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 97.47 train loss: 0.0012 val acc: 89.93 val loss: 0.0087\n",
      "Sub epoch 5 train acc: 98.37 train loss: 0.0008 val acc: 89.97 val loss: 0.0093\n",
      "Sub epoch 6 train acc: 98.69 train loss: 0.0007 val acc: 90.00 val loss: 0.0100\n",
      "Train Loss: 0.0007, Train Acc: 98.69  Validation Loss: 0.0100, Validation Acc: 90.00\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 62.86 train loss: 0.0204 val acc: 85.93 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 86.15 train loss: 0.0059 val acc: 89.55 val loss: 0.0060\n",
      "Sub epoch 2 train acc: 91.16 train loss: 0.0037 val acc: 89.83 val loss: 0.0064\n",
      "Sub epoch 3 train acc: 94.01 train loss: 0.0026 val acc: 89.38 val loss: 0.0079\n",
      "Sub epoch 4 train acc: 94.97 train loss: 0.0022 val acc: 90.05 val loss: 0.0086\n",
      "Sub epoch 5 train acc: 95.43 train loss: 0.0019 val acc: 89.42 val loss: 0.0094\n",
      "Sub epoch 6 train acc: 96.05 train loss: 0.0017 val acc: 89.35 val loss: 0.0096\n",
      "Train Loss: 0.0017, Train Acc: 96.05  Validation Loss: 0.0096, Validation Acc: 89.35\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 65.21 train loss: 0.0191 val acc: 87.57 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 90.78 train loss: 0.0042 val acc: 89.62 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 94.88 train loss: 0.0023 val acc: 89.57 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 96.04 train loss: 0.0018 val acc: 89.65 val loss: 0.0087\n",
      "Sub epoch 4 train acc: 96.67 train loss: 0.0015 val acc: 90.02 val loss: 0.0090\n",
      "Sub epoch 5 train acc: 96.96 train loss: 0.0013 val acc: 89.93 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 97.00 train loss: 0.0014 val acc: 89.10 val loss: 0.0109\n",
      "Train Loss: 0.0014, Train Acc: 97.00  Validation Loss: 0.0109, Validation Acc: 89.10\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 65.84 train loss: 0.0190 val acc: 87.07 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 92.18 train loss: 0.0037 val acc: 89.88 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 96.13 train loss: 0.0018 val acc: 89.87 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 97.03 train loss: 0.0014 val acc: 89.88 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 97.76 train loss: 0.0011 val acc: 89.55 val loss: 0.0095\n",
      "Sub epoch 5 train acc: 99.23 train loss: 0.0004 val acc: 90.00 val loss: 0.0100\n",
      "Sub epoch 6 train acc: 99.61 train loss: 0.0003 val acc: 89.98 val loss: 0.0105\n",
      "Train Loss: 0.0003, Train Acc: 99.61  Validation Loss: 0.0105, Validation Acc: 89.98\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 65.57 train loss: 0.0187 val acc: 87.28 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 93.13 train loss: 0.0034 val acc: 89.50 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 96.61 train loss: 0.0017 val acc: 90.00 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 97.33 train loss: 0.0013 val acc: 89.65 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 97.45 train loss: 0.0012 val acc: 89.65 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 97.80 train loss: 0.0010 val acc: 89.75 val loss: 0.0097\n",
      "Sub epoch 6 train acc: 97.64 train loss: 0.0011 val acc: 88.95 val loss: 0.0107\n",
      "Train Loss: 0.0011, Train Acc: 97.64  Validation Loss: 0.0107, Validation Acc: 88.95\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 68.80 train loss: 0.0177 val acc: 87.97 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 94.92 train loss: 0.0026 val acc: 90.10 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 98.44 train loss: 0.0010 val acc: 90.32 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 98.90 train loss: 0.0008 val acc: 90.32 val loss: 0.0074\n",
      "Sub epoch 4 train acc: 99.07 train loss: 0.0007 val acc: 90.48 val loss: 0.0076\n",
      "Sub epoch 5 train acc: 99.20 train loss: 0.0006 val acc: 90.28 val loss: 0.0078\n",
      "Sub epoch 6 train acc: 99.25 train loss: 0.0005 val acc: 90.37 val loss: 0.0081\n",
      "Train Loss: 0.0005, Train Acc: 99.25  Validation Loss: 0.0081, Validation Acc: 90.37\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 68.42 train loss: 0.0169 val acc: 88.70 val loss: 0.0054\n",
      "Sub epoch 1 train acc: 94.47 train loss: 0.0028 val acc: 90.00 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 97.23 train loss: 0.0015 val acc: 90.03 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 97.96 train loss: 0.0011 val acc: 89.98 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 98.18 train loss: 0.0009 val acc: 89.77 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 97.87 train loss: 0.0011 val acc: 89.23 val loss: 0.0100\n",
      "Sub epoch 6 train acc: 97.87 train loss: 0.0010 val acc: 89.73 val loss: 0.0099\n",
      "Train Loss: 0.0010, Train Acc: 97.87  Validation Loss: 0.0099, Validation Acc: 89.73\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 73.57 train loss: 0.0150 val acc: 89.45 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 97.03 train loss: 0.0016 val acc: 90.02 val loss: 0.0080\n",
      "Sub epoch 2 train acc: 98.83 train loss: 0.0007 val acc: 89.85 val loss: 0.0089\n",
      "Sub epoch 3 train acc: 99.06 train loss: 0.0005 val acc: 89.40 val loss: 0.0111\n",
      "Sub epoch 4 train acc: 98.70 train loss: 0.0006 val acc: 89.60 val loss: 0.0106\n",
      "Sub epoch 5 train acc: 99.62 train loss: 0.0002 val acc: 90.25 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 99.86 train loss: 0.0001 val acc: 90.33 val loss: 0.0116\n",
      "Train Loss: 0.0001, Train Acc: 99.86  Validation Loss: 0.0116, Validation Acc: 90.33\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 72.52 train loss: 0.0153 val acc: 89.12 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 96.51 train loss: 0.0016 val acc: 89.95 val loss: 0.0082\n",
      "Sub epoch 2 train acc: 98.82 train loss: 0.0006 val acc: 89.72 val loss: 0.0105\n",
      "Sub epoch 3 train acc: 99.16 train loss: 0.0004 val acc: 90.10 val loss: 0.0119\n",
      "Sub epoch 4 train acc: 98.95 train loss: 0.0005 val acc: 89.67 val loss: 0.0110\n",
      "Sub epoch 5 train acc: 99.03 train loss: 0.0005 val acc: 89.83 val loss: 0.0119\n",
      "Sub epoch 6 train acc: 98.57 train loss: 0.0007 val acc: 89.37 val loss: 0.0115\n",
      "Train Loss: 0.0007, Train Acc: 98.57  Validation Loss: 0.0115, Validation Acc: 89.37\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 75.91 train loss: 0.0146 val acc: 88.33 val loss: 0.0058\n",
      "Sub epoch 1 train acc: 97.33 train loss: 0.0013 val acc: 90.00 val loss: 0.0091\n",
      "Sub epoch 2 train acc: 99.52 train loss: 0.0003 val acc: 90.12 val loss: 0.0095\n",
      "Sub epoch 3 train acc: 99.72 train loss: 0.0002 val acc: 90.15 val loss: 0.0100\n",
      "Sub epoch 4 train acc: 99.79 train loss: 0.0002 val acc: 90.07 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 99.86 train loss: 0.0001 val acc: 90.05 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 99.90 train loss: 0.0001 val acc: 90.13 val loss: 0.0113\n",
      "Train Loss: 0.0001, Train Acc: 99.90  Validation Loss: 0.0113, Validation Acc: 90.13\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 73.81 train loss: 0.0152 val acc: 88.60 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 96.36 train loss: 0.0018 val acc: 89.95 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 98.89 train loss: 0.0006 val acc: 89.92 val loss: 0.0098\n",
      "Sub epoch 3 train acc: 99.12 train loss: 0.0004 val acc: 89.88 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 99.07 train loss: 0.0005 val acc: 90.10 val loss: 0.0115\n",
      "Sub epoch 5 train acc: 98.83 train loss: 0.0006 val acc: 89.50 val loss: 0.0122\n",
      "Sub epoch 6 train acc: 99.64 train loss: 0.0002 val acc: 90.15 val loss: 0.0117\n",
      "Train Loss: 0.0002, Train Acc: 99.64  Validation Loss: 0.0117, Validation Acc: 90.15\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 77.39 train loss: 0.0133 val acc: 89.67 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 97.95 train loss: 0.0011 val acc: 90.17 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 99.40 train loss: 0.0003 val acc: 90.10 val loss: 0.0111\n",
      "Sub epoch 3 train acc: 99.50 train loss: 0.0003 val acc: 90.03 val loss: 0.0126\n",
      "Sub epoch 4 train acc: 99.17 train loss: 0.0004 val acc: 89.95 val loss: 0.0121\n",
      "Sub epoch 5 train acc: 99.05 train loss: 0.0005 val acc: 89.68 val loss: 0.0135\n",
      "Sub epoch 6 train acc: 99.03 train loss: 0.0005 val acc: 89.23 val loss: 0.0135\n",
      "Train Loss: 0.0005, Train Acc: 99.03  Validation Loss: 0.0135, Validation Acc: 89.23\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 79.12 train loss: 0.0128 val acc: 89.07 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 98.38 train loss: 0.0009 val acc: 90.03 val loss: 0.0104\n",
      "Sub epoch 2 train acc: 99.60 train loss: 0.0002 val acc: 89.62 val loss: 0.0120\n",
      "Sub epoch 3 train acc: 99.86 train loss: 0.0001 val acc: 90.05 val loss: 0.0122\n",
      "Sub epoch 4 train acc: 99.94 train loss: 0.0001 val acc: 90.25 val loss: 0.0126\n",
      "Sub epoch 5 train acc: 99.95 train loss: 0.0001 val acc: 90.07 val loss: 0.0131\n",
      "Sub epoch 6 train acc: 99.96 train loss: 0.0000 val acc: 90.08 val loss: 0.0132\n",
      "Train Loss: 0.0000, Train Acc: 99.96  Validation Loss: 0.0132, Validation Acc: 90.08\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 77.28 train loss: 0.0136 val acc: 87.50 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 97.42 train loss: 0.0012 val acc: 89.82 val loss: 0.0089\n",
      "Sub epoch 2 train acc: 99.34 train loss: 0.0003 val acc: 90.08 val loss: 0.0122\n",
      "Sub epoch 3 train acc: 99.44 train loss: 0.0003 val acc: 89.47 val loss: 0.0125\n",
      "Sub epoch 4 train acc: 99.33 train loss: 0.0003 val acc: 90.03 val loss: 0.0135\n",
      "Sub epoch 5 train acc: 99.21 train loss: 0.0004 val acc: 89.27 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 99.12 train loss: 0.0005 val acc: 88.93 val loss: 0.0125\n",
      "Train Loss: 0.0005, Train Acc: 99.12  Validation Loss: 0.0125, Validation Acc: 88.93\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 80.68 train loss: 0.0125 val acc: 89.33 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 98.60 train loss: 0.0008 val acc: 89.75 val loss: 0.0111\n",
      "Sub epoch 2 train acc: 99.64 train loss: 0.0002 val acc: 89.48 val loss: 0.0121\n",
      "Sub epoch 3 train acc: 99.68 train loss: 0.0002 val acc: 90.05 val loss: 0.0119\n",
      "Sub epoch 4 train acc: 99.63 train loss: 0.0002 val acc: 90.05 val loss: 0.0141\n",
      "Sub epoch 5 train acc: 99.35 train loss: 0.0003 val acc: 89.70 val loss: 0.0139\n",
      "Sub epoch 6 train acc: 99.13 train loss: 0.0004 val acc: 89.55 val loss: 0.0128\n",
      "Train Loss: 0.0004, Train Acc: 99.13  Validation Loss: 0.0128, Validation Acc: 89.55\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 80.04 train loss: 0.0127 val acc: 89.18 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 98.73 train loss: 0.0007 val acc: 89.78 val loss: 0.0111\n",
      "Sub epoch 2 train acc: 99.67 train loss: 0.0002 val acc: 90.02 val loss: 0.0125\n",
      "Sub epoch 3 train acc: 99.66 train loss: 0.0002 val acc: 89.70 val loss: 0.0140\n",
      "Sub epoch 4 train acc: 99.87 train loss: 0.0001 val acc: 90.08 val loss: 0.0142\n",
      "Sub epoch 5 train acc: 99.98 train loss: 0.0000 val acc: 90.10 val loss: 0.0149\n",
      "Sub epoch 6 train acc: 99.98 train loss: 0.0000 val acc: 90.07 val loss: 0.0156\n",
      "Train Loss: 0.0000, Train Acc: 99.98  Validation Loss: 0.0156, Validation Acc: 90.07\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 79.65 train loss: 0.0128 val acc: 88.88 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 98.36 train loss: 0.0008 val acc: 90.17 val loss: 0.0106\n",
      "Sub epoch 2 train acc: 99.62 train loss: 0.0002 val acc: 90.22 val loss: 0.0125\n",
      "Sub epoch 3 train acc: 99.56 train loss: 0.0002 val acc: 89.83 val loss: 0.0130\n",
      "Sub epoch 4 train acc: 99.43 train loss: 0.0003 val acc: 90.10 val loss: 0.0135\n",
      "Sub epoch 5 train acc: 99.29 train loss: 0.0004 val acc: 89.67 val loss: 0.0145\n",
      "Sub epoch 6 train acc: 99.32 train loss: 0.0004 val acc: 89.78 val loss: 0.0148\n",
      "Train Loss: 0.0004, Train Acc: 99.32  Validation Loss: 0.0148, Validation Acc: 89.78\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.69 train loss: 0.0142 val acc: 80.80 val loss: 0.0091\n",
      "Sub epoch 1 train acc: 86.50 train loss: 0.0059 val acc: 85.72 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 89.10 train loss: 0.0047 val acc: 86.37 val loss: 0.0060\n",
      "Sub epoch 3 train acc: 90.96 train loss: 0.0038 val acc: 87.03 val loss: 0.0058\n",
      "Sub epoch 4 train acc: 92.60 train loss: 0.0032 val acc: 86.72 val loss: 0.0059\n",
      "Sub epoch 5 train acc: 94.11 train loss: 0.0025 val acc: 87.08 val loss: 0.0061\n",
      "Sub epoch 6 train acc: 95.60 train loss: 0.0020 val acc: 87.15 val loss: 0.0065\n",
      "Train Loss: 0.0020, Train Acc: 95.60  Validation Loss: 0.0065, Validation Acc: 87.15\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 65.03 train loss: 0.0218 val acc: 77.40 val loss: 0.0102\n",
      "Sub epoch 1 train acc: 79.23 train loss: 0.0088 val acc: 85.27 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 84.44 train loss: 0.0062 val acc: 87.02 val loss: 0.0060\n",
      "Sub epoch 3 train acc: 87.95 train loss: 0.0049 val acc: 87.05 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 90.05 train loss: 0.0041 val acc: 87.13 val loss: 0.0073\n",
      "Sub epoch 5 train acc: 95.01 train loss: 0.0022 val acc: 88.87 val loss: 0.0065\n",
      "Sub epoch 6 train acc: 96.73 train loss: 0.0016 val acc: 88.80 val loss: 0.0069\n",
      "Train Loss: 0.0016, Train Acc: 96.73  Validation Loss: 0.0069, Validation Acc: 88.80\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 64.93 train loss: 0.0197 val acc: 82.45 val loss: 0.0085\n",
      "Sub epoch 1 train acc: 80.81 train loss: 0.0076 val acc: 87.70 val loss: 0.0059\n",
      "Sub epoch 2 train acc: 86.46 train loss: 0.0052 val acc: 88.12 val loss: 0.0062\n",
      "Sub epoch 3 train acc: 89.89 train loss: 0.0041 val acc: 88.17 val loss: 0.0074\n",
      "Sub epoch 4 train acc: 92.11 train loss: 0.0032 val acc: 88.70 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 93.13 train loss: 0.0028 val acc: 88.77 val loss: 0.0081\n",
      "Sub epoch 6 train acc: 94.41 train loss: 0.0023 val acc: 87.73 val loss: 0.0096\n",
      "Train Loss: 0.0023, Train Acc: 94.41  Validation Loss: 0.0096, Validation Acc: 87.73\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 64.30 train loss: 0.0190 val acc: 81.78 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 85.60 train loss: 0.0060 val acc: 88.43 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 93.26 train loss: 0.0031 val acc: 89.23 val loss: 0.0064\n",
      "Sub epoch 3 train acc: 94.51 train loss: 0.0025 val acc: 89.53 val loss: 0.0067\n",
      "Sub epoch 4 train acc: 95.26 train loss: 0.0022 val acc: 89.45 val loss: 0.0071\n",
      "Sub epoch 5 train acc: 95.97 train loss: 0.0019 val acc: 89.40 val loss: 0.0076\n",
      "Sub epoch 6 train acc: 96.53 train loss: 0.0017 val acc: 89.32 val loss: 0.0078\n",
      "Train Loss: 0.0017, Train Acc: 96.53  Validation Loss: 0.0078, Validation Acc: 89.32\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 61.86 train loss: 0.0196 val acc: 86.17 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 84.01 train loss: 0.0066 val acc: 89.30 val loss: 0.0054\n",
      "Sub epoch 2 train acc: 89.30 train loss: 0.0045 val acc: 88.70 val loss: 0.0066\n",
      "Sub epoch 3 train acc: 91.55 train loss: 0.0035 val acc: 88.92 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 93.43 train loss: 0.0028 val acc: 88.80 val loss: 0.0076\n",
      "Sub epoch 5 train acc: 94.09 train loss: 0.0025 val acc: 89.15 val loss: 0.0087\n",
      "Sub epoch 6 train acc: 94.82 train loss: 0.0022 val acc: 88.67 val loss: 0.0096\n",
      "Train Loss: 0.0022, Train Acc: 94.82  Validation Loss: 0.0096, Validation Acc: 88.67\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 64.88 train loss: 0.0184 val acc: 86.08 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 89.70 train loss: 0.0046 val acc: 89.72 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 94.12 train loss: 0.0027 val acc: 89.37 val loss: 0.0078\n",
      "Sub epoch 3 train acc: 95.46 train loss: 0.0021 val acc: 89.57 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 96.03 train loss: 0.0017 val acc: 89.10 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 96.38 train loss: 0.0016 val acc: 89.12 val loss: 0.0099\n",
      "Sub epoch 6 train acc: 98.47 train loss: 0.0007 val acc: 89.92 val loss: 0.0102\n",
      "Train Loss: 0.0007, Train Acc: 98.47  Validation Loss: 0.0102, Validation Acc: 89.92\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 65.43 train loss: 0.0177 val acc: 87.73 val loss: 0.0061\n",
      "Sub epoch 1 train acc: 91.93 train loss: 0.0039 val acc: 89.82 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 95.30 train loss: 0.0022 val acc: 89.80 val loss: 0.0081\n",
      "Sub epoch 3 train acc: 96.56 train loss: 0.0021 val acc: 88.87 val loss: 0.0093\n",
      "Sub epoch 4 train acc: 96.92 train loss: 0.0014 val acc: 89.20 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 97.06 train loss: 0.0013 val acc: 89.37 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 96.59 train loss: 0.0016 val acc: 89.10 val loss: 0.0108\n",
      "Train Loss: 0.0016, Train Acc: 96.59  Validation Loss: 0.0108, Validation Acc: 89.10\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 68.86 train loss: 0.0163 val acc: 88.10 val loss: 0.0058\n",
      "Sub epoch 1 train acc: 94.10 train loss: 0.0029 val acc: 90.00 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 97.12 train loss: 0.0016 val acc: 89.22 val loss: 0.0085\n",
      "Sub epoch 3 train acc: 98.86 train loss: 0.0007 val acc: 89.85 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 99.24 train loss: 0.0005 val acc: 89.85 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 99.43 train loss: 0.0004 val acc: 89.97 val loss: 0.0098\n",
      "Sub epoch 6 train acc: 99.49 train loss: 0.0004 val acc: 89.98 val loss: 0.0102\n",
      "Train Loss: 0.0004, Train Acc: 99.49  Validation Loss: 0.0102, Validation Acc: 89.98\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 67.85 train loss: 0.0172 val acc: 88.50 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 93.43 train loss: 0.0033 val acc: 89.87 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 96.65 train loss: 0.0018 val acc: 90.02 val loss: 0.0074\n",
      "Sub epoch 3 train acc: 97.59 train loss: 0.0013 val acc: 89.57 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 97.41 train loss: 0.0012 val acc: 89.03 val loss: 0.0099\n",
      "Sub epoch 5 train acc: 97.70 train loss: 0.0011 val acc: 89.40 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 97.43 train loss: 0.0012 val acc: 89.48 val loss: 0.0098\n",
      "Train Loss: 0.0012, Train Acc: 97.43  Validation Loss: 0.0098, Validation Acc: 89.48\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 72.45 train loss: 0.0147 val acc: 88.37 val loss: 0.0054\n",
      "Sub epoch 1 train acc: 96.33 train loss: 0.0020 val acc: 89.43 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 98.47 train loss: 0.0008 val acc: 89.38 val loss: 0.0100\n",
      "Sub epoch 3 train acc: 98.80 train loss: 0.0006 val acc: 89.08 val loss: 0.0109\n",
      "Sub epoch 4 train acc: 98.44 train loss: 0.0008 val acc: 89.50 val loss: 0.0107\n",
      "Sub epoch 5 train acc: 98.52 train loss: 0.0007 val acc: 89.40 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 98.36 train loss: 0.0008 val acc: 89.13 val loss: 0.0127\n",
      "Train Loss: 0.0008, Train Acc: 98.36  Validation Loss: 0.0127, Validation Acc: 89.13\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 73.73 train loss: 0.0149 val acc: 88.57 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 96.85 train loss: 0.0017 val acc: 89.40 val loss: 0.0084\n",
      "Sub epoch 2 train acc: 98.99 train loss: 0.0006 val acc: 89.08 val loss: 0.0101\n",
      "Sub epoch 3 train acc: 99.04 train loss: 0.0005 val acc: 89.42 val loss: 0.0111\n",
      "Sub epoch 4 train acc: 99.67 train loss: 0.0002 val acc: 89.72 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 99.87 train loss: 0.0001 val acc: 89.70 val loss: 0.0111\n",
      "Sub epoch 6 train acc: 99.89 train loss: 0.0001 val acc: 89.85 val loss: 0.0115\n",
      "Train Loss: 0.0001, Train Acc: 99.89  Validation Loss: 0.0115, Validation Acc: 89.85\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 73.61 train loss: 0.0149 val acc: 87.95 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 96.41 train loss: 0.0018 val acc: 89.28 val loss: 0.0083\n",
      "Sub epoch 2 train acc: 98.58 train loss: 0.0008 val acc: 89.28 val loss: 0.0089\n",
      "Sub epoch 3 train acc: 98.94 train loss: 0.0005 val acc: 89.42 val loss: 0.0101\n",
      "Sub epoch 4 train acc: 99.22 train loss: 0.0004 val acc: 89.13 val loss: 0.0111\n",
      "Sub epoch 5 train acc: 98.61 train loss: 0.0007 val acc: 88.93 val loss: 0.0116\n",
      "Sub epoch 6 train acc: 98.42 train loss: 0.0008 val acc: 89.13 val loss: 0.0108\n",
      "Train Loss: 0.0008, Train Acc: 98.42  Validation Loss: 0.0108, Validation Acc: 89.13\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 76.79 train loss: 0.0135 val acc: 89.00 val loss: 0.0061\n",
      "Sub epoch 1 train acc: 95.33 train loss: 0.0024 val acc: 89.77 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 97.53 train loss: 0.0013 val acc: 89.98 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 98.56 train loss: 0.0008 val acc: 89.90 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 99.15 train loss: 0.0005 val acc: 89.68 val loss: 0.0091\n",
      "Sub epoch 5 train acc: 99.47 train loss: 0.0004 val acc: 89.83 val loss: 0.0098\n",
      "Sub epoch 6 train acc: 99.64 train loss: 0.0003 val acc: 89.82 val loss: 0.0104\n",
      "Train Loss: 0.0003, Train Acc: 99.64  Validation Loss: 0.0104, Validation Acc: 89.82\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 76.82 train loss: 0.0140 val acc: 88.73 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 96.92 train loss: 0.0016 val acc: 89.58 val loss: 0.0088\n",
      "Sub epoch 2 train acc: 98.88 train loss: 0.0006 val acc: 89.02 val loss: 0.0093\n",
      "Sub epoch 3 train acc: 99.24 train loss: 0.0004 val acc: 89.38 val loss: 0.0111\n",
      "Sub epoch 4 train acc: 99.05 train loss: 0.0005 val acc: 89.50 val loss: 0.0121\n",
      "Sub epoch 5 train acc: 99.73 train loss: 0.0002 val acc: 89.83 val loss: 0.0121\n",
      "Sub epoch 6 train acc: 99.90 train loss: 0.0001 val acc: 89.87 val loss: 0.0125\n",
      "Train Loss: 0.0001, Train Acc: 99.90  Validation Loss: 0.0125, Validation Acc: 89.87\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 77.44 train loss: 0.0136 val acc: 88.98 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 97.28 train loss: 0.0014 val acc: 89.87 val loss: 0.0098\n",
      "Sub epoch 2 train acc: 99.09 train loss: 0.0005 val acc: 89.65 val loss: 0.0111\n",
      "Sub epoch 3 train acc: 99.29 train loss: 0.0004 val acc: 89.72 val loss: 0.0120\n",
      "Sub epoch 4 train acc: 99.38 train loss: 0.0003 val acc: 89.47 val loss: 0.0125\n",
      "Sub epoch 5 train acc: 98.91 train loss: 0.0006 val acc: 89.32 val loss: 0.0136\n",
      "Sub epoch 6 train acc: 98.71 train loss: 0.0007 val acc: 89.25 val loss: 0.0127\n",
      "Train Loss: 0.0007, Train Acc: 98.71  Validation Loss: 0.0127, Validation Acc: 89.25\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 79.94 train loss: 0.0120 val acc: 89.02 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 98.43 train loss: 0.0009 val acc: 89.55 val loss: 0.0099\n",
      "Sub epoch 2 train acc: 99.76 train loss: 0.0002 val acc: 89.53 val loss: 0.0103\n",
      "Sub epoch 3 train acc: 99.86 train loss: 0.0001 val acc: 89.63 val loss: 0.0110\n",
      "Sub epoch 4 train acc: 99.90 train loss: 0.0001 val acc: 89.60 val loss: 0.0116\n",
      "Sub epoch 5 train acc: 99.93 train loss: 0.0001 val acc: 89.80 val loss: 0.0121\n",
      "Sub epoch 6 train acc: 99.94 train loss: 0.0001 val acc: 89.67 val loss: 0.0127\n",
      "Train Loss: 0.0001, Train Acc: 99.94  Validation Loss: 0.0127, Validation Acc: 89.67\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 77.67 train loss: 0.0133 val acc: 87.87 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 97.14 train loss: 0.0014 val acc: 89.23 val loss: 0.0094\n",
      "Sub epoch 2 train acc: 99.26 train loss: 0.0004 val acc: 89.80 val loss: 0.0122\n",
      "Sub epoch 3 train acc: 99.35 train loss: 0.0003 val acc: 89.33 val loss: 0.0124\n",
      "Sub epoch 4 train acc: 99.29 train loss: 0.0004 val acc: 90.00 val loss: 0.0124\n",
      "Sub epoch 5 train acc: 99.09 train loss: 0.0004 val acc: 89.27 val loss: 0.0129\n",
      "Sub epoch 6 train acc: 99.76 train loss: 0.0001 val acc: 89.77 val loss: 0.0130\n",
      "Train Loss: 0.0001, Train Acc: 99.76  Validation Loss: 0.0130, Validation Acc: 89.77\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 80.12 train loss: 0.0122 val acc: 88.50 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 98.23 train loss: 0.0009 val acc: 89.83 val loss: 0.0102\n",
      "Sub epoch 2 train acc: 99.57 train loss: 0.0003 val acc: 89.60 val loss: 0.0120\n",
      "Sub epoch 3 train acc: 99.64 train loss: 0.0002 val acc: 89.08 val loss: 0.0147\n",
      "Sub epoch 4 train acc: 99.41 train loss: 0.0003 val acc: 89.42 val loss: 0.0137\n",
      "Sub epoch 5 train acc: 99.28 train loss: 0.0004 val acc: 89.42 val loss: 0.0135\n",
      "Sub epoch 6 train acc: 99.14 train loss: 0.0004 val acc: 89.33 val loss: 0.0139\n",
      "Train Loss: 0.0004, Train Acc: 99.14  Validation Loss: 0.0139, Validation Acc: 89.33\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 82.34 train loss: 0.0111 val acc: 88.90 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 98.72 train loss: 0.0007 val acc: 89.88 val loss: 0.0105\n",
      "Sub epoch 2 train acc: 99.67 train loss: 0.0002 val acc: 89.75 val loss: 0.0134\n",
      "Sub epoch 3 train acc: 99.90 train loss: 0.0001 val acc: 89.97 val loss: 0.0133\n",
      "Sub epoch 4 train acc: 99.96 train loss: 0.0000 val acc: 89.92 val loss: 0.0136\n",
      "Sub epoch 5 train acc: 99.98 train loss: 0.0000 val acc: 89.92 val loss: 0.0141\n",
      "Sub epoch 6 train acc: 99.98 train loss: 0.0000 val acc: 90.00 val loss: 0.0146\n",
      "Train Loss: 0.0000, Train Acc: 99.98  Validation Loss: 0.0146, Validation Acc: 90.00\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 79.07 train loss: 0.0131 val acc: 88.05 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 97.95 train loss: 0.0011 val acc: 89.50 val loss: 0.0110\n",
      "Sub epoch 2 train acc: 99.34 train loss: 0.0003 val acc: 89.77 val loss: 0.0129\n",
      "Sub epoch 3 train acc: 99.47 train loss: 0.0003 val acc: 89.40 val loss: 0.0133\n",
      "Sub epoch 4 train acc: 99.48 train loss: 0.0002 val acc: 89.72 val loss: 0.0151\n",
      "Sub epoch 5 train acc: 99.18 train loss: 0.0004 val acc: 89.23 val loss: 0.0169\n",
      "Sub epoch 6 train acc: 99.19 train loss: 0.0005 val acc: 89.70 val loss: 0.0152\n",
      "Train Loss: 0.0005, Train Acc: 99.19  Validation Loss: 0.0152, Validation Acc: 89.70\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.57 train loss: 0.0142 val acc: 79.18 val loss: 0.0106\n",
      "Sub epoch 1 train acc: 84.88 train loss: 0.0068 val acc: 83.20 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 88.58 train loss: 0.0049 val acc: 85.20 val loss: 0.0068\n",
      "Sub epoch 3 train acc: 90.69 train loss: 0.0041 val acc: 85.82 val loss: 0.0069\n",
      "Sub epoch 4 train acc: 92.06 train loss: 0.0034 val acc: 85.58 val loss: 0.0075\n",
      "Sub epoch 5 train acc: 93.70 train loss: 0.0028 val acc: 85.05 val loss: 0.0086\n",
      "Sub epoch 6 train acc: 94.03 train loss: 0.0027 val acc: 85.55 val loss: 0.0106\n",
      "Train Loss: 0.0027, Train Acc: 94.03  Validation Loss: 0.0106, Validation Acc: 85.55\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 72.44 train loss: 0.0170 val acc: 81.68 val loss: 0.0090\n",
      "Sub epoch 1 train acc: 85.37 train loss: 0.0060 val acc: 86.23 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 90.19 train loss: 0.0040 val acc: 86.97 val loss: 0.0079\n",
      "Sub epoch 3 train acc: 92.68 train loss: 0.0030 val acc: 86.93 val loss: 0.0088\n",
      "Sub epoch 4 train acc: 96.50 train loss: 0.0015 val acc: 87.97 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 97.79 train loss: 0.0011 val acc: 87.92 val loss: 0.0090\n",
      "Sub epoch 6 train acc: 98.33 train loss: 0.0008 val acc: 88.07 val loss: 0.0093\n",
      "Train Loss: 0.0008, Train Acc: 98.33  Validation Loss: 0.0093, Validation Acc: 88.07\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 65.85 train loss: 0.0201 val acc: 84.40 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 82.29 train loss: 0.0070 val acc: 87.23 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 87.64 train loss: 0.0049 val acc: 87.48 val loss: 0.0067\n",
      "Sub epoch 3 train acc: 90.78 train loss: 0.0037 val acc: 87.77 val loss: 0.0075\n",
      "Sub epoch 4 train acc: 93.09 train loss: 0.0028 val acc: 87.63 val loss: 0.0084\n",
      "Sub epoch 5 train acc: 94.01 train loss: 0.0024 val acc: 87.13 val loss: 0.0099\n",
      "Sub epoch 6 train acc: 94.41 train loss: 0.0023 val acc: 88.02 val loss: 0.0099\n",
      "Train Loss: 0.0023, Train Acc: 94.41  Validation Loss: 0.0099, Validation Acc: 88.02\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 65.33 train loss: 0.0192 val acc: 85.20 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 84.34 train loss: 0.0064 val acc: 87.92 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 88.28 train loss: 0.0048 val acc: 88.38 val loss: 0.0064\n",
      "Sub epoch 3 train acc: 90.58 train loss: 0.0039 val acc: 88.68 val loss: 0.0067\n",
      "Sub epoch 4 train acc: 92.40 train loss: 0.0033 val acc: 88.93 val loss: 0.0069\n",
      "Sub epoch 5 train acc: 93.70 train loss: 0.0029 val acc: 89.02 val loss: 0.0073\n",
      "Sub epoch 6 train acc: 94.72 train loss: 0.0024 val acc: 88.88 val loss: 0.0076\n",
      "Train Loss: 0.0024, Train Acc: 94.72  Validation Loss: 0.0076, Validation Acc: 88.88\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 63.32 train loss: 0.0193 val acc: 85.47 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 83.69 train loss: 0.0068 val acc: 88.43 val loss: 0.0063\n",
      "Sub epoch 2 train acc: 88.58 train loss: 0.0046 val acc: 88.65 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 91.44 train loss: 0.0036 val acc: 88.73 val loss: 0.0077\n",
      "Sub epoch 4 train acc: 93.23 train loss: 0.0027 val acc: 88.83 val loss: 0.0089\n",
      "Sub epoch 5 train acc: 97.04 train loss: 0.0014 val acc: 89.50 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 98.07 train loss: 0.0010 val acc: 89.75 val loss: 0.0098\n",
      "Train Loss: 0.0010, Train Acc: 98.07  Validation Loss: 0.0098, Validation Acc: 89.75\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 63.80 train loss: 0.0193 val acc: 85.80 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 87.46 train loss: 0.0055 val acc: 89.15 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 92.33 train loss: 0.0033 val acc: 89.20 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 94.36 train loss: 0.0024 val acc: 89.38 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 95.38 train loss: 0.0020 val acc: 89.62 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 95.64 train loss: 0.0019 val acc: 89.40 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 96.22 train loss: 0.0017 val acc: 88.52 val loss: 0.0115\n",
      "Train Loss: 0.0017, Train Acc: 96.22  Validation Loss: 0.0115, Validation Acc: 88.52\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 65.65 train loss: 0.0185 val acc: 86.32 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 92.04 train loss: 0.0037 val acc: 89.62 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 97.43 train loss: 0.0015 val acc: 89.65 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 97.96 train loss: 0.0012 val acc: 89.72 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 98.27 train loss: 0.0010 val acc: 89.80 val loss: 0.0084\n",
      "Sub epoch 5 train acc: 98.56 train loss: 0.0009 val acc: 89.62 val loss: 0.0087\n",
      "Sub epoch 6 train acc: 98.82 train loss: 0.0008 val acc: 89.93 val loss: 0.0091\n",
      "Train Loss: 0.0008, Train Acc: 98.82  Validation Loss: 0.0091, Validation Acc: 89.93\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 66.70 train loss: 0.0181 val acc: 87.40 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 91.57 train loss: 0.0041 val acc: 89.52 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 95.26 train loss: 0.0022 val acc: 89.22 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 96.48 train loss: 0.0016 val acc: 89.33 val loss: 0.0095\n",
      "Sub epoch 4 train acc: 97.01 train loss: 0.0014 val acc: 89.68 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 97.64 train loss: 0.0011 val acc: 88.90 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 98.93 train loss: 0.0005 val acc: 89.37 val loss: 0.0111\n",
      "Train Loss: 0.0005, Train Acc: 98.93  Validation Loss: 0.0111, Validation Acc: 89.37\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 69.21 train loss: 0.0173 val acc: 88.53 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 94.55 train loss: 0.0028 val acc: 89.67 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 97.34 train loss: 0.0014 val acc: 89.57 val loss: 0.0089\n",
      "Sub epoch 3 train acc: 97.96 train loss: 0.0010 val acc: 89.42 val loss: 0.0101\n",
      "Sub epoch 4 train acc: 97.94 train loss: 0.0010 val acc: 89.33 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 98.27 train loss: 0.0008 val acc: 89.13 val loss: 0.0116\n",
      "Sub epoch 6 train acc: 97.71 train loss: 0.0011 val acc: 89.10 val loss: 0.0120\n",
      "Train Loss: 0.0011, Train Acc: 97.71  Validation Loss: 0.0120, Validation Acc: 89.10\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 72.71 train loss: 0.0158 val acc: 88.43 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 96.17 train loss: 0.0020 val acc: 89.70 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 98.56 train loss: 0.0008 val acc: 89.38 val loss: 0.0091\n",
      "Sub epoch 3 train acc: 99.47 train loss: 0.0004 val acc: 89.78 val loss: 0.0098\n",
      "Sub epoch 4 train acc: 99.73 train loss: 0.0003 val acc: 89.80 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 99.80 train loss: 0.0002 val acc: 89.77 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 99.82 train loss: 0.0002 val acc: 89.77 val loss: 0.0107\n",
      "Train Loss: 0.0002, Train Acc: 99.82  Validation Loss: 0.0107, Validation Acc: 89.77\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 71.17 train loss: 0.0165 val acc: 87.85 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 95.01 train loss: 0.0025 val acc: 89.28 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 98.07 train loss: 0.0011 val acc: 89.62 val loss: 0.0091\n",
      "Sub epoch 3 train acc: 98.51 train loss: 0.0008 val acc: 89.88 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 98.68 train loss: 0.0007 val acc: 89.55 val loss: 0.0107\n",
      "Sub epoch 5 train acc: 98.61 train loss: 0.0007 val acc: 88.97 val loss: 0.0119\n",
      "Sub epoch 6 train acc: 98.26 train loss: 0.0008 val acc: 88.73 val loss: 0.0138\n",
      "Train Loss: 0.0008, Train Acc: 98.26  Validation Loss: 0.0138, Validation Acc: 88.73\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 75.21 train loss: 0.0145 val acc: 88.38 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 97.50 train loss: 0.0013 val acc: 89.22 val loss: 0.0102\n",
      "Sub epoch 2 train acc: 99.17 train loss: 0.0005 val acc: 89.22 val loss: 0.0114\n",
      "Sub epoch 3 train acc: 99.33 train loss: 0.0004 val acc: 88.92 val loss: 0.0126\n",
      "Sub epoch 4 train acc: 99.15 train loss: 0.0004 val acc: 89.03 val loss: 0.0134\n",
      "Sub epoch 5 train acc: 98.73 train loss: 0.0006 val acc: 89.05 val loss: 0.0141\n",
      "Sub epoch 6 train acc: 98.75 train loss: 0.0006 val acc: 88.95 val loss: 0.0136\n",
      "Train Loss: 0.0006, Train Acc: 98.75  Validation Loss: 0.0136, Validation Acc: 88.95\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 76.48 train loss: 0.0139 val acc: 88.32 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 97.65 train loss: 0.0012 val acc: 89.72 val loss: 0.0103\n",
      "Sub epoch 2 train acc: 99.30 train loss: 0.0004 val acc: 89.73 val loss: 0.0117\n",
      "Sub epoch 3 train acc: 99.35 train loss: 0.0003 val acc: 88.48 val loss: 0.0134\n",
      "Sub epoch 4 train acc: 99.76 train loss: 0.0002 val acc: 89.43 val loss: 0.0129\n",
      "Sub epoch 5 train acc: 99.96 train loss: 0.0001 val acc: 89.55 val loss: 0.0134\n",
      "Sub epoch 6 train acc: 99.97 train loss: 0.0000 val acc: 89.57 val loss: 0.0139\n",
      "Train Loss: 0.0000, Train Acc: 99.97  Validation Loss: 0.0139, Validation Acc: 89.57\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 75.87 train loss: 0.0146 val acc: 88.22 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 97.11 train loss: 0.0014 val acc: 89.33 val loss: 0.0095\n",
      "Sub epoch 2 train acc: 99.19 train loss: 0.0004 val acc: 89.68 val loss: 0.0111\n",
      "Sub epoch 3 train acc: 99.49 train loss: 0.0003 val acc: 89.92 val loss: 0.0131\n",
      "Sub epoch 4 train acc: 99.23 train loss: 0.0004 val acc: 89.32 val loss: 0.0128\n",
      "Sub epoch 5 train acc: 99.05 train loss: 0.0004 val acc: 88.92 val loss: 0.0143\n",
      "Sub epoch 6 train acc: 98.82 train loss: 0.0006 val acc: 88.78 val loss: 0.0158\n",
      "Train Loss: 0.0006, Train Acc: 98.82  Validation Loss: 0.0158, Validation Acc: 88.78\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 79.56 train loss: 0.0127 val acc: 87.35 val loss: 0.0083\n",
      "Sub epoch 1 train acc: 95.95 train loss: 0.0021 val acc: 89.37 val loss: 0.0090\n",
      "Sub epoch 2 train acc: 98.31 train loss: 0.0009 val acc: 89.47 val loss: 0.0100\n",
      "Sub epoch 3 train acc: 99.10 train loss: 0.0005 val acc: 89.68 val loss: 0.0107\n",
      "Sub epoch 4 train acc: 99.55 train loss: 0.0003 val acc: 90.02 val loss: 0.0113\n",
      "Sub epoch 5 train acc: 99.72 train loss: 0.0002 val acc: 89.75 val loss: 0.0120\n",
      "Sub epoch 6 train acc: 99.83 train loss: 0.0002 val acc: 89.92 val loss: 0.0129\n",
      "Train Loss: 0.0002, Train Acc: 99.83  Validation Loss: 0.0129, Validation Acc: 89.92\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 78.02 train loss: 0.0132 val acc: 87.65 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 97.38 train loss: 0.0013 val acc: 89.47 val loss: 0.0098\n",
      "Sub epoch 2 train acc: 99.14 train loss: 0.0004 val acc: 89.37 val loss: 0.0128\n",
      "Sub epoch 3 train acc: 99.33 train loss: 0.0003 val acc: 89.38 val loss: 0.0140\n",
      "Sub epoch 4 train acc: 99.31 train loss: 0.0003 val acc: 89.93 val loss: 0.0152\n",
      "Sub epoch 5 train acc: 99.73 train loss: 0.0001 val acc: 90.02 val loss: 0.0155\n",
      "Sub epoch 6 train acc: 99.95 train loss: 0.0000 val acc: 89.93 val loss: 0.0162\n",
      "Train Loss: 0.0000, Train Acc: 99.95  Validation Loss: 0.0162, Validation Acc: 89.93\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 78.61 train loss: 0.0130 val acc: 88.20 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 97.94 train loss: 0.0010 val acc: 89.40 val loss: 0.0115\n",
      "Sub epoch 2 train acc: 99.49 train loss: 0.0003 val acc: 89.38 val loss: 0.0136\n",
      "Sub epoch 3 train acc: 99.58 train loss: 0.0002 val acc: 89.25 val loss: 0.0140\n",
      "Sub epoch 4 train acc: 99.34 train loss: 0.0003 val acc: 89.25 val loss: 0.0156\n",
      "Sub epoch 5 train acc: 99.18 train loss: 0.0004 val acc: 89.40 val loss: 0.0166\n",
      "Sub epoch 6 train acc: 99.05 train loss: 0.0005 val acc: 89.00 val loss: 0.0152\n",
      "Train Loss: 0.0005, Train Acc: 99.05  Validation Loss: 0.0152, Validation Acc: 89.00\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 81.63 train loss: 0.0118 val acc: 88.27 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 98.62 train loss: 0.0007 val acc: 89.83 val loss: 0.0131\n",
      "Sub epoch 2 train acc: 99.83 train loss: 0.0001 val acc: 90.03 val loss: 0.0134\n",
      "Sub epoch 3 train acc: 99.92 train loss: 0.0001 val acc: 89.63 val loss: 0.0140\n",
      "Sub epoch 4 train acc: 99.94 train loss: 0.0001 val acc: 89.78 val loss: 0.0145\n",
      "Sub epoch 5 train acc: 99.97 train loss: 0.0000 val acc: 89.72 val loss: 0.0151\n",
      "Sub epoch 6 train acc: 99.98 train loss: 0.0000 val acc: 89.80 val loss: 0.0158\n",
      "Train Loss: 0.0000, Train Acc: 99.98  Validation Loss: 0.0158, Validation Acc: 89.80\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 79.00 train loss: 0.0133 val acc: 87.67 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 97.87 train loss: 0.0010 val acc: 89.48 val loss: 0.0113\n",
      "Sub epoch 2 train acc: 99.43 train loss: 0.0003 val acc: 89.38 val loss: 0.0127\n",
      "Sub epoch 3 train acc: 99.55 train loss: 0.0002 val acc: 89.40 val loss: 0.0152\n",
      "Sub epoch 4 train acc: 99.37 train loss: 0.0003 val acc: 89.32 val loss: 0.0151\n",
      "Sub epoch 5 train acc: 99.19 train loss: 0.0005 val acc: 89.40 val loss: 0.0168\n",
      "Sub epoch 6 train acc: 99.69 train loss: 0.0002 val acc: 89.90 val loss: 0.0166\n",
      "Train Loss: 0.0002, Train Acc: 99.69  Validation Loss: 0.0166, Validation Acc: 89.90\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 81.73 train loss: 0.0114 val acc: 88.03 val loss: 0.0092\n",
      "Sub epoch 1 train acc: 98.65 train loss: 0.0008 val acc: 89.42 val loss: 0.0142\n",
      "Sub epoch 2 train acc: 99.75 train loss: 0.0001 val acc: 89.43 val loss: 0.0174\n",
      "Sub epoch 3 train acc: 99.49 train loss: 0.0002 val acc: 89.32 val loss: 0.0166\n",
      "Sub epoch 4 train acc: 99.54 train loss: 0.0002 val acc: 89.30 val loss: 0.0186\n",
      "Sub epoch 5 train acc: 99.35 train loss: 0.0003 val acc: 89.10 val loss: 0.0192\n",
      "Sub epoch 6 train acc: 99.41 train loss: 0.0003 val acc: 89.18 val loss: 0.0182\n",
      "Train Loss: 0.0003, Train Acc: 99.41  Validation Loss: 0.0182, Validation Acc: 89.18\n"
     ]
    }
   ],
   "source": [
    "train_loss_df = pd.DataFrame()\n",
    "train_acc_df = pd.DataFrame()\n",
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    train_loss, train_acc, validation_loss, validation_acc = \\\n",
    "        active_learn(net, trainset, np.array(train_df[i].to_list()), np.array(val_idx_df[i].to_list()), \n",
    "                      heuristic=smallest_margin_heuristic, initial_train_idx=np.array(batch_df[i].to_list()),\n",
    "                     experiment_id=i)\n",
    "    train_loss_df[i] = train_loss\n",
    "    train_acc_df[i] = train_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9825b5e-b05b-4008-a94a-1ee023fdf388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T09:45:49.941181Z",
     "iopub.status.busy": "2023-01-12T09:45:49.940979Z",
     "iopub.status.idle": "2023-01-12T09:54:09.223716Z",
     "shell.execute_reply": "2023-01-12T09:54:09.222533Z",
     "shell.execute_reply.started": "2023-01-12T09:45:49.941161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00850279885828495, 0.01094275106266141, 0.010985550126433373, 0.010955968340486288, 0.010293313064798713, 0.01146764251217246, 0.0117316101802513, 0.01210487645715475, 0.009209453854709863, 0.011155574430525303, 0.012956572581082583, 0.01312733051404357, 0.012662731570005416, 0.013357287845015525, 0.015396043821424246, 0.015175096303969621, 0.015565117588639259, 0.014985355918109417, 0.017139974722266197, 0.01664181820973754] [85.82, 86.68, 88.04, 88.93, 88.77, 88.45, 89.2, 88.71, 89.44, 88.7, 89.42, 88.56, 89.5, 89.29, 88.57, 89.4, 88.42, 88.56, 89.5, 88.68]\n",
      "[0.00670951359719038, 0.007821355031430721, 0.010095248490571977, 0.0084762027669698, 0.010289756299555301, 0.011323608185350895, 0.011802571365237236, 0.010857636739313602, 0.010739322362840175, 0.01390401451215148, 0.012665992404520512, 0.01206184198986739, 0.011357007205486297, 0.013619412818085402, 0.014408696500211954, 0.013848421279340982, 0.014816562727093696, 0.0164042951811105, 0.016008747147768735, 0.017458106987178326] [86.62, 87.88, 87.69, 89.38, 88.67, 89.46, 88.66, 89.82, 88.62, 88.9, 89.43, 88.97, 89.62, 89.69, 88.73, 89.57, 89.56, 89.22, 89.71, 89.33]\n",
      "[0.009845876994729042, 0.009146329528093338, 0.00985764323323965, 0.00772586430683732, 0.009498520339280368, 0.011464460272341966, 0.009000232250615954, 0.010473506353423, 0.011836966947466134, 0.010418554896861314, 0.012945313297957181, 0.013231750296801329, 0.013186881685256958, 0.015127963224798441, 0.01209231579452753, 0.014851256349508185, 0.014332627669349313, 0.015652440828830005, 0.015398981415480375, 0.017692414918914438] [85.94, 88.31, 87.77, 89.22, 89.47, 88.51, 89.54, 89.33, 88.62, 89.61, 88.69, 88.93, 89.54, 88.88, 89.7, 89.62, 89.11, 89.74, 89.5, 88.5]\n"
     ]
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_acc_df = pd.DataFrame()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(trainset, val_idx_df[i]),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    for k in range(20):\n",
    "        net.load_state_dict(torch.load(f\"{i}/epoch{k}_smallest_margin_heuristic.pt\"))\n",
    "        \n",
    "        v_loss, v_acc = validate(net, val_loader)\n",
    "        t_loss, t_acc = validate(net, test_loader)\n",
    "        validation_loss.append(v_loss)\n",
    "        validation_acc.append(v_acc)\n",
    "        test_loss.append(t_loss)\n",
    "        test_acc.append(t_acc)\n",
    "        # print(t_acc)\n",
    "\n",
    "    test_loss_df[i] = test_loss\n",
    "    test_acc_df[i] = test_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68a534b7-a7ec-4944-8773-55cbaab07170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T09:54:09.228684Z",
     "iopub.status.busy": "2023-01-12T09:54:09.228517Z",
     "iopub.status.idle": "2023-01-12T09:54:10.534071Z",
     "shell.execute_reply": "2023-01-12T09:54:10.533501Z",
     "shell.execute_reply.started": "2023-01-12T09:54:09.228663Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.to_csv(\"val_loss_sm.csv\",index=False)\n",
    "val_acc_df.to_csv(\"val_acc_sm.csv\",index=False)\n",
    "test_loss_df.to_csv(\"test_loss_sm.csv\",index=False)\n",
    "test_acc_df.to_csv(\"test_acc_sm.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709b9f7-d6aa-456e-a41c-e54c69a79d7e",
   "metadata": {},
   "source": [
    "### NajwiÄ™kszy margines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e6b14b8-b71f-46dc-9706-8010e30ee163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T09:54:10.535131Z",
     "iopub.status.busy": "2023-01-12T09:54:10.534952Z",
     "iopub.status.idle": "2023-01-12T12:52:39.506345Z",
     "shell.execute_reply": "2023-01-12T12:52:39.505222Z",
     "shell.execute_reply.started": "2023-01-12T09:54:10.535115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.67 train loss: 0.0142 val acc: 82.57 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 85.03 train loss: 0.0068 val acc: 86.78 val loss: 0.0058\n",
      "Sub epoch 2 train acc: 89.06 train loss: 0.0049 val acc: 84.73 val loss: 0.0068\n",
      "Sub epoch 3 train acc: 93.54 train loss: 0.0028 val acc: 88.23 val loss: 0.0057\n",
      "Sub epoch 4 train acc: 95.50 train loss: 0.0020 val acc: 88.18 val loss: 0.0058\n",
      "Sub epoch 5 train acc: 96.57 train loss: 0.0016 val acc: 87.80 val loss: 0.0061\n",
      "Sub epoch 6 train acc: 97.41 train loss: 0.0012 val acc: 88.32 val loss: 0.0065\n",
      "Train Loss: 0.0012, Train Acc: 97.41  Validation Loss: 0.0065, Validation Acc: 88.32\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 66.57 train loss: 0.0209 val acc: 82.28 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 79.72 train loss: 0.0082 val acc: 88.00 val loss: 0.0056\n",
      "Sub epoch 2 train acc: 85.07 train loss: 0.0059 val acc: 88.22 val loss: 0.0057\n",
      "Sub epoch 3 train acc: 88.60 train loss: 0.0045 val acc: 87.20 val loss: 0.0064\n",
      "Sub epoch 4 train acc: 91.32 train loss: 0.0036 val acc: 88.23 val loss: 0.0071\n",
      "Sub epoch 5 train acc: 92.63 train loss: 0.0031 val acc: 87.30 val loss: 0.0073\n",
      "Sub epoch 6 train acc: 93.56 train loss: 0.0027 val acc: 87.75 val loss: 0.0087\n",
      "Train Loss: 0.0027, Train Acc: 93.56  Validation Loss: 0.0087, Validation Acc: 87.75\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 66.54 train loss: 0.0185 val acc: 83.68 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 84.50 train loss: 0.0063 val acc: 88.73 val loss: 0.0059\n",
      "Sub epoch 2 train acc: 89.73 train loss: 0.0041 val acc: 89.25 val loss: 0.0064\n",
      "Sub epoch 3 train acc: 92.74 train loss: 0.0031 val acc: 88.62 val loss: 0.0078\n",
      "Sub epoch 4 train acc: 93.55 train loss: 0.0026 val acc: 88.70 val loss: 0.0082\n",
      "Sub epoch 5 train acc: 95.19 train loss: 0.0021 val acc: 89.03 val loss: 0.0089\n",
      "Sub epoch 6 train acc: 95.19 train loss: 0.0020 val acc: 89.35 val loss: 0.0094\n",
      "Train Loss: 0.0020, Train Acc: 95.19  Validation Loss: 0.0094, Validation Acc: 89.35\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 63.97 train loss: 0.0194 val acc: 84.95 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 85.44 train loss: 0.0061 val acc: 89.03 val loss: 0.0059\n",
      "Sub epoch 2 train acc: 90.97 train loss: 0.0037 val acc: 89.55 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 93.64 train loss: 0.0027 val acc: 89.47 val loss: 0.0073\n",
      "Sub epoch 4 train acc: 97.57 train loss: 0.0013 val acc: 90.10 val loss: 0.0081\n",
      "Sub epoch 5 train acc: 98.39 train loss: 0.0009 val acc: 90.05 val loss: 0.0087\n",
      "Sub epoch 6 train acc: 98.74 train loss: 0.0007 val acc: 90.10 val loss: 0.0092\n",
      "Train Loss: 0.0007, Train Acc: 98.74  Validation Loss: 0.0092, Validation Acc: 90.10\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 63.09 train loss: 0.0196 val acc: 86.92 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 85.62 train loss: 0.0061 val acc: 89.75 val loss: 0.0056\n",
      "Sub epoch 2 train acc: 91.34 train loss: 0.0037 val acc: 89.92 val loss: 0.0065\n",
      "Sub epoch 3 train acc: 93.40 train loss: 0.0028 val acc: 89.55 val loss: 0.0078\n",
      "Sub epoch 4 train acc: 94.81 train loss: 0.0022 val acc: 89.28 val loss: 0.0088\n",
      "Sub epoch 5 train acc: 95.36 train loss: 0.0021 val acc: 89.57 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 96.17 train loss: 0.0017 val acc: 89.72 val loss: 0.0099\n",
      "Train Loss: 0.0017, Train Acc: 96.17  Validation Loss: 0.0099, Validation Acc: 89.72\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 65.38 train loss: 0.0192 val acc: 87.90 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 88.94 train loss: 0.0051 val acc: 90.10 val loss: 0.0060\n",
      "Sub epoch 2 train acc: 92.65 train loss: 0.0035 val acc: 90.03 val loss: 0.0063\n",
      "Sub epoch 3 train acc: 94.58 train loss: 0.0027 val acc: 90.17 val loss: 0.0066\n",
      "Sub epoch 4 train acc: 95.63 train loss: 0.0023 val acc: 90.27 val loss: 0.0068\n",
      "Sub epoch 5 train acc: 96.54 train loss: 0.0019 val acc: 90.22 val loss: 0.0072\n",
      "Sub epoch 6 train acc: 97.13 train loss: 0.0016 val acc: 90.27 val loss: 0.0076\n",
      "Train Loss: 0.0016, Train Acc: 97.13  Validation Loss: 0.0076, Validation Acc: 90.27\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 65.52 train loss: 0.0173 val acc: 87.67 val loss: 0.0058\n",
      "Sub epoch 1 train acc: 89.50 train loss: 0.0049 val acc: 89.80 val loss: 0.0057\n",
      "Sub epoch 2 train acc: 93.94 train loss: 0.0030 val acc: 89.60 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 95.12 train loss: 0.0023 val acc: 90.33 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 96.13 train loss: 0.0018 val acc: 89.90 val loss: 0.0079\n",
      "Sub epoch 5 train acc: 98.27 train loss: 0.0009 val acc: 90.50 val loss: 0.0085\n",
      "Sub epoch 6 train acc: 98.87 train loss: 0.0006 val acc: 90.55 val loss: 0.0089\n",
      "Train Loss: 0.0006, Train Acc: 98.87  Validation Loss: 0.0089, Validation Acc: 90.55\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 68.17 train loss: 0.0168 val acc: 87.95 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 92.03 train loss: 0.0038 val acc: 89.88 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 95.66 train loss: 0.0021 val acc: 89.47 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 96.67 train loss: 0.0016 val acc: 89.70 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 97.03 train loss: 0.0013 val acc: 89.70 val loss: 0.0083\n",
      "Sub epoch 5 train acc: 96.93 train loss: 0.0014 val acc: 89.95 val loss: 0.0084\n",
      "Sub epoch 6 train acc: 97.06 train loss: 0.0014 val acc: 89.58 val loss: 0.0097\n",
      "Train Loss: 0.0014, Train Acc: 97.06  Validation Loss: 0.0097, Validation Acc: 89.58\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 70.29 train loss: 0.0160 val acc: 88.88 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 95.28 train loss: 0.0024 val acc: 90.03 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 98.72 train loss: 0.0008 val acc: 90.33 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 99.16 train loss: 0.0006 val acc: 90.25 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 99.30 train loss: 0.0005 val acc: 90.32 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 99.40 train loss: 0.0005 val acc: 90.25 val loss: 0.0089\n",
      "Sub epoch 6 train acc: 99.49 train loss: 0.0004 val acc: 90.17 val loss: 0.0093\n",
      "Train Loss: 0.0004, Train Acc: 99.49  Validation Loss: 0.0093, Validation Acc: 90.17\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 69.76 train loss: 0.0164 val acc: 88.92 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 93.99 train loss: 0.0029 val acc: 89.68 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 97.07 train loss: 0.0015 val acc: 89.95 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 97.47 train loss: 0.0012 val acc: 89.83 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 97.96 train loss: 0.0010 val acc: 89.70 val loss: 0.0087\n",
      "Sub epoch 5 train acc: 97.72 train loss: 0.0011 val acc: 89.07 val loss: 0.0107\n",
      "Sub epoch 6 train acc: 99.18 train loss: 0.0004 val acc: 89.80 val loss: 0.0101\n",
      "Train Loss: 0.0004, Train Acc: 99.18  Validation Loss: 0.0101, Validation Acc: 89.80\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 72.33 train loss: 0.0155 val acc: 88.15 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 95.85 train loss: 0.0021 val acc: 89.97 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 98.46 train loss: 0.0009 val acc: 89.75 val loss: 0.0083\n",
      "Sub epoch 3 train acc: 98.68 train loss: 0.0007 val acc: 89.82 val loss: 0.0088\n",
      "Sub epoch 4 train acc: 98.87 train loss: 0.0006 val acc: 89.67 val loss: 0.0105\n",
      "Sub epoch 5 train acc: 98.48 train loss: 0.0007 val acc: 89.57 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 98.37 train loss: 0.0008 val acc: 90.07 val loss: 0.0108\n",
      "Train Loss: 0.0008, Train Acc: 98.37  Validation Loss: 0.0108, Validation Acc: 90.07\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 74.92 train loss: 0.0144 val acc: 89.60 val loss: 0.0059\n",
      "Sub epoch 1 train acc: 96.99 train loss: 0.0015 val acc: 90.15 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 99.01 train loss: 0.0005 val acc: 89.22 val loss: 0.0108\n",
      "Sub epoch 3 train acc: 99.63 train loss: 0.0002 val acc: 90.10 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 99.84 train loss: 0.0001 val acc: 90.05 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 99.88 train loss: 0.0001 val acc: 89.98 val loss: 0.0112\n",
      "Sub epoch 6 train acc: 99.90 train loss: 0.0001 val acc: 90.10 val loss: 0.0116\n",
      "Train Loss: 0.0001, Train Acc: 99.90  Validation Loss: 0.0116, Validation Acc: 90.10\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 72.71 train loss: 0.0154 val acc: 88.60 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 96.31 train loss: 0.0018 val acc: 89.53 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.50 train loss: 0.0008 val acc: 89.85 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 98.83 train loss: 0.0006 val acc: 90.22 val loss: 0.0095\n",
      "Sub epoch 4 train acc: 98.88 train loss: 0.0005 val acc: 89.47 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 98.85 train loss: 0.0006 val acc: 89.80 val loss: 0.0111\n",
      "Sub epoch 6 train acc: 98.77 train loss: 0.0006 val acc: 89.92 val loss: 0.0117\n",
      "Train Loss: 0.0006, Train Acc: 98.77  Validation Loss: 0.0117, Validation Acc: 89.92\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 77.49 train loss: 0.0129 val acc: 89.43 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 98.03 train loss: 0.0011 val acc: 90.25 val loss: 0.0106\n",
      "Sub epoch 2 train acc: 99.31 train loss: 0.0004 val acc: 90.15 val loss: 0.0107\n",
      "Sub epoch 3 train acc: 99.46 train loss: 0.0003 val acc: 89.98 val loss: 0.0128\n",
      "Sub epoch 4 train acc: 99.14 train loss: 0.0004 val acc: 90.10 val loss: 0.0120\n",
      "Sub epoch 5 train acc: 99.03 train loss: 0.0005 val acc: 89.63 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 98.76 train loss: 0.0006 val acc: 89.95 val loss: 0.0116\n",
      "Train Loss: 0.0006, Train Acc: 98.76  Validation Loss: 0.0116, Validation Acc: 89.95\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 78.32 train loss: 0.0131 val acc: 89.55 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 98.25 train loss: 0.0009 val acc: 90.53 val loss: 0.0093\n",
      "Sub epoch 2 train acc: 99.58 train loss: 0.0002 val acc: 90.02 val loss: 0.0124\n",
      "Sub epoch 3 train acc: 99.46 train loss: 0.0003 val acc: 89.78 val loss: 0.0123\n",
      "Sub epoch 4 train acc: 99.79 train loss: 0.0001 val acc: 90.07 val loss: 0.0122\n",
      "Sub epoch 5 train acc: 99.96 train loss: 0.0000 val acc: 90.22 val loss: 0.0126\n",
      "Sub epoch 6 train acc: 99.98 train loss: 0.0000 val acc: 90.25 val loss: 0.0131\n",
      "Train Loss: 0.0000, Train Acc: 99.98  Validation Loss: 0.0131, Validation Acc: 90.25\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 77.23 train loss: 0.0138 val acc: 89.12 val loss: 0.0058\n",
      "Sub epoch 1 train acc: 97.68 train loss: 0.0012 val acc: 89.93 val loss: 0.0101\n",
      "Sub epoch 2 train acc: 99.23 train loss: 0.0004 val acc: 89.57 val loss: 0.0124\n",
      "Sub epoch 3 train acc: 99.41 train loss: 0.0003 val acc: 89.73 val loss: 0.0120\n",
      "Sub epoch 4 train acc: 99.31 train loss: 0.0004 val acc: 89.40 val loss: 0.0113\n",
      "Sub epoch 5 train acc: 99.34 train loss: 0.0003 val acc: 89.72 val loss: 0.0144\n",
      "Sub epoch 6 train acc: 99.12 train loss: 0.0004 val acc: 90.00 val loss: 0.0128\n",
      "Train Loss: 0.0004, Train Acc: 99.12  Validation Loss: 0.0128, Validation Acc: 90.00\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 79.38 train loss: 0.0125 val acc: 88.53 val loss: 0.0061\n",
      "Sub epoch 1 train acc: 96.76 train loss: 0.0017 val acc: 90.72 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 98.42 train loss: 0.0008 val acc: 90.62 val loss: 0.0085\n",
      "Sub epoch 3 train acc: 99.09 train loss: 0.0005 val acc: 90.45 val loss: 0.0094\n",
      "Sub epoch 4 train acc: 99.52 train loss: 0.0003 val acc: 90.47 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 99.70 train loss: 0.0002 val acc: 90.35 val loss: 0.0110\n",
      "Sub epoch 6 train acc: 99.87 train loss: 0.0001 val acc: 90.22 val loss: 0.0118\n",
      "Train Loss: 0.0001, Train Acc: 99.87  Validation Loss: 0.0118, Validation Acc: 90.22\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 79.78 train loss: 0.0120 val acc: 88.90 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 97.79 train loss: 0.0011 val acc: 89.88 val loss: 0.0090\n",
      "Sub epoch 2 train acc: 99.33 train loss: 0.0003 val acc: 89.62 val loss: 0.0119\n",
      "Sub epoch 3 train acc: 99.52 train loss: 0.0003 val acc: 90.00 val loss: 0.0122\n",
      "Sub epoch 4 train acc: 99.35 train loss: 0.0003 val acc: 89.50 val loss: 0.0134\n",
      "Sub epoch 5 train acc: 99.82 train loss: 0.0001 val acc: 89.82 val loss: 0.0131\n",
      "Sub epoch 6 train acc: 99.96 train loss: 0.0000 val acc: 90.03 val loss: 0.0137\n",
      "Train Loss: 0.0000, Train Acc: 99.96  Validation Loss: 0.0137, Validation Acc: 90.03\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 79.72 train loss: 0.0126 val acc: 88.62 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 98.16 train loss: 0.0010 val acc: 89.65 val loss: 0.0106\n",
      "Sub epoch 2 train acc: 99.63 train loss: 0.0002 val acc: 90.23 val loss: 0.0128\n",
      "Sub epoch 3 train acc: 99.43 train loss: 0.0003 val acc: 89.80 val loss: 0.0116\n",
      "Sub epoch 4 train acc: 99.46 train loss: 0.0003 val acc: 90.03 val loss: 0.0136\n",
      "Sub epoch 5 train acc: 99.42 train loss: 0.0003 val acc: 90.17 val loss: 0.0149\n",
      "Sub epoch 6 train acc: 98.96 train loss: 0.0006 val acc: 89.68 val loss: 0.0136\n",
      "Train Loss: 0.0006, Train Acc: 98.96  Validation Loss: 0.0136, Validation Acc: 89.68\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 82.59 train loss: 0.0110 val acc: 89.28 val loss: 0.0064\n",
      "Sub epoch 1 train acc: 98.97 train loss: 0.0006 val acc: 90.13 val loss: 0.0125\n",
      "Sub epoch 2 train acc: 99.86 train loss: 0.0001 val acc: 90.05 val loss: 0.0128\n",
      "Sub epoch 3 train acc: 99.94 train loss: 0.0001 val acc: 90.03 val loss: 0.0133\n",
      "Sub epoch 4 train acc: 99.97 train loss: 0.0001 val acc: 90.02 val loss: 0.0140\n",
      "Sub epoch 5 train acc: 99.97 train loss: 0.0000 val acc: 90.07 val loss: 0.0145\n",
      "Sub epoch 6 train acc: 99.97 train loss: 0.0000 val acc: 90.05 val loss: 0.0152\n",
      "Train Loss: 0.0000, Train Acc: 99.97  Validation Loss: 0.0152, Validation Acc: 90.05\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.30 train loss: 0.0139 val acc: 81.85 val loss: 0.0087\n",
      "Sub epoch 1 train acc: 84.99 train loss: 0.0070 val acc: 85.15 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 88.64 train loss: 0.0049 val acc: 85.03 val loss: 0.0073\n",
      "Sub epoch 3 train acc: 89.88 train loss: 0.0043 val acc: 85.23 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 91.90 train loss: 0.0035 val acc: 85.45 val loss: 0.0079\n",
      "Sub epoch 5 train acc: 92.96 train loss: 0.0032 val acc: 85.37 val loss: 0.0082\n",
      "Sub epoch 6 train acc: 96.97 train loss: 0.0014 val acc: 87.63 val loss: 0.0073\n",
      "Train Loss: 0.0014, Train Acc: 96.97  Validation Loss: 0.0073, Validation Acc: 87.63\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 70.31 train loss: 0.0186 val acc: 79.70 val loss: 0.0088\n",
      "Sub epoch 1 train acc: 83.29 train loss: 0.0068 val acc: 86.95 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 88.41 train loss: 0.0047 val acc: 87.28 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 91.32 train loss: 0.0036 val acc: 87.98 val loss: 0.0070\n",
      "Sub epoch 4 train acc: 93.13 train loss: 0.0028 val acc: 88.40 val loss: 0.0083\n",
      "Sub epoch 5 train acc: 94.50 train loss: 0.0023 val acc: 87.58 val loss: 0.0095\n",
      "Sub epoch 6 train acc: 94.91 train loss: 0.0022 val acc: 87.23 val loss: 0.0104\n",
      "Train Loss: 0.0022, Train Acc: 94.91  Validation Loss: 0.0104, Validation Acc: 87.23\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 67.11 train loss: 0.0198 val acc: 85.05 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 85.30 train loss: 0.0063 val acc: 87.98 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 90.24 train loss: 0.0040 val acc: 88.37 val loss: 0.0070\n",
      "Sub epoch 3 train acc: 95.22 train loss: 0.0022 val acc: 89.17 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 96.61 train loss: 0.0017 val acc: 89.38 val loss: 0.0077\n",
      "Sub epoch 5 train acc: 97.07 train loss: 0.0014 val acc: 89.45 val loss: 0.0083\n",
      "Sub epoch 6 train acc: 97.59 train loss: 0.0013 val acc: 89.20 val loss: 0.0086\n",
      "Train Loss: 0.0013, Train Acc: 97.59  Validation Loss: 0.0086, Validation Acc: 89.20\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 64.12 train loss: 0.0195 val acc: 86.08 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 83.38 train loss: 0.0067 val acc: 88.47 val loss: 0.0058\n",
      "Sub epoch 2 train acc: 88.28 train loss: 0.0047 val acc: 88.05 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 91.71 train loss: 0.0034 val acc: 88.98 val loss: 0.0074\n",
      "Sub epoch 4 train acc: 93.47 train loss: 0.0027 val acc: 88.50 val loss: 0.0084\n",
      "Sub epoch 5 train acc: 94.40 train loss: 0.0024 val acc: 88.02 val loss: 0.0087\n",
      "Sub epoch 6 train acc: 94.18 train loss: 0.0026 val acc: 88.18 val loss: 0.0097\n",
      "Train Loss: 0.0026, Train Acc: 94.18  Validation Loss: 0.0097, Validation Acc: 88.18\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 65.58 train loss: 0.0184 val acc: 86.93 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 89.37 train loss: 0.0047 val acc: 88.92 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 93.43 train loss: 0.0029 val acc: 88.95 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 95.37 train loss: 0.0021 val acc: 89.17 val loss: 0.0079\n",
      "Sub epoch 4 train acc: 95.97 train loss: 0.0018 val acc: 88.85 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 96.37 train loss: 0.0016 val acc: 89.33 val loss: 0.0090\n",
      "Sub epoch 6 train acc: 97.04 train loss: 0.0013 val acc: 89.38 val loss: 0.0105\n",
      "Train Loss: 0.0013, Train Acc: 97.04  Validation Loss: 0.0105, Validation Acc: 89.38\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 64.59 train loss: 0.0192 val acc: 84.52 val loss: 0.0068\n",
      "Sub epoch 1 train acc: 89.98 train loss: 0.0046 val acc: 89.42 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 95.00 train loss: 0.0024 val acc: 89.45 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 96.57 train loss: 0.0016 val acc: 89.32 val loss: 0.0085\n",
      "Sub epoch 4 train acc: 98.48 train loss: 0.0008 val acc: 89.82 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 99.25 train loss: 0.0005 val acc: 89.87 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 99.45 train loss: 0.0004 val acc: 89.72 val loss: 0.0096\n",
      "Train Loss: 0.0004, Train Acc: 99.45  Validation Loss: 0.0096, Validation Acc: 89.72\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 64.17 train loss: 0.0189 val acc: 86.98 val loss: 0.0060\n",
      "Sub epoch 1 train acc: 91.02 train loss: 0.0043 val acc: 89.47 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 95.14 train loss: 0.0023 val acc: 89.83 val loss: 0.0071\n",
      "Sub epoch 3 train acc: 96.77 train loss: 0.0016 val acc: 89.85 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 96.96 train loss: 0.0014 val acc: 89.78 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 97.03 train loss: 0.0014 val acc: 89.60 val loss: 0.0094\n",
      "Sub epoch 6 train acc: 97.39 train loss: 0.0012 val acc: 89.45 val loss: 0.0095\n",
      "Train Loss: 0.0012, Train Acc: 97.39  Validation Loss: 0.0095, Validation Acc: 89.45\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 68.12 train loss: 0.0175 val acc: 87.55 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 91.94 train loss: 0.0040 val acc: 90.10 val loss: 0.0056\n",
      "Sub epoch 2 train acc: 95.37 train loss: 0.0025 val acc: 90.17 val loss: 0.0060\n",
      "Sub epoch 3 train acc: 96.60 train loss: 0.0019 val acc: 90.03 val loss: 0.0063\n",
      "Sub epoch 4 train acc: 97.34 train loss: 0.0016 val acc: 89.98 val loss: 0.0065\n",
      "Sub epoch 5 train acc: 97.94 train loss: 0.0013 val acc: 89.97 val loss: 0.0069\n",
      "Sub epoch 6 train acc: 98.34 train loss: 0.0011 val acc: 90.02 val loss: 0.0072\n",
      "Train Loss: 0.0011, Train Acc: 98.34  Validation Loss: 0.0072, Validation Acc: 90.02\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 68.85 train loss: 0.0163 val acc: 88.12 val loss: 0.0059\n",
      "Sub epoch 1 train acc: 93.21 train loss: 0.0034 val acc: 89.42 val loss: 0.0063\n",
      "Sub epoch 2 train acc: 96.40 train loss: 0.0018 val acc: 89.95 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 97.22 train loss: 0.0013 val acc: 89.50 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 97.60 train loss: 0.0012 val acc: 89.22 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 99.06 train loss: 0.0005 val acc: 90.27 val loss: 0.0090\n",
      "Sub epoch 6 train acc: 99.56 train loss: 0.0003 val acc: 90.13 val loss: 0.0095\n",
      "Train Loss: 0.0003, Train Acc: 99.56  Validation Loss: 0.0095, Validation Acc: 90.13\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 70.28 train loss: 0.0164 val acc: 89.27 val loss: 0.0055\n",
      "Sub epoch 1 train acc: 95.12 train loss: 0.0025 val acc: 89.45 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 97.71 train loss: 0.0012 val acc: 89.93 val loss: 0.0076\n",
      "Sub epoch 3 train acc: 98.14 train loss: 0.0009 val acc: 89.50 val loss: 0.0087\n",
      "Sub epoch 4 train acc: 98.34 train loss: 0.0008 val acc: 89.27 val loss: 0.0091\n",
      "Sub epoch 5 train acc: 98.35 train loss: 0.0008 val acc: 88.95 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 98.11 train loss: 0.0009 val acc: 89.20 val loss: 0.0115\n",
      "Train Loss: 0.0009, Train Acc: 98.11  Validation Loss: 0.0115, Validation Acc: 89.20\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 72.81 train loss: 0.0152 val acc: 88.17 val loss: 0.0057\n",
      "Sub epoch 1 train acc: 96.04 train loss: 0.0019 val acc: 89.40 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 99.12 train loss: 0.0006 val acc: 89.87 val loss: 0.0083\n",
      "Sub epoch 3 train acc: 99.45 train loss: 0.0005 val acc: 89.85 val loss: 0.0085\n",
      "Sub epoch 4 train acc: 99.58 train loss: 0.0004 val acc: 89.85 val loss: 0.0088\n",
      "Sub epoch 5 train acc: 99.65 train loss: 0.0004 val acc: 89.88 val loss: 0.0091\n",
      "Sub epoch 6 train acc: 99.70 train loss: 0.0003 val acc: 89.75 val loss: 0.0095\n",
      "Train Loss: 0.0003, Train Acc: 99.70  Validation Loss: 0.0095, Validation Acc: 89.75\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 72.21 train loss: 0.0161 val acc: 88.68 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 95.59 train loss: 0.0023 val acc: 89.48 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 98.29 train loss: 0.0009 val acc: 89.63 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.81 train loss: 0.0007 val acc: 89.25 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 99.11 train loss: 0.0005 val acc: 89.55 val loss: 0.0112\n",
      "Sub epoch 5 train acc: 98.25 train loss: 0.0009 val acc: 89.25 val loss: 0.0097\n",
      "Sub epoch 6 train acc: 99.47 train loss: 0.0003 val acc: 89.90 val loss: 0.0099\n",
      "Train Loss: 0.0003, Train Acc: 99.47  Validation Loss: 0.0099, Validation Acc: 89.90\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 75.12 train loss: 0.0139 val acc: 88.92 val loss: 0.0056\n",
      "Sub epoch 1 train acc: 97.26 train loss: 0.0014 val acc: 89.80 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 98.92 train loss: 0.0006 val acc: 89.98 val loss: 0.0098\n",
      "Sub epoch 3 train acc: 99.16 train loss: 0.0004 val acc: 89.78 val loss: 0.0105\n",
      "Sub epoch 4 train acc: 99.22 train loss: 0.0004 val acc: 89.67 val loss: 0.0101\n",
      "Sub epoch 5 train acc: 98.87 train loss: 0.0006 val acc: 89.73 val loss: 0.0107\n",
      "Sub epoch 6 train acc: 98.72 train loss: 0.0007 val acc: 89.57 val loss: 0.0110\n",
      "Train Loss: 0.0007, Train Acc: 98.72  Validation Loss: 0.0110, Validation Acc: 89.57\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 78.08 train loss: 0.0127 val acc: 89.02 val loss: 0.0058\n",
      "Sub epoch 1 train acc: 98.04 train loss: 0.0010 val acc: 90.17 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 99.37 train loss: 0.0004 val acc: 90.22 val loss: 0.0105\n",
      "Sub epoch 3 train acc: 99.86 train loss: 0.0001 val acc: 90.33 val loss: 0.0108\n",
      "Sub epoch 4 train acc: 99.94 train loss: 0.0001 val acc: 90.38 val loss: 0.0111\n",
      "Sub epoch 5 train acc: 99.95 train loss: 0.0001 val acc: 90.33 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 99.96 train loss: 0.0000 val acc: 90.33 val loss: 0.0119\n",
      "Train Loss: 0.0000, Train Acc: 99.96  Validation Loss: 0.0119, Validation Acc: 90.33\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 76.42 train loss: 0.0138 val acc: 88.92 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 97.44 train loss: 0.0013 val acc: 89.57 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 99.10 train loss: 0.0004 val acc: 89.72 val loss: 0.0094\n",
      "Sub epoch 3 train acc: 99.34 train loss: 0.0004 val acc: 90.07 val loss: 0.0112\n",
      "Sub epoch 4 train acc: 99.12 train loss: 0.0004 val acc: 90.20 val loss: 0.0120\n",
      "Sub epoch 5 train acc: 99.23 train loss: 0.0004 val acc: 90.07 val loss: 0.0132\n",
      "Sub epoch 6 train acc: 98.91 train loss: 0.0006 val acc: 89.57 val loss: 0.0131\n",
      "Train Loss: 0.0006, Train Acc: 98.91  Validation Loss: 0.0131, Validation Acc: 89.57\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 78.92 train loss: 0.0124 val acc: 88.83 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 98.50 train loss: 0.0009 val acc: 90.00 val loss: 0.0102\n",
      "Sub epoch 2 train acc: 99.61 train loss: 0.0002 val acc: 90.37 val loss: 0.0110\n",
      "Sub epoch 3 train acc: 99.66 train loss: 0.0002 val acc: 90.78 val loss: 0.0129\n",
      "Sub epoch 4 train acc: 99.30 train loss: 0.0004 val acc: 89.88 val loss: 0.0122\n",
      "Sub epoch 5 train acc: 99.07 train loss: 0.0005 val acc: 89.62 val loss: 0.0124\n",
      "Sub epoch 6 train acc: 99.19 train loss: 0.0004 val acc: 89.93 val loss: 0.0133\n",
      "Train Loss: 0.0004, Train Acc: 99.19  Validation Loss: 0.0133, Validation Acc: 89.93\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 80.76 train loss: 0.0118 val acc: 87.33 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 98.67 train loss: 0.0007 val acc: 90.02 val loss: 0.0107\n",
      "Sub epoch 2 train acc: 99.72 train loss: 0.0002 val acc: 90.35 val loss: 0.0127\n",
      "Sub epoch 3 train acc: 99.62 train loss: 0.0002 val acc: 90.05 val loss: 0.0130\n",
      "Sub epoch 4 train acc: 99.89 train loss: 0.0001 val acc: 90.28 val loss: 0.0133\n",
      "Sub epoch 5 train acc: 99.97 train loss: 0.0000 val acc: 90.23 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 99.98 train loss: 0.0000 val acc: 90.30 val loss: 0.0141\n",
      "Train Loss: 0.0000, Train Acc: 99.98  Validation Loss: 0.0141, Validation Acc: 90.30\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 77.98 train loss: 0.0131 val acc: 87.85 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 97.85 train loss: 0.0011 val acc: 89.92 val loss: 0.0107\n",
      "Sub epoch 2 train acc: 99.53 train loss: 0.0003 val acc: 90.00 val loss: 0.0108\n",
      "Sub epoch 3 train acc: 99.42 train loss: 0.0003 val acc: 89.42 val loss: 0.0121\n",
      "Sub epoch 4 train acc: 99.43 train loss: 0.0003 val acc: 89.55 val loss: 0.0137\n",
      "Sub epoch 5 train acc: 99.51 train loss: 0.0003 val acc: 89.63 val loss: 0.0171\n",
      "Sub epoch 6 train acc: 99.22 train loss: 0.0005 val acc: 89.18 val loss: 0.0164\n",
      "Train Loss: 0.0005, Train Acc: 99.22  Validation Loss: 0.0164, Validation Acc: 89.18\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 81.26 train loss: 0.0117 val acc: 88.48 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 97.17 train loss: 0.0015 val acc: 89.97 val loss: 0.0093\n",
      "Sub epoch 2 train acc: 99.00 train loss: 0.0006 val acc: 90.10 val loss: 0.0106\n",
      "Sub epoch 3 train acc: 99.52 train loss: 0.0003 val acc: 90.23 val loss: 0.0115\n",
      "Sub epoch 4 train acc: 99.75 train loss: 0.0002 val acc: 90.27 val loss: 0.0126\n",
      "Sub epoch 5 train acc: 99.87 train loss: 0.0001 val acc: 90.17 val loss: 0.0133\n",
      "Sub epoch 6 train acc: 99.93 train loss: 0.0001 val acc: 90.18 val loss: 0.0141\n",
      "Train Loss: 0.0001, Train Acc: 99.93  Validation Loss: 0.0141, Validation Acc: 90.18\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 79.98 train loss: 0.0123 val acc: 87.80 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 98.16 train loss: 0.0009 val acc: 90.18 val loss: 0.0115\n",
      "Sub epoch 2 train acc: 99.52 train loss: 0.0003 val acc: 89.82 val loss: 0.0129\n",
      "Sub epoch 3 train acc: 99.64 train loss: 0.0002 val acc: 89.75 val loss: 0.0142\n",
      "Sub epoch 4 train acc: 99.40 train loss: 0.0003 val acc: 89.88 val loss: 0.0144\n",
      "Sub epoch 5 train acc: 99.85 train loss: 0.0001 val acc: 90.33 val loss: 0.0147\n",
      "Sub epoch 6 train acc: 99.97 train loss: 0.0000 val acc: 90.32 val loss: 0.0154\n",
      "Train Loss: 0.0000, Train Acc: 99.97  Validation Loss: 0.0154, Validation Acc: 90.32\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 71.71 train loss: 0.0143 val acc: 81.22 val loss: 0.0090\n",
      "Sub epoch 1 train acc: 84.29 train loss: 0.0070 val acc: 84.07 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 88.38 train loss: 0.0050 val acc: 84.28 val loss: 0.0075\n",
      "Sub epoch 3 train acc: 90.50 train loss: 0.0041 val acc: 83.98 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 92.17 train loss: 0.0035 val acc: 84.97 val loss: 0.0085\n",
      "Sub epoch 5 train acc: 93.68 train loss: 0.0027 val acc: 85.52 val loss: 0.0103\n",
      "Sub epoch 6 train acc: 94.89 train loss: 0.0023 val acc: 84.70 val loss: 0.0108\n",
      "Train Loss: 0.0023, Train Acc: 94.89  Validation Loss: 0.0108, Validation Acc: 84.70\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 71.82 train loss: 0.0186 val acc: 81.38 val loss: 0.0089\n",
      "Sub epoch 1 train acc: 84.93 train loss: 0.0063 val acc: 85.05 val loss: 0.0077\n",
      "Sub epoch 2 train acc: 91.74 train loss: 0.0035 val acc: 87.42 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 93.43 train loss: 0.0028 val acc: 87.70 val loss: 0.0072\n",
      "Sub epoch 4 train acc: 94.25 train loss: 0.0025 val acc: 87.77 val loss: 0.0074\n",
      "Sub epoch 5 train acc: 94.97 train loss: 0.0022 val acc: 87.68 val loss: 0.0079\n",
      "Sub epoch 6 train acc: 95.81 train loss: 0.0019 val acc: 87.65 val loss: 0.0082\n",
      "Train Loss: 0.0019, Train Acc: 95.81  Validation Loss: 0.0082, Validation Acc: 87.65\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 66.04 train loss: 0.0184 val acc: 84.57 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 81.67 train loss: 0.0072 val acc: 88.08 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 86.88 train loss: 0.0051 val acc: 86.82 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 90.13 train loss: 0.0039 val acc: 87.93 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 92.55 train loss: 0.0030 val acc: 86.93 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 93.53 train loss: 0.0027 val acc: 87.33 val loss: 0.0100\n",
      "Sub epoch 6 train acc: 97.27 train loss: 0.0013 val acc: 88.75 val loss: 0.0099\n",
      "Train Loss: 0.0013, Train Acc: 97.27  Validation Loss: 0.0099, Validation Acc: 88.75\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 63.14 train loss: 0.0206 val acc: 84.93 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 83.92 train loss: 0.0066 val acc: 87.70 val loss: 0.0066\n",
      "Sub epoch 2 train acc: 89.27 train loss: 0.0044 val acc: 88.30 val loss: 0.0074\n",
      "Sub epoch 3 train acc: 92.34 train loss: 0.0032 val acc: 88.05 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 93.67 train loss: 0.0026 val acc: 88.47 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 94.84 train loss: 0.0022 val acc: 88.60 val loss: 0.0103\n",
      "Sub epoch 6 train acc: 95.72 train loss: 0.0019 val acc: 88.73 val loss: 0.0106\n",
      "Train Loss: 0.0019, Train Acc: 95.72  Validation Loss: 0.0106, Validation Acc: 88.73\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 64.72 train loss: 0.0198 val acc: 85.28 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 87.99 train loss: 0.0052 val acc: 88.68 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 92.85 train loss: 0.0031 val acc: 88.97 val loss: 0.0083\n",
      "Sub epoch 3 train acc: 96.77 train loss: 0.0016 val acc: 89.57 val loss: 0.0082\n",
      "Sub epoch 4 train acc: 97.84 train loss: 0.0012 val acc: 89.52 val loss: 0.0088\n",
      "Sub epoch 5 train acc: 98.18 train loss: 0.0010 val acc: 89.52 val loss: 0.0092\n",
      "Sub epoch 6 train acc: 98.55 train loss: 0.0008 val acc: 89.48 val loss: 0.0098\n",
      "Train Loss: 0.0008, Train Acc: 98.55  Validation Loss: 0.0098, Validation Acc: 89.48\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 62.99 train loss: 0.0194 val acc: 85.93 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 87.97 train loss: 0.0054 val acc: 88.53 val loss: 0.0064\n",
      "Sub epoch 2 train acc: 92.53 train loss: 0.0034 val acc: 88.93 val loss: 0.0068\n",
      "Sub epoch 3 train acc: 94.44 train loss: 0.0025 val acc: 88.90 val loss: 0.0080\n",
      "Sub epoch 4 train acc: 95.57 train loss: 0.0020 val acc: 89.23 val loss: 0.0089\n",
      "Sub epoch 5 train acc: 95.82 train loss: 0.0018 val acc: 88.83 val loss: 0.0105\n",
      "Sub epoch 6 train acc: 96.32 train loss: 0.0016 val acc: 88.87 val loss: 0.0117\n",
      "Train Loss: 0.0016, Train Acc: 96.32  Validation Loss: 0.0117, Validation Acc: 88.87\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 65.96 train loss: 0.0181 val acc: 87.13 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 92.15 train loss: 0.0039 val acc: 88.98 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 96.00 train loss: 0.0020 val acc: 89.23 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 96.85 train loss: 0.0015 val acc: 88.78 val loss: 0.0100\n",
      "Sub epoch 4 train acc: 97.26 train loss: 0.0013 val acc: 89.07 val loss: 0.0107\n",
      "Sub epoch 5 train acc: 97.56 train loss: 0.0011 val acc: 88.55 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 97.23 train loss: 0.0013 val acc: 88.53 val loss: 0.0122\n",
      "Train Loss: 0.0013, Train Acc: 97.23  Validation Loss: 0.0122, Validation Acc: 88.53\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 67.18 train loss: 0.0178 val acc: 88.45 val loss: 0.0061\n",
      "Sub epoch 1 train acc: 94.46 train loss: 0.0028 val acc: 89.65 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 97.16 train loss: 0.0015 val acc: 89.58 val loss: 0.0086\n",
      "Sub epoch 3 train acc: 97.69 train loss: 0.0011 val acc: 89.32 val loss: 0.0110\n",
      "Sub epoch 4 train acc: 99.11 train loss: 0.0005 val acc: 89.87 val loss: 0.0113\n",
      "Sub epoch 5 train acc: 99.49 train loss: 0.0003 val acc: 89.83 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 99.61 train loss: 0.0002 val acc: 89.75 val loss: 0.0122\n",
      "Train Loss: 0.0002, Train Acc: 99.61  Validation Loss: 0.0122, Validation Acc: 89.75\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 67.18 train loss: 0.0176 val acc: 87.53 val loss: 0.0063\n",
      "Sub epoch 1 train acc: 93.76 train loss: 0.0030 val acc: 89.15 val loss: 0.0080\n",
      "Sub epoch 2 train acc: 97.21 train loss: 0.0014 val acc: 88.60 val loss: 0.0091\n",
      "Sub epoch 3 train acc: 97.84 train loss: 0.0010 val acc: 89.38 val loss: 0.0101\n",
      "Sub epoch 4 train acc: 98.19 train loss: 0.0009 val acc: 89.22 val loss: 0.0102\n",
      "Sub epoch 5 train acc: 98.04 train loss: 0.0009 val acc: 88.75 val loss: 0.0122\n",
      "Sub epoch 6 train acc: 97.72 train loss: 0.0010 val acc: 89.05 val loss: 0.0115\n",
      "Train Loss: 0.0010, Train Acc: 97.72  Validation Loss: 0.0115, Validation Acc: 89.05\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 72.81 train loss: 0.0151 val acc: 87.57 val loss: 0.0062\n",
      "Sub epoch 1 train acc: 94.34 train loss: 0.0028 val acc: 88.75 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 97.24 train loss: 0.0015 val acc: 89.15 val loss: 0.0081\n",
      "Sub epoch 3 train acc: 98.29 train loss: 0.0010 val acc: 89.12 val loss: 0.0087\n",
      "Sub epoch 4 train acc: 98.91 train loss: 0.0008 val acc: 89.17 val loss: 0.0093\n",
      "Sub epoch 5 train acc: 99.20 train loss: 0.0006 val acc: 89.18 val loss: 0.0098\n",
      "Sub epoch 6 train acc: 99.44 train loss: 0.0005 val acc: 89.27 val loss: 0.0103\n",
      "Train Loss: 0.0005, Train Acc: 99.44  Validation Loss: 0.0103, Validation Acc: 89.27\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 72.36 train loss: 0.0155 val acc: 87.45 val loss: 0.0066\n",
      "Sub epoch 1 train acc: 95.37 train loss: 0.0024 val acc: 89.38 val loss: 0.0086\n",
      "Sub epoch 2 train acc: 97.79 train loss: 0.0012 val acc: 88.80 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 98.13 train loss: 0.0009 val acc: 88.87 val loss: 0.0109\n",
      "Sub epoch 4 train acc: 98.29 train loss: 0.0008 val acc: 88.70 val loss: 0.0121\n",
      "Sub epoch 5 train acc: 99.30 train loss: 0.0004 val acc: 89.22 val loss: 0.0120\n",
      "Sub epoch 6 train acc: 99.75 train loss: 0.0002 val acc: 89.35 val loss: 0.0126\n",
      "Train Loss: 0.0002, Train Acc: 99.75  Validation Loss: 0.0126, Validation Acc: 89.35\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 74.38 train loss: 0.0146 val acc: 87.87 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 96.60 train loss: 0.0017 val acc: 89.13 val loss: 0.0097\n",
      "Sub epoch 2 train acc: 98.68 train loss: 0.0007 val acc: 88.97 val loss: 0.0114\n",
      "Sub epoch 3 train acc: 98.80 train loss: 0.0006 val acc: 88.98 val loss: 0.0129\n",
      "Sub epoch 4 train acc: 98.79 train loss: 0.0006 val acc: 89.32 val loss: 0.0134\n",
      "Sub epoch 5 train acc: 98.98 train loss: 0.0005 val acc: 89.43 val loss: 0.0149\n",
      "Sub epoch 6 train acc: 98.56 train loss: 0.0007 val acc: 88.98 val loss: 0.0145\n",
      "Train Loss: 0.0007, Train Acc: 98.56  Validation Loss: 0.0145, Validation Acc: 88.98\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 77.30 train loss: 0.0135 val acc: 88.28 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 97.21 train loss: 0.0014 val acc: 88.88 val loss: 0.0116\n",
      "Sub epoch 2 train acc: 99.44 train loss: 0.0003 val acc: 89.28 val loss: 0.0120\n",
      "Sub epoch 3 train acc: 99.68 train loss: 0.0002 val acc: 89.35 val loss: 0.0127\n",
      "Sub epoch 4 train acc: 99.77 train loss: 0.0002 val acc: 89.40 val loss: 0.0131\n",
      "Sub epoch 5 train acc: 99.83 train loss: 0.0002 val acc: 89.50 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 99.87 train loss: 0.0001 val acc: 89.43 val loss: 0.0141\n",
      "Train Loss: 0.0001, Train Acc: 99.87  Validation Loss: 0.0141, Validation Acc: 89.43\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 75.99 train loss: 0.0140 val acc: 87.97 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 96.81 train loss: 0.0016 val acc: 89.68 val loss: 0.0098\n",
      "Sub epoch 2 train acc: 98.88 train loss: 0.0006 val acc: 89.28 val loss: 0.0121\n",
      "Sub epoch 3 train acc: 99.16 train loss: 0.0004 val acc: 88.85 val loss: 0.0134\n",
      "Sub epoch 4 train acc: 99.04 train loss: 0.0005 val acc: 88.95 val loss: 0.0131\n",
      "Sub epoch 5 train acc: 99.11 train loss: 0.0005 val acc: 89.60 val loss: 0.0134\n",
      "Sub epoch 6 train acc: 99.67 train loss: 0.0002 val acc: 89.95 val loss: 0.0141\n",
      "Train Loss: 0.0002, Train Acc: 99.67  Validation Loss: 0.0141, Validation Acc: 89.95\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 78.48 train loss: 0.0131 val acc: 87.38 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 97.76 train loss: 0.0011 val acc: 89.62 val loss: 0.0102\n",
      "Sub epoch 2 train acc: 99.42 train loss: 0.0003 val acc: 89.85 val loss: 0.0135\n",
      "Sub epoch 3 train acc: 99.42 train loss: 0.0003 val acc: 89.62 val loss: 0.0140\n",
      "Sub epoch 4 train acc: 99.35 train loss: 0.0003 val acc: 89.35 val loss: 0.0150\n",
      "Sub epoch 5 train acc: 99.06 train loss: 0.0005 val acc: 89.30 val loss: 0.0148\n",
      "Sub epoch 6 train acc: 98.97 train loss: 0.0005 val acc: 88.98 val loss: 0.0149\n",
      "Train Loss: 0.0005, Train Acc: 98.97  Validation Loss: 0.0149, Validation Acc: 88.98\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 80.05 train loss: 0.0123 val acc: 88.25 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 98.22 train loss: 0.0010 val acc: 89.52 val loss: 0.0105\n",
      "Sub epoch 2 train acc: 99.48 train loss: 0.0003 val acc: 89.35 val loss: 0.0137\n",
      "Sub epoch 3 train acc: 99.80 train loss: 0.0001 val acc: 89.60 val loss: 0.0139\n",
      "Sub epoch 4 train acc: 99.93 train loss: 0.0001 val acc: 89.78 val loss: 0.0144\n",
      "Sub epoch 5 train acc: 99.94 train loss: 0.0001 val acc: 89.83 val loss: 0.0149\n",
      "Sub epoch 6 train acc: 99.96 train loss: 0.0000 val acc: 89.88 val loss: 0.0155\n",
      "Train Loss: 0.0000, Train Acc: 99.96  Validation Loss: 0.0155, Validation Acc: 89.88\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 77.30 train loss: 0.0134 val acc: 86.83 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 97.12 train loss: 0.0014 val acc: 89.35 val loss: 0.0122\n",
      "Sub epoch 2 train acc: 99.17 train loss: 0.0004 val acc: 89.42 val loss: 0.0130\n",
      "Sub epoch 3 train acc: 99.42 train loss: 0.0003 val acc: 89.70 val loss: 0.0150\n",
      "Sub epoch 4 train acc: 99.43 train loss: 0.0004 val acc: 88.97 val loss: 0.0175\n",
      "Sub epoch 5 train acc: 99.23 train loss: 0.0004 val acc: 88.85 val loss: 0.0174\n",
      "Sub epoch 6 train acc: 99.07 train loss: 0.0005 val acc: 88.78 val loss: 0.0175\n",
      "Train Loss: 0.0005, Train Acc: 99.07  Validation Loss: 0.0175, Validation Acc: 88.78\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 80.98 train loss: 0.0115 val acc: 87.05 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 98.58 train loss: 0.0008 val acc: 89.38 val loss: 0.0133\n",
      "Sub epoch 2 train acc: 99.68 train loss: 0.0002 val acc: 89.30 val loss: 0.0165\n",
      "Sub epoch 3 train acc: 99.67 train loss: 0.0002 val acc: 89.32 val loss: 0.0170\n",
      "Sub epoch 4 train acc: 99.41 train loss: 0.0003 val acc: 88.88 val loss: 0.0149\n",
      "Sub epoch 5 train acc: 99.50 train loss: 0.0003 val acc: 89.37 val loss: 0.0178\n",
      "Sub epoch 6 train acc: 99.33 train loss: 0.0004 val acc: 88.92 val loss: 0.0184\n",
      "Train Loss: 0.0004, Train Acc: 99.33  Validation Loss: 0.0184, Validation Acc: 88.92\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 81.33 train loss: 0.0121 val acc: 88.05 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 98.48 train loss: 0.0009 val acc: 89.25 val loss: 0.0122\n",
      "Sub epoch 2 train acc: 99.67 train loss: 0.0002 val acc: 89.37 val loss: 0.0149\n",
      "Sub epoch 3 train acc: 99.70 train loss: 0.0002 val acc: 89.47 val loss: 0.0171\n",
      "Sub epoch 4 train acc: 99.88 train loss: 0.0001 val acc: 89.60 val loss: 0.0171\n",
      "Sub epoch 5 train acc: 99.97 train loss: 0.0000 val acc: 89.57 val loss: 0.0174\n",
      "Sub epoch 6 train acc: 99.96 train loss: 0.0000 val acc: 89.68 val loss: 0.0179\n",
      "Train Loss: 0.0000, Train Acc: 99.96  Validation Loss: 0.0179, Validation Acc: 89.68\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 80.15 train loss: 0.0124 val acc: 87.07 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 97.99 train loss: 0.0011 val acc: 89.78 val loss: 0.0129\n",
      "Sub epoch 2 train acc: 99.49 train loss: 0.0003 val acc: 88.85 val loss: 0.0167\n",
      "Sub epoch 3 train acc: 99.60 train loss: 0.0002 val acc: 88.82 val loss: 0.0174\n",
      "Sub epoch 4 train acc: 99.50 train loss: 0.0003 val acc: 89.03 val loss: 0.0179\n",
      "Sub epoch 5 train acc: 99.23 train loss: 0.0004 val acc: 89.15 val loss: 0.0166\n",
      "Sub epoch 6 train acc: 99.30 train loss: 0.0004 val acc: 88.67 val loss: 0.0198\n",
      "Train Loss: 0.0004, Train Acc: 99.30  Validation Loss: 0.0198, Validation Acc: 88.67\n"
     ]
    }
   ],
   "source": [
    "train_loss_df = pd.DataFrame()\n",
    "train_acc_df = pd.DataFrame()\n",
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    train_loss, train_acc, validation_loss, validation_acc = \\\n",
    "        active_learn(net, trainset, np.array(train_df[i].to_list()), np.array(val_idx_df[i].to_list()), \n",
    "                      heuristic=largest_margin_heuristic, initial_train_idx=np.array(batch_df[i].to_list()),\n",
    "                     experiment_id=i)\n",
    "    train_loss_df[i] = train_loss\n",
    "    train_acc_df[i] = train_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a86c468-edce-4d54-ba2c-1b323bb529b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T12:52:39.507646Z",
     "iopub.status.busy": "2023-01-12T12:52:39.507439Z",
     "iopub.status.idle": "2023-01-12T12:59:42.411323Z",
     "shell.execute_reply": "2023-01-12T12:59:42.410645Z",
     "shell.execute_reply.started": "2023-01-12T12:52:39.507626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007315501609444618, 0.009128691370785237, 0.010758210846409202, 0.010151913174614311, 0.011240235884860159, 0.008678121334686876, 0.009817339851334692, 0.01062840114980936, 0.010194336104393005, 0.011200559078156948, 0.01257193700298667, 0.013291245955228806, 0.01333250345736742, 0.013487906490638852, 0.015669973218347878, 0.015749315512925386, 0.012892290412448347, 0.015163356708362698, 0.015294528330117465, 0.01626269641183317] [87.23, 87.28, 88.1, 89.19, 88.72, 89.36, 89.37, 88.61, 89.45, 89.58, 88.98, 89.28, 89.12, 89.05, 89.28, 89.21, 89.72, 89.45, 88.83, 89.48]\n",
      "[0.008011613088846207, 0.011610482408106327, 0.009774842622876168, 0.010948193832486868, 0.012980619324743748, 0.011376658616960049, 0.011853212324902416, 0.00837470477372408, 0.01105210838392377, 0.013304176506400109, 0.010909470396488905, 0.011708913784474135, 0.013817470186203717, 0.014680499859899282, 0.01591632227152586, 0.01582236551195383, 0.01710845746099949, 0.018078218510653825, 0.016278653800487517, 0.01852291157990694] [86.93, 86.49, 88.47, 87.82, 88.73, 89.25, 88.65, 89.27, 89.51, 88.82, 89.49, 89.54, 88.89, 89.36, 88.41, 88.94, 89.54, 88.41, 89.63, 89.39]\n",
      "[0.009933461871743203, 0.007692136636376381, 0.00934287291020155, 0.010242804322391748, 0.00965152098312974, 0.010809247649461031, 0.01130845110937953, 0.011499473570659757, 0.010694089950248599, 0.009318808583170175, 0.011903667747229337, 0.014236379270255565, 0.012931800126284362, 0.013345728808641434, 0.013991184362024069, 0.014991021165065467, 0.016226730395294726, 0.016704657676443458, 0.01711426244787872, 0.018562545420974495] [85.27, 88.05, 88.72, 88.94, 89.51, 89.29, 88.93, 89.5, 89.38, 89.43, 89.6, 89.06, 89.57, 89.6, 88.58, 89.73, 89.17, 89.39, 89.57, 89.21]\n"
     ]
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_acc_df = pd.DataFrame()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(trainset, val_idx_df[i]),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    for k in range(20):\n",
    "        net.load_state_dict(torch.load(f\"{i}/epoch{k}_largest_margin_heuristic.pt\"))\n",
    "        \n",
    "        v_loss, v_acc = validate(net, val_loader)\n",
    "        t_loss, t_acc = validate(net, test_loader)\n",
    "        validation_loss.append(v_loss)\n",
    "        validation_acc.append(v_acc)\n",
    "        test_loss.append(t_loss)\n",
    "        test_acc.append(t_acc)\n",
    "        # print(t_acc)\n",
    "\n",
    "    test_loss_df[i] = test_loss\n",
    "    test_acc_df[i] = test_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fb80a01-7aed-4739-8f31-3d46a94062ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T12:59:42.412618Z",
     "iopub.status.busy": "2023-01-12T12:59:42.412418Z",
     "iopub.status.idle": "2023-01-12T12:59:43.203547Z",
     "shell.execute_reply": "2023-01-12T12:59:43.202982Z",
     "shell.execute_reply.started": "2023-01-12T12:59:42.412618Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.to_csv(\"val_loss_lm.csv\",index=False)\n",
    "val_acc_df.to_csv(\"val_acc_lm.csv\",index=False)\n",
    "test_loss_df.to_csv(\"test_loss_lm.csv\",index=False)\n",
    "test_acc_df.to_csv(\"test_acc_lm.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bb7b4-7545-47cc-8e70-81f9d18ed29d",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d924608-6e15-4759-9cd3-19572c766931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T12:59:43.204576Z",
     "iopub.status.busy": "2023-01-12T12:59:43.204405Z",
     "iopub.status.idle": "2023-01-12T16:28:42.091447Z",
     "shell.execute_reply": "2023-01-12T16:28:42.090572Z",
     "shell.execute_reply.started": "2023-01-12T12:59:43.204559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.93 train loss: 0.0138 val acc: 77.88 val loss: 0.0102\n",
      "Sub epoch 1 train acc: 86.56 train loss: 0.0060 val acc: 86.78 val loss: 0.0059\n",
      "Sub epoch 2 train acc: 89.49 train loss: 0.0045 val acc: 87.53 val loss: 0.0056\n",
      "Sub epoch 3 train acc: 91.40 train loss: 0.0038 val acc: 88.15 val loss: 0.0054\n",
      "Sub epoch 4 train acc: 92.53 train loss: 0.0031 val acc: 88.03 val loss: 0.0054\n",
      "Sub epoch 5 train acc: 94.34 train loss: 0.0025 val acc: 88.12 val loss: 0.0055\n",
      "Sub epoch 6 train acc: 95.63 train loss: 0.0020 val acc: 88.05 val loss: 0.0060\n",
      "Train Loss: 0.0020, Train Acc: 95.63  Validation Loss: 0.0060, Validation Acc: 88.05\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 71.68 train loss: 0.0210 val acc: 80.95 val loss: 0.0104\n",
      "Sub epoch 1 train acc: 85.14 train loss: 0.0070 val acc: 86.53 val loss: 0.0061\n",
      "Sub epoch 2 train acc: 89.72 train loss: 0.0044 val acc: 86.97 val loss: 0.0061\n",
      "Sub epoch 3 train acc: 91.97 train loss: 0.0035 val acc: 86.78 val loss: 0.0065\n",
      "Sub epoch 4 train acc: 93.33 train loss: 0.0029 val acc: 86.15 val loss: 0.0074\n",
      "Sub epoch 5 train acc: 96.52 train loss: 0.0016 val acc: 88.63 val loss: 0.0065\n",
      "Sub epoch 6 train acc: 97.79 train loss: 0.0011 val acc: 88.73 val loss: 0.0069\n",
      "Train Loss: 0.0011, Train Acc: 97.79  Validation Loss: 0.0069, Validation Acc: 88.73\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 74.14 train loss: 0.0178 val acc: 83.63 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 87.99 train loss: 0.0051 val acc: 87.98 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 91.86 train loss: 0.0034 val acc: 87.70 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 93.70 train loss: 0.0025 val acc: 88.22 val loss: 0.0070\n",
      "Sub epoch 4 train acc: 95.34 train loss: 0.0020 val acc: 88.08 val loss: 0.0080\n",
      "Sub epoch 5 train acc: 95.96 train loss: 0.0016 val acc: 87.88 val loss: 0.0094\n",
      "Sub epoch 6 train acc: 96.59 train loss: 0.0015 val acc: 86.98 val loss: 0.0104\n",
      "Train Loss: 0.0015, Train Acc: 96.59  Validation Loss: 0.0104, Validation Acc: 86.98\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 79.73 train loss: 0.0151 val acc: 84.68 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 93.08 train loss: 0.0030 val acc: 88.40 val loss: 0.0067\n",
      "Sub epoch 2 train acc: 97.44 train loss: 0.0012 val acc: 89.27 val loss: 0.0069\n",
      "Sub epoch 3 train acc: 98.08 train loss: 0.0010 val acc: 89.37 val loss: 0.0073\n",
      "Sub epoch 4 train acc: 98.30 train loss: 0.0008 val acc: 89.27 val loss: 0.0074\n",
      "Sub epoch 5 train acc: 98.65 train loss: 0.0007 val acc: 88.98 val loss: 0.0077\n",
      "Sub epoch 6 train acc: 98.84 train loss: 0.0006 val acc: 89.10 val loss: 0.0080\n",
      "Train Loss: 0.0006, Train Acc: 98.84  Validation Loss: 0.0080, Validation Acc: 89.10\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 80.07 train loss: 0.0131 val acc: 86.05 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 92.90 train loss: 0.0032 val acc: 88.52 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 96.07 train loss: 0.0018 val acc: 89.07 val loss: 0.0073\n",
      "Sub epoch 3 train acc: 97.41 train loss: 0.0012 val acc: 87.87 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 97.83 train loss: 0.0010 val acc: 89.13 val loss: 0.0092\n",
      "Sub epoch 5 train acc: 98.22 train loss: 0.0008 val acc: 88.30 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 99.19 train loss: 0.0004 val acc: 89.10 val loss: 0.0105\n",
      "Train Loss: 0.0004, Train Acc: 99.19  Validation Loss: 0.0105, Validation Acc: 89.10\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 82.31 train loss: 0.0127 val acc: 85.23 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 95.41 train loss: 0.0022 val acc: 88.62 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 97.95 train loss: 0.0010 val acc: 88.60 val loss: 0.0094\n",
      "Sub epoch 3 train acc: 98.60 train loss: 0.0006 val acc: 88.85 val loss: 0.0108\n",
      "Sub epoch 4 train acc: 98.89 train loss: 0.0005 val acc: 88.02 val loss: 0.0109\n",
      "Sub epoch 5 train acc: 99.05 train loss: 0.0005 val acc: 88.18 val loss: 0.0132\n",
      "Sub epoch 6 train acc: 98.99 train loss: 0.0005 val acc: 88.57 val loss: 0.0128\n",
      "Train Loss: 0.0005, Train Acc: 98.99  Validation Loss: 0.0128, Validation Acc: 88.57\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 79.95 train loss: 0.0135 val acc: 86.13 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 94.44 train loss: 0.0027 val acc: 88.30 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 96.89 train loss: 0.0014 val acc: 88.93 val loss: 0.0074\n",
      "Sub epoch 3 train acc: 98.67 train loss: 0.0007 val acc: 89.37 val loss: 0.0083\n",
      "Sub epoch 4 train acc: 98.99 train loss: 0.0005 val acc: 89.28 val loss: 0.0087\n",
      "Sub epoch 5 train acc: 99.16 train loss: 0.0005 val acc: 89.15 val loss: 0.0090\n",
      "Sub epoch 6 train acc: 99.32 train loss: 0.0004 val acc: 89.08 val loss: 0.0095\n",
      "Train Loss: 0.0004, Train Acc: 99.32  Validation Loss: 0.0095, Validation Acc: 89.08\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 80.13 train loss: 0.0127 val acc: 85.95 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 94.15 train loss: 0.0029 val acc: 89.25 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 96.54 train loss: 0.0016 val acc: 89.58 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 97.56 train loss: 0.0011 val acc: 88.55 val loss: 0.0093\n",
      "Sub epoch 4 train acc: 98.11 train loss: 0.0008 val acc: 87.48 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 98.27 train loss: 0.0008 val acc: 89.42 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 98.44 train loss: 0.0007 val acc: 88.07 val loss: 0.0108\n",
      "Train Loss: 0.0007, Train Acc: 98.44  Validation Loss: 0.0108, Validation Acc: 88.07\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 81.20 train loss: 0.0126 val acc: 87.75 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 96.11 train loss: 0.0020 val acc: 88.88 val loss: 0.0072\n",
      "Sub epoch 2 train acc: 97.97 train loss: 0.0010 val acc: 88.27 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.41 train loss: 0.0007 val acc: 88.90 val loss: 0.0094\n",
      "Sub epoch 4 train acc: 98.97 train loss: 0.0005 val acc: 89.03 val loss: 0.0109\n",
      "Sub epoch 5 train acc: 98.80 train loss: 0.0006 val acc: 89.02 val loss: 0.0108\n",
      "Sub epoch 6 train acc: 98.69 train loss: 0.0006 val acc: 88.65 val loss: 0.0113\n",
      "Train Loss: 0.0006, Train Acc: 98.69  Validation Loss: 0.0113, Validation Acc: 88.65\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 82.45 train loss: 0.0115 val acc: 87.40 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 96.81 train loss: 0.0016 val acc: 89.13 val loss: 0.0084\n",
      "Sub epoch 2 train acc: 98.70 train loss: 0.0007 val acc: 89.38 val loss: 0.0104\n",
      "Sub epoch 3 train acc: 99.16 train loss: 0.0005 val acc: 88.58 val loss: 0.0107\n",
      "Sub epoch 4 train acc: 99.75 train loss: 0.0002 val acc: 89.32 val loss: 0.0112\n",
      "Sub epoch 5 train acc: 99.86 train loss: 0.0001 val acc: 89.32 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 99.88 train loss: 0.0001 val acc: 89.38 val loss: 0.0120\n",
      "Train Loss: 0.0001, Train Acc: 99.88  Validation Loss: 0.0120, Validation Acc: 89.38\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 80.83 train loss: 0.0128 val acc: 86.65 val loss: 0.0065\n",
      "Sub epoch 1 train acc: 95.99 train loss: 0.0019 val acc: 88.85 val loss: 0.0075\n",
      "Sub epoch 2 train acc: 97.98 train loss: 0.0010 val acc: 88.98 val loss: 0.0081\n",
      "Sub epoch 3 train acc: 98.66 train loss: 0.0007 val acc: 88.88 val loss: 0.0090\n",
      "Sub epoch 4 train acc: 98.83 train loss: 0.0006 val acc: 88.92 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 98.78 train loss: 0.0006 val acc: 88.90 val loss: 0.0106\n",
      "Sub epoch 6 train acc: 98.99 train loss: 0.0005 val acc: 88.40 val loss: 0.0127\n",
      "Train Loss: 0.0005, Train Acc: 98.99  Validation Loss: 0.0127, Validation Acc: 88.40\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 81.65 train loss: 0.0124 val acc: 87.67 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 95.47 train loss: 0.0024 val acc: 89.05 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 96.76 train loss: 0.0017 val acc: 89.38 val loss: 0.0077\n",
      "Sub epoch 3 train acc: 97.67 train loss: 0.0013 val acc: 89.45 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 98.17 train loss: 0.0010 val acc: 89.32 val loss: 0.0086\n",
      "Sub epoch 5 train acc: 98.60 train loss: 0.0008 val acc: 89.45 val loss: 0.0089\n",
      "Sub epoch 6 train acc: 98.89 train loss: 0.0007 val acc: 89.40 val loss: 0.0094\n",
      "Train Loss: 0.0007, Train Acc: 98.89  Validation Loss: 0.0094, Validation Acc: 89.40\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 82.03 train loss: 0.0115 val acc: 86.50 val loss: 0.0067\n",
      "Sub epoch 1 train acc: 96.43 train loss: 0.0019 val acc: 88.88 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 98.17 train loss: 0.0010 val acc: 89.02 val loss: 0.0080\n",
      "Sub epoch 3 train acc: 98.68 train loss: 0.0007 val acc: 89.10 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 98.83 train loss: 0.0006 val acc: 89.33 val loss: 0.0105\n",
      "Sub epoch 5 train acc: 99.55 train loss: 0.0003 val acc: 89.57 val loss: 0.0103\n",
      "Sub epoch 6 train acc: 99.74 train loss: 0.0002 val acc: 89.50 val loss: 0.0107\n",
      "Train Loss: 0.0002, Train Acc: 99.74  Validation Loss: 0.0107, Validation Acc: 89.50\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 81.20 train loss: 0.0124 val acc: 86.88 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 97.10 train loss: 0.0016 val acc: 89.28 val loss: 0.0080\n",
      "Sub epoch 2 train acc: 98.64 train loss: 0.0008 val acc: 89.43 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.97 train loss: 0.0006 val acc: 89.47 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 99.06 train loss: 0.0005 val acc: 89.13 val loss: 0.0113\n",
      "Sub epoch 5 train acc: 99.00 train loss: 0.0005 val acc: 89.43 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 99.00 train loss: 0.0005 val acc: 89.02 val loss: 0.0116\n",
      "Train Loss: 0.0005, Train Acc: 99.00  Validation Loss: 0.0116, Validation Acc: 89.02\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 82.50 train loss: 0.0119 val acc: 87.60 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 97.76 train loss: 0.0013 val acc: 89.25 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 99.33 train loss: 0.0004 val acc: 89.32 val loss: 0.0090\n",
      "Sub epoch 3 train acc: 99.50 train loss: 0.0003 val acc: 89.43 val loss: 0.0094\n",
      "Sub epoch 4 train acc: 99.60 train loss: 0.0003 val acc: 89.48 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 99.63 train loss: 0.0003 val acc: 89.45 val loss: 0.0099\n",
      "Sub epoch 6 train acc: 99.70 train loss: 0.0002 val acc: 89.60 val loss: 0.0103\n",
      "Train Loss: 0.0002, Train Acc: 99.70  Validation Loss: 0.0103, Validation Acc: 89.60\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 82.29 train loss: 0.0114 val acc: 87.58 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 97.10 train loss: 0.0016 val acc: 89.43 val loss: 0.0088\n",
      "Sub epoch 2 train acc: 98.66 train loss: 0.0007 val acc: 89.67 val loss: 0.0093\n",
      "Sub epoch 3 train acc: 99.08 train loss: 0.0005 val acc: 89.08 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 99.15 train loss: 0.0005 val acc: 89.58 val loss: 0.0112\n",
      "Sub epoch 5 train acc: 99.07 train loss: 0.0005 val acc: 89.13 val loss: 0.0116\n",
      "Sub epoch 6 train acc: 99.57 train loss: 0.0002 val acc: 89.62 val loss: 0.0119\n",
      "Train Loss: 0.0002, Train Acc: 99.57  Validation Loss: 0.0119, Validation Acc: 89.62\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 82.62 train loss: 0.0123 val acc: 87.77 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 97.59 train loss: 0.0013 val acc: 89.30 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.96 train loss: 0.0006 val acc: 89.57 val loss: 0.0098\n",
      "Sub epoch 3 train acc: 99.12 train loss: 0.0005 val acc: 89.08 val loss: 0.0112\n",
      "Sub epoch 4 train acc: 99.28 train loss: 0.0004 val acc: 89.30 val loss: 0.0122\n",
      "Sub epoch 5 train acc: 99.17 train loss: 0.0004 val acc: 89.25 val loss: 0.0123\n",
      "Sub epoch 6 train acc: 99.00 train loss: 0.0005 val acc: 88.78 val loss: 0.0131\n",
      "Train Loss: 0.0005, Train Acc: 99.00  Validation Loss: 0.0131, Validation Acc: 88.78\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 82.36 train loss: 0.0116 val acc: 87.22 val loss: 0.0071\n",
      "Sub epoch 1 train acc: 97.49 train loss: 0.0013 val acc: 89.35 val loss: 0.0083\n",
      "Sub epoch 2 train acc: 98.69 train loss: 0.0007 val acc: 89.67 val loss: 0.0099\n",
      "Sub epoch 3 train acc: 99.32 train loss: 0.0004 val acc: 89.48 val loss: 0.0102\n",
      "Sub epoch 4 train acc: 99.47 train loss: 0.0003 val acc: 89.55 val loss: 0.0106\n",
      "Sub epoch 5 train acc: 99.53 train loss: 0.0003 val acc: 89.53 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 99.58 train loss: 0.0002 val acc: 89.60 val loss: 0.0116\n",
      "Train Loss: 0.0002, Train Acc: 99.58  Validation Loss: 0.0116, Validation Acc: 89.60\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 80.01 train loss: 0.0123 val acc: 86.50 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 96.52 train loss: 0.0018 val acc: 89.40 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 98.15 train loss: 0.0010 val acc: 89.65 val loss: 0.0085\n",
      "Sub epoch 3 train acc: 98.58 train loss: 0.0007 val acc: 89.37 val loss: 0.0100\n",
      "Sub epoch 4 train acc: 98.72 train loss: 0.0007 val acc: 89.08 val loss: 0.0110\n",
      "Sub epoch 5 train acc: 98.92 train loss: 0.0006 val acc: 89.50 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 98.85 train loss: 0.0006 val acc: 88.92 val loss: 0.0127\n",
      "Train Loss: 0.0006, Train Acc: 98.85  Validation Loss: 0.0127, Validation Acc: 88.92\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 80.07 train loss: 0.0125 val acc: 87.45 val loss: 0.0069\n",
      "Sub epoch 1 train acc: 97.00 train loss: 0.0016 val acc: 89.43 val loss: 0.0078\n",
      "Sub epoch 2 train acc: 98.40 train loss: 0.0009 val acc: 89.58 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 98.74 train loss: 0.0007 val acc: 89.82 val loss: 0.0098\n",
      "Sub epoch 4 train acc: 98.77 train loss: 0.0006 val acc: 89.58 val loss: 0.0109\n",
      "Sub epoch 5 train acc: 98.87 train loss: 0.0006 val acc: 89.38 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 98.82 train loss: 0.0006 val acc: 89.28 val loss: 0.0121\n",
      "Train Loss: 0.0006, Train Acc: 98.82  Validation Loss: 0.0121, Validation Acc: 89.28\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 72.13 train loss: 0.0141 val acc: 81.40 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 84.52 train loss: 0.0070 val acc: 83.67 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 88.23 train loss: 0.0050 val acc: 85.57 val loss: 0.0067\n",
      "Sub epoch 3 train acc: 90.66 train loss: 0.0040 val acc: 86.33 val loss: 0.0074\n",
      "Sub epoch 4 train acc: 95.01 train loss: 0.0021 val acc: 87.65 val loss: 0.0065\n",
      "Sub epoch 5 train acc: 96.77 train loss: 0.0014 val acc: 87.45 val loss: 0.0070\n",
      "Sub epoch 6 train acc: 97.60 train loss: 0.0011 val acc: 87.63 val loss: 0.0074\n",
      "Train Loss: 0.0011, Train Acc: 97.60  Validation Loss: 0.0074, Validation Acc: 87.63\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 73.41 train loss: 0.0181 val acc: 79.95 val loss: 0.0093\n",
      "Sub epoch 1 train acc: 85.18 train loss: 0.0064 val acc: 85.87 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 90.00 train loss: 0.0042 val acc: 86.22 val loss: 0.0065\n",
      "Sub epoch 3 train acc: 92.37 train loss: 0.0030 val acc: 87.83 val loss: 0.0065\n",
      "Sub epoch 4 train acc: 93.85 train loss: 0.0027 val acc: 86.52 val loss: 0.0075\n",
      "Sub epoch 5 train acc: 94.90 train loss: 0.0021 val acc: 87.05 val loss: 0.0090\n",
      "Sub epoch 6 train acc: 95.33 train loss: 0.0020 val acc: 87.12 val loss: 0.0089\n",
      "Train Loss: 0.0020, Train Acc: 95.33  Validation Loss: 0.0089, Validation Acc: 87.12\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 76.71 train loss: 0.0187 val acc: 85.27 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 90.79 train loss: 0.0042 val acc: 87.12 val loss: 0.0068\n",
      "Sub epoch 2 train acc: 92.84 train loss: 0.0031 val acc: 87.43 val loss: 0.0067\n",
      "Sub epoch 3 train acc: 94.10 train loss: 0.0025 val acc: 87.33 val loss: 0.0068\n",
      "Sub epoch 4 train acc: 95.16 train loss: 0.0021 val acc: 87.35 val loss: 0.0069\n",
      "Sub epoch 5 train acc: 96.23 train loss: 0.0018 val acc: 87.25 val loss: 0.0072\n",
      "Sub epoch 6 train acc: 96.69 train loss: 0.0015 val acc: 87.63 val loss: 0.0075\n",
      "Train Loss: 0.0015, Train Acc: 96.69  Validation Loss: 0.0075, Validation Acc: 87.63\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 75.96 train loss: 0.0167 val acc: 85.87 val loss: 0.0077\n",
      "Sub epoch 1 train acc: 90.27 train loss: 0.0043 val acc: 87.67 val loss: 0.0062\n",
      "Sub epoch 2 train acc: 93.53 train loss: 0.0028 val acc: 88.20 val loss: 0.0068\n",
      "Sub epoch 3 train acc: 95.12 train loss: 0.0021 val acc: 88.05 val loss: 0.0078\n",
      "Sub epoch 4 train acc: 96.36 train loss: 0.0016 val acc: 87.95 val loss: 0.0082\n",
      "Sub epoch 5 train acc: 98.21 train loss: 0.0009 val acc: 88.40 val loss: 0.0080\n",
      "Sub epoch 6 train acc: 98.87 train loss: 0.0006 val acc: 88.58 val loss: 0.0084\n",
      "Train Loss: 0.0006, Train Acc: 98.87  Validation Loss: 0.0084, Validation Acc: 88.58\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 76.55 train loss: 0.0159 val acc: 85.60 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 91.77 train loss: 0.0037 val acc: 88.07 val loss: 0.0065\n",
      "Sub epoch 2 train acc: 95.17 train loss: 0.0022 val acc: 88.35 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 96.66 train loss: 0.0015 val acc: 88.35 val loss: 0.0081\n",
      "Sub epoch 4 train acc: 97.46 train loss: 0.0012 val acc: 87.92 val loss: 0.0095\n",
      "Sub epoch 5 train acc: 97.87 train loss: 0.0009 val acc: 88.55 val loss: 0.0101\n",
      "Sub epoch 6 train acc: 97.66 train loss: 0.0010 val acc: 87.15 val loss: 0.0111\n",
      "Train Loss: 0.0010, Train Acc: 97.66  Validation Loss: 0.0111, Validation Acc: 87.15\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 80.85 train loss: 0.0130 val acc: 87.15 val loss: 0.0073\n",
      "Sub epoch 1 train acc: 95.36 train loss: 0.0021 val acc: 88.97 val loss: 0.0075\n",
      "Sub epoch 2 train acc: 98.43 train loss: 0.0008 val acc: 89.23 val loss: 0.0082\n",
      "Sub epoch 3 train acc: 98.87 train loss: 0.0006 val acc: 89.13 val loss: 0.0085\n",
      "Sub epoch 4 train acc: 99.09 train loss: 0.0005 val acc: 89.08 val loss: 0.0089\n",
      "Sub epoch 5 train acc: 99.25 train loss: 0.0004 val acc: 89.08 val loss: 0.0093\n",
      "Sub epoch 6 train acc: 99.44 train loss: 0.0004 val acc: 89.12 val loss: 0.0097\n",
      "Train Loss: 0.0004, Train Acc: 99.44  Validation Loss: 0.0097, Validation Acc: 89.12\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 80.80 train loss: 0.0129 val acc: 84.63 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 94.58 train loss: 0.0026 val acc: 88.43 val loss: 0.0071\n",
      "Sub epoch 2 train acc: 97.20 train loss: 0.0014 val acc: 88.57 val loss: 0.0072\n",
      "Sub epoch 3 train acc: 98.12 train loss: 0.0009 val acc: 88.52 val loss: 0.0088\n",
      "Sub epoch 4 train acc: 98.43 train loss: 0.0007 val acc: 87.88 val loss: 0.0097\n",
      "Sub epoch 5 train acc: 98.64 train loss: 0.0006 val acc: 88.13 val loss: 0.0105\n",
      "Sub epoch 6 train acc: 99.40 train loss: 0.0003 val acc: 88.37 val loss: 0.0103\n",
      "Train Loss: 0.0003, Train Acc: 99.40  Validation Loss: 0.0103, Validation Acc: 88.37\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 81.06 train loss: 0.0128 val acc: 85.92 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 95.51 train loss: 0.0022 val acc: 88.72 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 97.83 train loss: 0.0011 val acc: 88.67 val loss: 0.0086\n",
      "Sub epoch 3 train acc: 98.61 train loss: 0.0007 val acc: 88.83 val loss: 0.0092\n",
      "Sub epoch 4 train acc: 99.00 train loss: 0.0005 val acc: 88.43 val loss: 0.0112\n",
      "Sub epoch 5 train acc: 98.77 train loss: 0.0006 val acc: 87.77 val loss: 0.0107\n",
      "Sub epoch 6 train acc: 98.59 train loss: 0.0006 val acc: 88.55 val loss: 0.0102\n",
      "Train Loss: 0.0006, Train Acc: 98.59  Validation Loss: 0.0102, Validation Acc: 88.55\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 81.42 train loss: 0.0126 val acc: 85.58 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 95.82 train loss: 0.0021 val acc: 88.92 val loss: 0.0076\n",
      "Sub epoch 2 train acc: 98.01 train loss: 0.0010 val acc: 88.37 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 99.19 train loss: 0.0005 val acc: 88.92 val loss: 0.0096\n",
      "Sub epoch 4 train acc: 99.39 train loss: 0.0004 val acc: 88.92 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 99.46 train loss: 0.0003 val acc: 88.75 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 99.53 train loss: 0.0003 val acc: 88.85 val loss: 0.0109\n",
      "Train Loss: 0.0003, Train Acc: 99.53  Validation Loss: 0.0109, Validation Acc: 88.85\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 81.91 train loss: 0.0117 val acc: 86.82 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 96.40 train loss: 0.0019 val acc: 88.47 val loss: 0.0082\n",
      "Sub epoch 2 train acc: 98.38 train loss: 0.0009 val acc: 88.43 val loss: 0.0084\n",
      "Sub epoch 3 train acc: 98.75 train loss: 0.0006 val acc: 88.53 val loss: 0.0099\n",
      "Sub epoch 4 train acc: 98.86 train loss: 0.0006 val acc: 87.97 val loss: 0.0108\n",
      "Sub epoch 5 train acc: 98.95 train loss: 0.0005 val acc: 88.67 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 98.84 train loss: 0.0006 val acc: 88.58 val loss: 0.0118\n",
      "Train Loss: 0.0006, Train Acc: 98.84  Validation Loss: 0.0118, Validation Acc: 88.58\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 84.24 train loss: 0.0108 val acc: 86.18 val loss: 0.0075\n",
      "Sub epoch 1 train acc: 97.61 train loss: 0.0013 val acc: 88.73 val loss: 0.0083\n",
      "Sub epoch 2 train acc: 99.13 train loss: 0.0005 val acc: 88.45 val loss: 0.0100\n",
      "Sub epoch 3 train acc: 99.43 train loss: 0.0003 val acc: 88.90 val loss: 0.0115\n",
      "Sub epoch 4 train acc: 99.34 train loss: 0.0003 val acc: 89.13 val loss: 0.0115\n",
      "Sub epoch 5 train acc: 99.07 train loss: 0.0005 val acc: 88.78 val loss: 0.0130\n",
      "Sub epoch 6 train acc: 99.03 train loss: 0.0005 val acc: 88.22 val loss: 0.0140\n",
      "Train Loss: 0.0005, Train Acc: 99.03  Validation Loss: 0.0140, Validation Acc: 88.22\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 82.95 train loss: 0.0110 val acc: 87.17 val loss: 0.0070\n",
      "Sub epoch 1 train acc: 97.38 train loss: 0.0014 val acc: 89.20 val loss: 0.0083\n",
      "Sub epoch 2 train acc: 98.85 train loss: 0.0007 val acc: 89.18 val loss: 0.0094\n",
      "Sub epoch 3 train acc: 99.22 train loss: 0.0005 val acc: 88.50 val loss: 0.0113\n",
      "Sub epoch 4 train acc: 99.55 train loss: 0.0002 val acc: 89.52 val loss: 0.0107\n",
      "Sub epoch 5 train acc: 99.73 train loss: 0.0002 val acc: 89.42 val loss: 0.0113\n",
      "Sub epoch 6 train acc: 99.79 train loss: 0.0001 val acc: 89.53 val loss: 0.0116\n",
      "Train Loss: 0.0001, Train Acc: 99.79  Validation Loss: 0.0116, Validation Acc: 89.53\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 81.93 train loss: 0.0118 val acc: 86.22 val loss: 0.0074\n",
      "Sub epoch 1 train acc: 96.73 train loss: 0.0017 val acc: 88.92 val loss: 0.0077\n",
      "Sub epoch 2 train acc: 98.68 train loss: 0.0008 val acc: 88.48 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 99.04 train loss: 0.0005 val acc: 89.05 val loss: 0.0094\n",
      "Sub epoch 4 train acc: 99.14 train loss: 0.0004 val acc: 88.77 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 99.11 train loss: 0.0005 val acc: 88.27 val loss: 0.0117\n",
      "Sub epoch 6 train acc: 99.08 train loss: 0.0005 val acc: 88.52 val loss: 0.0123\n",
      "Train Loss: 0.0005, Train Acc: 99.08  Validation Loss: 0.0123, Validation Acc: 88.52\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 82.70 train loss: 0.0112 val acc: 87.42 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 95.98 train loss: 0.0021 val acc: 89.43 val loss: 0.0079\n",
      "Sub epoch 2 train acc: 97.35 train loss: 0.0014 val acc: 89.60 val loss: 0.0081\n",
      "Sub epoch 3 train acc: 98.06 train loss: 0.0011 val acc: 89.57 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 98.50 train loss: 0.0009 val acc: 89.52 val loss: 0.0088\n",
      "Sub epoch 5 train acc: 98.79 train loss: 0.0007 val acc: 89.53 val loss: 0.0092\n",
      "Sub epoch 6 train acc: 99.02 train loss: 0.0006 val acc: 89.43 val loss: 0.0097\n",
      "Train Loss: 0.0006, Train Acc: 99.02  Validation Loss: 0.0097, Validation Acc: 89.43\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 80.97 train loss: 0.0119 val acc: 86.37 val loss: 0.0081\n",
      "Sub epoch 1 train acc: 95.99 train loss: 0.0021 val acc: 89.18 val loss: 0.0069\n",
      "Sub epoch 2 train acc: 97.82 train loss: 0.0011 val acc: 89.37 val loss: 0.0078\n",
      "Sub epoch 3 train acc: 98.50 train loss: 0.0008 val acc: 88.95 val loss: 0.0084\n",
      "Sub epoch 4 train acc: 98.81 train loss: 0.0006 val acc: 88.80 val loss: 0.0109\n",
      "Sub epoch 5 train acc: 99.44 train loss: 0.0003 val acc: 89.20 val loss: 0.0107\n",
      "Sub epoch 6 train acc: 99.66 train loss: 0.0002 val acc: 89.15 val loss: 0.0112\n",
      "Train Loss: 0.0002, Train Acc: 99.66  Validation Loss: 0.0112, Validation Acc: 89.15\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 81.12 train loss: 0.0118 val acc: 86.03 val loss: 0.0072\n",
      "Sub epoch 1 train acc: 96.75 train loss: 0.0016 val acc: 89.22 val loss: 0.0082\n",
      "Sub epoch 2 train acc: 98.58 train loss: 0.0008 val acc: 89.28 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 98.88 train loss: 0.0006 val acc: 88.88 val loss: 0.0093\n",
      "Sub epoch 4 train acc: 98.90 train loss: 0.0006 val acc: 88.68 val loss: 0.0104\n",
      "Sub epoch 5 train acc: 98.84 train loss: 0.0006 val acc: 88.90 val loss: 0.0120\n",
      "Sub epoch 6 train acc: 98.95 train loss: 0.0005 val acc: 88.57 val loss: 0.0133\n",
      "Train Loss: 0.0005, Train Acc: 98.95  Validation Loss: 0.0133, Validation Acc: 88.57\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 82.29 train loss: 0.0117 val acc: 87.90 val loss: 0.0086\n",
      "Sub epoch 1 train acc: 97.61 train loss: 0.0013 val acc: 89.37 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 99.17 train loss: 0.0005 val acc: 89.78 val loss: 0.0090\n",
      "Sub epoch 3 train acc: 99.37 train loss: 0.0004 val acc: 89.63 val loss: 0.0093\n",
      "Sub epoch 4 train acc: 99.47 train loss: 0.0004 val acc: 89.73 val loss: 0.0096\n",
      "Sub epoch 5 train acc: 99.54 train loss: 0.0003 val acc: 89.60 val loss: 0.0100\n",
      "Sub epoch 6 train acc: 99.61 train loss: 0.0003 val acc: 89.50 val loss: 0.0103\n",
      "Train Loss: 0.0003, Train Acc: 99.61  Validation Loss: 0.0103, Validation Acc: 89.50\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 80.27 train loss: 0.0122 val acc: 86.70 val loss: 0.0078\n",
      "Sub epoch 1 train acc: 96.74 train loss: 0.0017 val acc: 89.23 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 98.38 train loss: 0.0009 val acc: 89.15 val loss: 0.0090\n",
      "Sub epoch 3 train acc: 98.70 train loss: 0.0007 val acc: 89.13 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 98.83 train loss: 0.0006 val acc: 89.17 val loss: 0.0118\n",
      "Sub epoch 5 train acc: 98.81 train loss: 0.0006 val acc: 88.27 val loss: 0.0137\n",
      "Sub epoch 6 train acc: 99.50 train loss: 0.0003 val acc: 88.92 val loss: 0.0134\n",
      "Train Loss: 0.0003, Train Acc: 99.50  Validation Loss: 0.0134, Validation Acc: 88.92\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 81.90 train loss: 0.0119 val acc: 86.37 val loss: 0.0091\n",
      "Sub epoch 1 train acc: 97.22 train loss: 0.0015 val acc: 88.73 val loss: 0.0096\n",
      "Sub epoch 2 train acc: 98.73 train loss: 0.0008 val acc: 88.60 val loss: 0.0104\n",
      "Sub epoch 3 train acc: 98.98 train loss: 0.0006 val acc: 89.12 val loss: 0.0111\n",
      "Sub epoch 4 train acc: 99.10 train loss: 0.0005 val acc: 88.90 val loss: 0.0125\n",
      "Sub epoch 5 train acc: 99.01 train loss: 0.0005 val acc: 88.57 val loss: 0.0147\n",
      "Sub epoch 6 train acc: 99.08 train loss: 0.0005 val acc: 87.93 val loss: 0.0157\n",
      "Train Loss: 0.0005, Train Acc: 99.08  Validation Loss: 0.0157, Validation Acc: 87.93\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 81.20 train loss: 0.0119 val acc: 86.27 val loss: 0.0076\n",
      "Sub epoch 1 train acc: 97.63 train loss: 0.0013 val acc: 88.55 val loss: 0.0096\n",
      "Sub epoch 2 train acc: 98.76 train loss: 0.0007 val acc: 88.75 val loss: 0.0096\n",
      "Sub epoch 3 train acc: 99.36 train loss: 0.0004 val acc: 89.13 val loss: 0.0105\n",
      "Sub epoch 4 train acc: 99.53 train loss: 0.0003 val acc: 89.15 val loss: 0.0111\n",
      "Sub epoch 5 train acc: 99.60 train loss: 0.0002 val acc: 89.13 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 99.68 train loss: 0.0002 val acc: 89.17 val loss: 0.0119\n",
      "Train Loss: 0.0002, Train Acc: 99.68  Validation Loss: 0.0119, Validation Acc: 89.17\n",
      "Epoch:  0\n",
      "Training on 9000 samples\n",
      "Sub epoch 0 train acc: 73.13 train loss: 0.0136 val acc: 80.18 val loss: 0.0107\n",
      "Sub epoch 1 train acc: 84.77 train loss: 0.0068 val acc: 84.38 val loss: 0.0070\n",
      "Sub epoch 2 train acc: 88.46 train loss: 0.0050 val acc: 83.13 val loss: 0.0084\n",
      "Sub epoch 3 train acc: 90.41 train loss: 0.0041 val acc: 85.10 val loss: 0.0089\n",
      "Sub epoch 4 train acc: 92.13 train loss: 0.0032 val acc: 85.70 val loss: 0.0090\n",
      "Sub epoch 5 train acc: 93.06 train loss: 0.0031 val acc: 83.83 val loss: 0.0097\n",
      "Sub epoch 6 train acc: 94.53 train loss: 0.0025 val acc: 85.05 val loss: 0.0105\n",
      "Train Loss: 0.0025, Train Acc: 94.53  Validation Loss: 0.0105, Validation Acc: 85.05\n",
      "Epoch:  1\n",
      "Training on 11250 samples\n",
      "Sub epoch 0 train acc: 76.92 train loss: 0.0160 val acc: 83.27 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 89.16 train loss: 0.0048 val acc: 86.12 val loss: 0.0077\n",
      "Sub epoch 2 train acc: 93.26 train loss: 0.0029 val acc: 86.40 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 95.24 train loss: 0.0020 val acc: 86.73 val loss: 0.0091\n",
      "Sub epoch 4 train acc: 95.97 train loss: 0.0018 val acc: 86.55 val loss: 0.0098\n",
      "Sub epoch 5 train acc: 96.15 train loss: 0.0018 val acc: 85.73 val loss: 0.0104\n",
      "Sub epoch 6 train acc: 97.31 train loss: 0.0012 val acc: 86.53 val loss: 0.0119\n",
      "Train Loss: 0.0012, Train Acc: 97.31  Validation Loss: 0.0119, Validation Acc: 86.53\n",
      "Epoch:  2\n",
      "Training on 13500 samples\n",
      "Sub epoch 0 train acc: 77.74 train loss: 0.0169 val acc: 84.17 val loss: 0.0086\n",
      "Sub epoch 1 train acc: 91.75 train loss: 0.0038 val acc: 86.88 val loss: 0.0082\n",
      "Sub epoch 2 train acc: 95.38 train loss: 0.0020 val acc: 87.77 val loss: 0.0084\n",
      "Sub epoch 3 train acc: 96.97 train loss: 0.0013 val acc: 87.45 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 98.66 train loss: 0.0006 val acc: 88.00 val loss: 0.0100\n",
      "Sub epoch 5 train acc: 99.35 train loss: 0.0004 val acc: 88.10 val loss: 0.0105\n",
      "Sub epoch 6 train acc: 99.57 train loss: 0.0003 val acc: 88.08 val loss: 0.0110\n",
      "Train Loss: 0.0003, Train Acc: 99.57  Validation Loss: 0.0110, Validation Acc: 88.08\n",
      "Epoch:  3\n",
      "Training on 15750 samples\n",
      "Sub epoch 0 train acc: 78.09 train loss: 0.0171 val acc: 84.78 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 92.89 train loss: 0.0034 val acc: 87.37 val loss: 0.0074\n",
      "Sub epoch 2 train acc: 96.24 train loss: 0.0017 val acc: 87.42 val loss: 0.0087\n",
      "Sub epoch 3 train acc: 97.48 train loss: 0.0011 val acc: 87.80 val loss: 0.0097\n",
      "Sub epoch 4 train acc: 97.89 train loss: 0.0009 val acc: 87.25 val loss: 0.0116\n",
      "Sub epoch 5 train acc: 98.32 train loss: 0.0008 val acc: 87.00 val loss: 0.0127\n",
      "Sub epoch 6 train acc: 98.75 train loss: 0.0006 val acc: 87.30 val loss: 0.0130\n",
      "Train Loss: 0.0006, Train Acc: 98.75  Validation Loss: 0.0130, Validation Acc: 87.30\n",
      "Epoch:  4\n",
      "Training on 18000 samples\n",
      "Sub epoch 0 train acc: 82.60 train loss: 0.0133 val acc: 85.47 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 94.49 train loss: 0.0026 val acc: 87.58 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 96.54 train loss: 0.0016 val acc: 87.70 val loss: 0.0084\n",
      "Sub epoch 3 train acc: 97.52 train loss: 0.0012 val acc: 87.92 val loss: 0.0091\n",
      "Sub epoch 4 train acc: 98.27 train loss: 0.0008 val acc: 87.98 val loss: 0.0095\n",
      "Sub epoch 5 train acc: 98.84 train loss: 0.0006 val acc: 88.07 val loss: 0.0102\n",
      "Sub epoch 6 train acc: 99.27 train loss: 0.0004 val acc: 87.97 val loss: 0.0112\n",
      "Train Loss: 0.0004, Train Acc: 99.27  Validation Loss: 0.0112, Validation Acc: 87.97\n",
      "Epoch:  5\n",
      "Training on 20250 samples\n",
      "Sub epoch 0 train acc: 80.28 train loss: 0.0145 val acc: 85.70 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 94.18 train loss: 0.0028 val acc: 87.35 val loss: 0.0073\n",
      "Sub epoch 2 train acc: 96.62 train loss: 0.0016 val acc: 87.48 val loss: 0.0088\n",
      "Sub epoch 3 train acc: 97.94 train loss: 0.0010 val acc: 87.38 val loss: 0.0104\n",
      "Sub epoch 4 train acc: 98.38 train loss: 0.0008 val acc: 87.30 val loss: 0.0118\n",
      "Sub epoch 5 train acc: 99.34 train loss: 0.0003 val acc: 88.28 val loss: 0.0114\n",
      "Sub epoch 6 train acc: 99.74 train loss: 0.0002 val acc: 87.95 val loss: 0.0119\n",
      "Train Loss: 0.0002, Train Acc: 99.74  Validation Loss: 0.0119, Validation Acc: 87.95\n",
      "Epoch:  6\n",
      "Training on 22500 samples\n",
      "Sub epoch 0 train acc: 82.13 train loss: 0.0127 val acc: 84.83 val loss: 0.0085\n",
      "Sub epoch 1 train acc: 95.71 train loss: 0.0021 val acc: 87.27 val loss: 0.0090\n",
      "Sub epoch 2 train acc: 98.03 train loss: 0.0009 val acc: 87.18 val loss: 0.0108\n",
      "Sub epoch 3 train acc: 98.94 train loss: 0.0005 val acc: 87.52 val loss: 0.0110\n",
      "Sub epoch 4 train acc: 99.27 train loss: 0.0004 val acc: 86.73 val loss: 0.0144\n",
      "Sub epoch 5 train acc: 98.98 train loss: 0.0005 val acc: 87.43 val loss: 0.0125\n",
      "Sub epoch 6 train acc: 99.04 train loss: 0.0005 val acc: 86.93 val loss: 0.0145\n",
      "Train Loss: 0.0005, Train Acc: 99.04  Validation Loss: 0.0145, Validation Acc: 86.93\n",
      "Epoch:  7\n",
      "Training on 24750 samples\n",
      "Sub epoch 0 train acc: 81.74 train loss: 0.0126 val acc: 85.75 val loss: 0.0079\n",
      "Sub epoch 1 train acc: 95.83 train loss: 0.0022 val acc: 87.77 val loss: 0.0085\n",
      "Sub epoch 2 train acc: 98.54 train loss: 0.0008 val acc: 88.50 val loss: 0.0086\n",
      "Sub epoch 3 train acc: 98.88 train loss: 0.0007 val acc: 88.48 val loss: 0.0088\n",
      "Sub epoch 4 train acc: 99.09 train loss: 0.0006 val acc: 88.33 val loss: 0.0091\n",
      "Sub epoch 5 train acc: 99.22 train loss: 0.0005 val acc: 88.43 val loss: 0.0095\n",
      "Sub epoch 6 train acc: 99.36 train loss: 0.0004 val acc: 88.38 val loss: 0.0098\n",
      "Train Loss: 0.0004, Train Acc: 99.36  Validation Loss: 0.0098, Validation Acc: 88.38\n",
      "Epoch:  8\n",
      "Training on 27000 samples\n",
      "Sub epoch 0 train acc: 79.57 train loss: 0.0141 val acc: 84.97 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 94.39 train loss: 0.0027 val acc: 88.20 val loss: 0.0079\n",
      "Sub epoch 2 train acc: 96.73 train loss: 0.0015 val acc: 88.18 val loss: 0.0087\n",
      "Sub epoch 3 train acc: 97.85 train loss: 0.0010 val acc: 88.27 val loss: 0.0098\n",
      "Sub epoch 4 train acc: 98.41 train loss: 0.0007 val acc: 88.27 val loss: 0.0106\n",
      "Sub epoch 5 train acc: 98.54 train loss: 0.0007 val acc: 88.10 val loss: 0.0124\n",
      "Sub epoch 6 train acc: 99.33 train loss: 0.0003 val acc: 88.27 val loss: 0.0124\n",
      "Train Loss: 0.0003, Train Acc: 99.33  Validation Loss: 0.0124, Validation Acc: 88.27\n",
      "Epoch:  9\n",
      "Training on 29250 samples\n",
      "Sub epoch 0 train acc: 80.48 train loss: 0.0137 val acc: 84.53 val loss: 0.0089\n",
      "Sub epoch 1 train acc: 95.70 train loss: 0.0021 val acc: 88.15 val loss: 0.0090\n",
      "Sub epoch 2 train acc: 97.78 train loss: 0.0012 val acc: 88.27 val loss: 0.0092\n",
      "Sub epoch 3 train acc: 98.56 train loss: 0.0008 val acc: 88.23 val loss: 0.0116\n",
      "Sub epoch 4 train acc: 98.74 train loss: 0.0006 val acc: 87.27 val loss: 0.0127\n",
      "Sub epoch 5 train acc: 98.72 train loss: 0.0007 val acc: 88.07 val loss: 0.0121\n",
      "Sub epoch 6 train acc: 98.74 train loss: 0.0006 val acc: 87.88 val loss: 0.0131\n",
      "Train Loss: 0.0006, Train Acc: 98.74  Validation Loss: 0.0131, Validation Acc: 87.88\n",
      "Epoch:  10\n",
      "Training on 31500 samples\n",
      "Sub epoch 0 train acc: 80.91 train loss: 0.0128 val acc: 85.92 val loss: 0.0086\n",
      "Sub epoch 1 train acc: 96.43 train loss: 0.0019 val acc: 88.17 val loss: 0.0087\n",
      "Sub epoch 2 train acc: 98.24 train loss: 0.0009 val acc: 88.25 val loss: 0.0101\n",
      "Sub epoch 3 train acc: 99.26 train loss: 0.0004 val acc: 88.32 val loss: 0.0103\n",
      "Sub epoch 4 train acc: 99.50 train loss: 0.0003 val acc: 88.53 val loss: 0.0106\n",
      "Sub epoch 5 train acc: 99.55 train loss: 0.0003 val acc: 88.57 val loss: 0.0109\n",
      "Sub epoch 6 train acc: 99.63 train loss: 0.0003 val acc: 88.57 val loss: 0.0113\n",
      "Train Loss: 0.0003, Train Acc: 99.63  Validation Loss: 0.0113, Validation Acc: 88.57\n",
      "Epoch:  11\n",
      "Training on 33750 samples\n",
      "Sub epoch 0 train acc: 79.15 train loss: 0.0136 val acc: 85.63 val loss: 0.0086\n",
      "Sub epoch 1 train acc: 95.75 train loss: 0.0021 val acc: 88.27 val loss: 0.0086\n",
      "Sub epoch 2 train acc: 97.88 train loss: 0.0011 val acc: 88.27 val loss: 0.0095\n",
      "Sub epoch 3 train acc: 98.47 train loss: 0.0008 val acc: 88.32 val loss: 0.0098\n",
      "Sub epoch 4 train acc: 98.61 train loss: 0.0007 val acc: 88.28 val loss: 0.0117\n",
      "Sub epoch 5 train acc: 98.79 train loss: 0.0006 val acc: 88.73 val loss: 0.0126\n",
      "Sub epoch 6 train acc: 98.74 train loss: 0.0006 val acc: 87.70 val loss: 0.0139\n",
      "Train Loss: 0.0006, Train Acc: 98.74  Validation Loss: 0.0139, Validation Acc: 87.70\n",
      "Epoch:  12\n",
      "Training on 36000 samples\n",
      "Sub epoch 0 train acc: 80.78 train loss: 0.0130 val acc: 84.73 val loss: 0.0080\n",
      "Sub epoch 1 train acc: 97.07 train loss: 0.0016 val acc: 88.50 val loss: 0.0094\n",
      "Sub epoch 2 train acc: 98.85 train loss: 0.0006 val acc: 88.25 val loss: 0.0105\n",
      "Sub epoch 3 train acc: 99.23 train loss: 0.0004 val acc: 88.48 val loss: 0.0117\n",
      "Sub epoch 4 train acc: 99.22 train loss: 0.0004 val acc: 88.43 val loss: 0.0127\n",
      "Sub epoch 5 train acc: 99.15 train loss: 0.0004 val acc: 88.38 val loss: 0.0131\n",
      "Sub epoch 6 train acc: 99.11 train loss: 0.0005 val acc: 87.98 val loss: 0.0151\n",
      "Train Loss: 0.0005, Train Acc: 99.11  Validation Loss: 0.0151, Validation Acc: 87.98\n",
      "Epoch:  13\n",
      "Training on 38250 samples\n",
      "Sub epoch 0 train acc: 82.21 train loss: 0.0122 val acc: 86.32 val loss: 0.0082\n",
      "Sub epoch 1 train acc: 97.87 train loss: 0.0012 val acc: 88.47 val loss: 0.0094\n",
      "Sub epoch 2 train acc: 99.17 train loss: 0.0004 val acc: 87.93 val loss: 0.0117\n",
      "Sub epoch 3 train acc: 99.39 train loss: 0.0003 val acc: 88.18 val loss: 0.0134\n",
      "Sub epoch 4 train acc: 99.80 train loss: 0.0001 val acc: 88.70 val loss: 0.0132\n",
      "Sub epoch 5 train acc: 99.91 train loss: 0.0001 val acc: 88.62 val loss: 0.0138\n",
      "Sub epoch 6 train acc: 99.92 train loss: 0.0001 val acc: 88.57 val loss: 0.0142\n",
      "Train Loss: 0.0001, Train Acc: 99.92  Validation Loss: 0.0142, Validation Acc: 88.57\n",
      "Epoch:  14\n",
      "Training on 40500 samples\n",
      "Sub epoch 0 train acc: 80.12 train loss: 0.0131 val acc: 85.43 val loss: 0.0084\n",
      "Sub epoch 1 train acc: 96.90 train loss: 0.0016 val acc: 88.37 val loss: 0.0095\n",
      "Sub epoch 2 train acc: 98.74 train loss: 0.0007 val acc: 88.25 val loss: 0.0100\n",
      "Sub epoch 3 train acc: 99.11 train loss: 0.0005 val acc: 88.20 val loss: 0.0122\n",
      "Sub epoch 4 train acc: 99.12 train loss: 0.0004 val acc: 88.55 val loss: 0.0126\n",
      "Sub epoch 5 train acc: 99.26 train loss: 0.0004 val acc: 88.18 val loss: 0.0153\n",
      "Sub epoch 6 train acc: 98.96 train loss: 0.0006 val acc: 88.20 val loss: 0.0135\n",
      "Train Loss: 0.0006, Train Acc: 98.96  Validation Loss: 0.0135, Validation Acc: 88.20\n",
      "Epoch:  15\n",
      "Training on 42750 samples\n",
      "Sub epoch 0 train acc: 80.91 train loss: 0.0128 val acc: 86.23 val loss: 0.0101\n",
      "Sub epoch 1 train acc: 96.33 train loss: 0.0021 val acc: 88.57 val loss: 0.0100\n",
      "Sub epoch 2 train acc: 97.90 train loss: 0.0012 val acc: 88.73 val loss: 0.0099\n",
      "Sub epoch 3 train acc: 98.55 train loss: 0.0008 val acc: 88.92 val loss: 0.0101\n",
      "Sub epoch 4 train acc: 98.99 train loss: 0.0006 val acc: 88.93 val loss: 0.0103\n",
      "Sub epoch 5 train acc: 99.25 train loss: 0.0005 val acc: 88.72 val loss: 0.0106\n",
      "Sub epoch 6 train acc: 99.43 train loss: 0.0004 val acc: 88.72 val loss: 0.0109\n",
      "Train Loss: 0.0004, Train Acc: 99.43  Validation Loss: 0.0109, Validation Acc: 88.72\n",
      "Epoch:  16\n",
      "Training on 45000 samples\n",
      "Sub epoch 0 train acc: 79.48 train loss: 0.0135 val acc: 84.42 val loss: 0.0092\n",
      "Sub epoch 1 train acc: 96.52 train loss: 0.0019 val acc: 88.48 val loss: 0.0081\n",
      "Sub epoch 2 train acc: 98.41 train loss: 0.0009 val acc: 88.63 val loss: 0.0098\n",
      "Sub epoch 3 train acc: 98.92 train loss: 0.0006 val acc: 88.60 val loss: 0.0118\n",
      "Sub epoch 4 train acc: 99.02 train loss: 0.0005 val acc: 88.58 val loss: 0.0123\n",
      "Sub epoch 5 train acc: 99.56 train loss: 0.0002 val acc: 89.02 val loss: 0.0122\n",
      "Sub epoch 6 train acc: 99.77 train loss: 0.0001 val acc: 89.00 val loss: 0.0126\n",
      "Train Loss: 0.0001, Train Acc: 99.77  Validation Loss: 0.0126, Validation Acc: 89.00\n",
      "Epoch:  17\n",
      "Training on 47250 samples\n",
      "Sub epoch 0 train acc: 80.18 train loss: 0.0129 val acc: 86.38 val loss: 0.0093\n",
      "Sub epoch 1 train acc: 97.24 train loss: 0.0015 val acc: 88.75 val loss: 0.0102\n",
      "Sub epoch 2 train acc: 98.83 train loss: 0.0006 val acc: 89.13 val loss: 0.0114\n",
      "Sub epoch 3 train acc: 99.13 train loss: 0.0005 val acc: 88.68 val loss: 0.0118\n",
      "Sub epoch 4 train acc: 99.25 train loss: 0.0004 val acc: 88.75 val loss: 0.0147\n",
      "Sub epoch 5 train acc: 99.16 train loss: 0.0005 val acc: 88.43 val loss: 0.0138\n",
      "Sub epoch 6 train acc: 99.05 train loss: 0.0005 val acc: 88.82 val loss: 0.0145\n",
      "Train Loss: 0.0005, Train Acc: 99.05  Validation Loss: 0.0145, Validation Acc: 88.82\n",
      "Epoch:  18\n",
      "Training on 49500 samples\n",
      "Sub epoch 0 train acc: 81.75 train loss: 0.0116 val acc: 84.72 val loss: 0.0095\n",
      "Sub epoch 1 train acc: 97.68 train loss: 0.0013 val acc: 88.70 val loss: 0.0108\n",
      "Sub epoch 2 train acc: 99.09 train loss: 0.0005 val acc: 88.90 val loss: 0.0112\n",
      "Sub epoch 3 train acc: 99.29 train loss: 0.0004 val acc: 89.13 val loss: 0.0116\n",
      "Sub epoch 4 train acc: 99.40 train loss: 0.0003 val acc: 89.02 val loss: 0.0121\n",
      "Sub epoch 5 train acc: 99.46 train loss: 0.0003 val acc: 89.07 val loss: 0.0125\n",
      "Sub epoch 6 train acc: 99.54 train loss: 0.0003 val acc: 89.23 val loss: 0.0130\n",
      "Train Loss: 0.0003, Train Acc: 99.54  Validation Loss: 0.0130, Validation Acc: 89.23\n",
      "Epoch:  19\n",
      "Training on 51750 samples\n",
      "Sub epoch 0 train acc: 78.70 train loss: 0.0136 val acc: 85.98 val loss: 0.0092\n",
      "Sub epoch 1 train acc: 96.18 train loss: 0.0020 val acc: 88.80 val loss: 0.0096\n",
      "Sub epoch 2 train acc: 97.83 train loss: 0.0012 val acc: 89.13 val loss: 0.0100\n",
      "Sub epoch 3 train acc: 98.24 train loss: 0.0009 val acc: 88.95 val loss: 0.0115\n",
      "Sub epoch 4 train acc: 98.48 train loss: 0.0007 val acc: 89.12 val loss: 0.0122\n",
      "Sub epoch 5 train acc: 98.45 train loss: 0.0008 val acc: 88.58 val loss: 0.0149\n",
      "Sub epoch 6 train acc: 99.23 train loss: 0.0004 val acc: 89.23 val loss: 0.0141\n",
      "Train Loss: 0.0004, Train Acc: 99.23  Validation Loss: 0.0141, Validation Acc: 89.23\n"
     ]
    }
   ],
   "source": [
    "train_loss_df = pd.DataFrame()\n",
    "train_acc_df = pd.DataFrame()\n",
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    train_loss, train_acc, validation_loss, validation_acc = \\\n",
    "        active_learn(net, trainset, np.array(train_df[i].to_list()), np.array(val_idx_df[i].to_list()), \n",
    "                      heuristic=mc_dropout_heuristic, initial_train_idx=np.array(batch_df[i].to_list()),\n",
    "                     experiment_id=i)\n",
    "    train_loss_df[i] = train_loss\n",
    "    train_acc_df[i] = train_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b90757e-ddb6-4687-a90e-fb345cf56b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T16:28:42.092721Z",
     "iopub.status.busy": "2023-01-12T16:28:42.092517Z",
     "iopub.status.idle": "2023-01-12T16:35:59.130494Z",
     "shell.execute_reply": "2023-01-12T16:35:59.129918Z",
     "shell.execute_reply.started": "2023-01-12T16:28:42.092700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006454977619647979, 0.007513852410018444, 0.012177833758294582, 0.009188270650804043, 0.011750051452219487, 0.01471312368772924, 0.010758466263860464, 0.011852485623955727, 0.01208242296129465, 0.01321757923066616, 0.01455217830836773, 0.00997222841978073, 0.011704740974283777, 0.012340587155148387, 0.010681984854862095, 0.012840399500261992, 0.012965312905609608, 0.012117489473894238, 0.015186554259806872, 0.01399996635504067] [86.89, 87.41, 86.01, 87.81, 87.59, 87.63, 88.26, 86.99, 87.58, 88.32, 87.92, 88.56, 88.6, 88.08, 89.0, 88.47, 88.29, 88.61, 88.1, 88.74]\n",
      "[0.007729027733206749, 0.009311564723402261, 0.007847379719465971, 0.008973601446300746, 0.011111704054474831, 0.01057498421818018, 0.011665396835654974, 0.01127609640955925, 0.011093707536906004, 0.012942299670353532, 0.015303342847526073, 0.012917837099730968, 0.012998149076104163, 0.010017135564237833, 0.01199440236017108, 0.014525871756672859, 0.01114669477045536, 0.014042772030830384, 0.0168311753064394, 0.01307680901736021] [87.29, 86.78, 87.68, 88.02, 87.1, 88.03, 87.99, 87.99, 88.51, 87.71, 87.61, 88.49, 87.62, 88.48, 88.56, 88.0, 88.98, 88.46, 87.76, 89.02]\n",
      "[0.00948145814910531, 0.011369958643615246, 0.010507740069180727, 0.01273157819584012, 0.010583362393826247, 0.011464727645367384, 0.013593580885231495, 0.009287595449388028, 0.01149538598433137, 0.012131154001504182, 0.0104970488473773, 0.012679602728597819, 0.013727822956442834, 0.01370093912165612, 0.013555537697672844, 0.010445657857507467, 0.011628263854980468, 0.013479359181970359, 0.012313920801132918, 0.013343847044557332] [85.33, 86.95, 87.94, 87.34, 87.92, 88.39, 87.62, 88.41, 88.66, 88.21, 88.51, 88.25, 88.37, 88.86, 87.78, 89.0, 89.02, 88.73, 89.05, 89.25]\n"
     ]
    }
   ],
   "source": [
    "val_loss_df = pd.DataFrame()\n",
    "val_acc_df = pd.DataFrame()\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_acc_df = pd.DataFrame()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "for i in range(3):\n",
    "    i = str(i)\n",
    "    validation_loss, validation_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.Subset(trainset, val_idx_df[i]),\n",
    "        batch_size=64,\n",
    "        num_workers=8)\n",
    "    for k in range(20):\n",
    "        net.load_state_dict(torch.load(f\"{i}/epoch{k}_mc_dropout_heuristic.pt\"))\n",
    "        \n",
    "        v_loss, v_acc = validate(net, val_loader)\n",
    "        t_loss, t_acc = validate(net, test_loader)\n",
    "        validation_loss.append(v_loss)\n",
    "        validation_acc.append(v_acc)\n",
    "        test_loss.append(t_loss)\n",
    "        test_acc.append(t_acc)\n",
    "        # print(t_acc)\n",
    "\n",
    "    test_loss_df[i] = test_loss\n",
    "    test_acc_df[i] = test_acc\n",
    "    val_loss_df[i] = validation_loss\n",
    "    val_acc_df[i] = validation_acc\n",
    "    print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c7e69a-0362-4f0c-8765-02030e5168f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T16:35:59.132396Z",
     "iopub.status.busy": "2023-01-12T16:35:59.132207Z",
     "iopub.status.idle": "2023-01-12T16:36:00.122712Z",
     "shell.execute_reply": "2023-01-12T16:36:00.122148Z",
     "shell.execute_reply.started": "2023-01-12T16:35:59.132377Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.to_csv(\"val_loss_mc.csv\",index=False)\n",
    "val_acc_df.to_csv(\"val_acc_mc.csv\",index=False)\n",
    "test_loss_df.to_csv(\"test_loss_mc.csv\",index=False)\n",
    "test_acc_df.to_csv(\"test_acc_mc.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417d0e0-85d0-44c1-bd0f-4f740232e4cf",
   "metadata": {},
   "source": [
    "### Maszyna wektorÃ³w noÅ›nych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fa07c-6733-4e85-bf9b-7be803f1f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_df = pd.DataFrame()\n",
    "# train_acc_df = pd.DataFrame()\n",
    "# val_loss_df = pd.DataFrame()\n",
    "# val_acc_df = pd.DataFrame()\n",
    "# for i in range(3):\n",
    "#     i = str(i)\n",
    "#     train_loss, train_acc, validation_loss, validation_acc = \\\n",
    "#         active_learn(net, trainset, np.array(train_df[i].to_list()), np.array(val_idx_df[i].to_list()), \n",
    "#                       heuristic=mc_dropout_heuristic, initial_train_idx=np.array(batch_df[i].to_list()),\n",
    "#                      experiment_id=i)\n",
    "#     train_loss_df[i] = train_loss\n",
    "#     train_acc_df[i] = train_acc\n",
    "#     val_loss_df[i] = validation_loss\n",
    "#     val_acc_df[i] = validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db46e6-87ad-4b8d-8c88-d80f157d888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_df = pd.DataFrame()\n",
    "# val_acc_df = pd.DataFrame()\n",
    "# test_loss_df = pd.DataFrame()\n",
    "# test_acc_df = pd.DataFrame()\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#         testset,\n",
    "#         batch_size=64,\n",
    "#         num_workers=8)\n",
    "# for i in range(3):\n",
    "#     i = str(i)\n",
    "#     validation_loss, validation_acc = [], []\n",
    "#     test_loss, test_acc = [], []\n",
    "#     val_loader = torch.utils.data.DataLoader(\n",
    "#         torch.utils.data.Subset(trainset, val_idx_df[i]),\n",
    "#         batch_size=64,\n",
    "#         num_workers=8)\n",
    "#     for k in range(20):\n",
    "#         net.load_state_dict(torch.load(f\"{i}/epoch{k}_mc_dropout_heuristic.pt\"))\n",
    "        \n",
    "#         v_loss, v_acc = validate(net, val_loader)\n",
    "#         t_loss, t_acc = validate(net, test_loader)\n",
    "#         validation_loss.append(v_loss)\n",
    "#         validation_acc.append(v_acc)\n",
    "#         test_loss.append(t_loss)\n",
    "#         test_acc.append(t_acc)\n",
    "#         # print(t_acc)\n",
    "\n",
    "#     test_loss_df[i] = test_loss\n",
    "#     test_acc_df[i] = test_acc\n",
    "#     val_loss_df[i] = validation_loss\n",
    "#     val_acc_df[i] = validation_acc\n",
    "#     print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd5579-00cc-47bd-938c-14e133b7b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_df.to_csv(\"val_loss_mc.csv\",index=False)\n",
    "# val_acc_df.to_csv(\"val_acc_mc.csv\",index=False)\n",
    "# test_loss_df.to_csv(\"test_loss_mc.csv\",index=False)\n",
    "# test_acc_df.to_csv(\"test_acc_mc.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
